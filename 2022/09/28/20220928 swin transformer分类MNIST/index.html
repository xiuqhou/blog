<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>swin transformeråˆ†ç±»MNIST | xiuqhouçš„ä¸ªäººåšå®¢</title><meta name="keywords" content="Classification"><meta name="author" content="xiuqhou"><meta name="copyright" content="xiuqhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="swin transformeråˆ†ç±»MNIST"><meta name="application-name" content="swin transformeråˆ†ç±»MNIST"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="swin transformeråˆ†ç±»MNIST"><meta property="og:url" content="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/index.html"><meta property="og:site_name" content="xiuqhouçš„ä¸ªäººåšå®¢"><meta property="og:description" content="å®‰è£…ä¾èµ–   timmåº“ä¸­æä¾›äº†swin transformerä½¿ç”¨çš„DropPathå±‚ç­‰ç»“æ„   torchåº“æ˜¯æ„å»ºç¥ç»ç½‘ç»œå’Œå®ç°è‡ªåŠ¨åå‘ä¼ æ’­çš„åŸºç¡€åº“   sysåº“æä¾›äº†ä¸€äº›ç³»ç»Ÿä¿¡æ¯å’Œæ“ä½œçš„æ¥å£   loggingåº“æä¾›äº†æ—¥å¿—è®°å½•çš„åŠŸèƒ½   1!pip install timm 123456789"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformerç»“æ„.webp"><meta property="article:author" content="xiuqhou"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformerç»“æ„.webp"><meta name="description" content="å®‰è£…ä¾èµ–   timmåº“ä¸­æä¾›äº†swin transformerä½¿ç”¨çš„DropPathå±‚ç­‰ç»“æ„   torchåº“æ˜¯æ„å»ºç¥ç»ç½‘ç»œå’Œå®ç°è‡ªåŠ¨åå‘ä¼ æ’­çš„åŸºç¡€åº“   sysåº“æä¾›äº†ä¸€äº›ç³»ç»Ÿä¿¡æ¯å’Œæ“ä½œçš„æ¥å£   loggingåº“æä¾›äº†æ—¥å¿—è®°å½•çš„åŠŸèƒ½   1!pip install timm 123456789"><link rel="shortcut icon" href="/blog/favicon.ico"><link rel="canonical" href="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ï¾ŸĞ”ï¾Ÿ)w ä¸è¦èµ°ï¼å†çœ‹çœ‹å˜›ï¼","backTitle":"â™ª(^âˆ‡^*)æ¬¢è¿è‚¥æ¥ï¼"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/blog/',
  preloader: {"source":2},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: undefined,
  algolia: undefined,
  localSearch: {"path":"/blog/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"ç¹","msgToSimplifiedChinese":"ç®€","rightMenuMsgToTraditionalChinese":"è½¬ä¸ºç¹ä½“","rightMenuMsgToSimplifiedChinese":"è½¬ä¸ºç®€ä½“"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"ä½œè€…: xiuqhou","link":"é“¾æ¥: ","source":"æ¥æº: xiuqhouçš„ä¸ªäººåšå®¢","info":"è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚","copySuccess":"å¤åˆ¶æˆåŠŸï¼Œå¤åˆ¶å’Œè½¬è½½è¯·æ ‡æ³¨æœ¬æ–‡åœ°å€"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'xiuqhouçš„ä¸ªäººåšå®¢',
  title: 'swin transformeråˆ†ç±»MNIST',
  postAI: '',
  pageFillDescription: 'å®‰è£…ä¾èµ–, pytorchå®˜æ–¹è®­ç»ƒå‚è€ƒä¸­ç»™å‡ºçš„ç›¸å…³ä»£ç å®‰è£…ä¾èµ–åº“ä¸­æä¾›äº†ä½¿ç”¨çš„å±‚ç­‰ç»“æ„åº“æ˜¯æ„å»ºç¥ç»ç½‘ç»œå’Œå®ç°è‡ªåŠ¨åå‘ä¼ æ’­çš„åŸºç¡€åº“åº“æä¾›äº†ä¸€äº›ç³»ç»Ÿä¿¡æ¯å’Œæ“ä½œçš„æ¥å£åº“æä¾›äº†æ—¥å¿—è®°å½•çš„åŠŸèƒ½åˆ›å»ºæ—¥å¿—è®°å½•çš„åŠŸèƒ½å¯å°†è¾“å‡ºæŒ‰ç…§ä¸€å®šæ ¼å¼é‡å®šå‘åˆ°æ–‡ä»¶ä¸­åœ¨å¤§å‹ç½‘ç»œè°ƒè¯•çš„æ—¶å€™å¯¹äºæŒæ¡ç½‘ç»œä¿¡æ¯å¾ˆæœ‰ç”¨å¦‚æœåå­—æœ‰å±‚çº§ç»“æ„åˆ™ä¹Ÿä¼šå‘å¯¹åº”çš„çˆ¶çº§ç»“æ„ä¼ é€’æ—¥å¿—è®¾ç½®è®°å½•çš„æ—¥å¿—ç­‰çº§æœ‰ç¦æ­¢å±‚çº§ä¼ é€’æ ¼å¼åŒ–è¾“å‡ºä¸»è¿›ç¨‹çš„æ§åˆ¶å¥æŸ„ä¸ºå‘æ ‡å‡†è¾“å‡ºè¾“å‡ºä¿¡æ¯æ–‡ä»¶å¥æŸ„å‘æ–‡ä»¶è¿›è¡Œè¾“å‡ºæ—¥å¿—ä¿¡æ¯åŸºæœ¬ç»“æ„åŒ…æ‹¬ä¸€ä¸ªç”¨äºå°†å›¾åƒæ‰“æˆçš„å±‚å¯¹åå›¾åƒæ•°ç›®è¿›è¡Œå˜æ¢çš„å±‚ä»¥åŠä¸ªç”¨äºå±‚çº§å˜æ¢æ•°æ®æµå‘ä¸ºå’Œå±‚åœ¨ä»£ç ä¸­åˆå¹¶ä¸ºå±‚ç”¨äºå°†å›¾ç‰‡æ‰“æˆå’Œå˜æ¢æ•°ç›®è¾“å…¥å›¾åƒä»å˜æ¢è‡³å†å˜æ¢è‡³ä»£ç å®ç°æ˜¯é€šè¿‡æ­¥é•¿ä¸ºçš„å·ç§¯æ“ä½œä¸€æ¬¡æ€§å®ç°ä¸¤ä¸ªæ“ä½œæ­¤æ—¶å½¢çŠ¶è®°ä¸ºæ³¨æ„è¯¥å½¢çŠ¶ä¸ºä»£ç å®ç°ä¸­çš„å†™æ³•å’Œå›¾ä¸­è¡¨ç¤ºæœ‰æ‰€ä¸åŒå±‚åŒ…æ‹¬å¤šä¸ªå’Œä¸€ä¸ªå±‚ç½‘ç»œä¸€å…±åŒ…å«ä¸ªæ¯å±‚å…·æœ‰çš„æ•°ç›®ä¸ºæ¯ä¸ªå±‚ä¸­ä»…ç¬¬ä¸€ä¸ªä¸éœ€è¦å®ç°æ“ä½œä¸­é¦–å…ˆé€šè¿‡æ“ä½œå°†å›¾ç‰‡åˆ’åˆ†ä¸ºä¸ªå½¢çŠ¶çš„å°çª—å£å¹¶å°†å…¶ç»„ç»‡æˆçš„å½¢çŠ¶è®°ä¸ºå…¶ä¸­ä¸ºåˆ’åˆ†çš„å°çª—å£æ•°é‡ç„¶åæ‰§è¡Œå¤šå¤´æ³¨æ„åŠ›æ“ä½œä¸­å…ˆå¯¹çŸ©é˜µé€šè¿‡å…¨è¿æ¥å±‚ç”Ÿæˆå½¢çŠ¶å‡ä¸ºçš„çš„ä¸‰ä¸ªçŸ©é˜µç„¶åæŒ‰ç…§å¤šå¤´æ³¨æ„åŠ›çš„æ•°ç›®å°†å…¶å½¢çŠ¶å˜ä¸ºå°†å…¶è§†ä¸ºä¸ªç»´åº¦ä¸ºçš„çŸ©é˜µé€šè¿‡è®¡ç®—å¾—åˆ°å½¢çŠ¶ä¸ºçš„æ³¨æ„åŠ›çŸ©é˜µç„¶åå°†é€šè¿‡å’ŒçŸ©é˜µç›¸ä¹˜å¾—åˆ°å½¢çŠ¶ä¸ºçš„çŸ©é˜µæœ€åå°†å…¶ç»„ç»‡ä¸ºå½¢çŠ¶çš„çŸ©é˜µè¿›è¡Œè¾“å‡ºå› æ­¤è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ç›¸åŒå¯ä»¥ä»»æ„å †å å¤šå±‚å±‚ä¹‹å‰é¦–å…ˆå°†å¤šå±‚å †å è¾“å‡ºå¾—åˆ°çš„çŸ©é˜µå…ˆæŒ‰ç…§å°çª—å£çš„åˆ’åˆ†æ–¹å¼å°†å°çª—å£åˆå¹¶ä¸ºç”¨è¾“å…¥å›¾ç‰‡çš„å½¢çŠ¶è¡¨ç¤ºå³ä¸ºç„¶åå°†å…¶å˜æ¢ä¸ºå±‚æœ€åˆè¾“å…¥çš„å›¾ç‰‡å½¢çŠ¶å±‚ä¸­é€šè¿‡é—´éš”é‡‡æ ·å¾—åˆ°ä¸ªå½¢çŠ¶ä¸ºçš„çŸ©é˜µæ‹¼æ¥å¾—åˆ°çš„å†é€åˆ°å…¨è¿æ¥é™ç»´æˆçš„çŸ©é˜µè¯¥å±‚è¾“å…¥ä¸ºè¾“å‡ºä¸ºä»è€Œå®ç°ä¸‹é‡‡æ ·çš„ç›®çš„ç”±äºæœ€åä¸€å±‚æ²¡æœ‰ä¸‹é‡‡æ ·å±‚å› æ­¤ç»è¿‡å±‚å¾—åˆ°çš„è¾“å‡ºä¸ºç»è¿‡è½¬åŒ–ä¸ºçš„å½¢å¼ç„¶åé€å…¥å…¨è¿æ¥è¿›è¡Œåˆ†ç±»å°†ä¸ªçª—å£åˆå¹¶ä¸ºä¸€ä¸ªé¿å…æŠ¥é”™ä¿¡æ¯æ³¨å†Œä¸ºä¸å¯å˜å‚æ•°åœ¨ä¿å­˜æ¨¡å‹æ—¶è¯¥å‚æ•°ä¹Ÿä¼šè¢«ä¿å­˜æˆªæ–­åˆ°ä¹‹å†…çš„æ­£æ€åˆ†å¸ƒæ³¨æ„åŠ›æœºåˆ¶ç”¨äºå±è”½ä¸åº”çš„æ³¨æ„åŠ›çŸ©é˜µéƒ¨åˆ†å¯¹æ³¨æ„åŠ›å›¾è¿›è¡Œæ“ä½œè®¡ç®—å¤æ‚åº¦å¹¶ä¸æ˜¯æ¯ç§’çš„æµ®ç‚¹è¿ç®—æ•°æ˜¯å°å—æ‹‰ç›´çš„æ•°ç›®é•¿åº¦æ˜¯æ¯ä¸ªåºåˆ—çš„é•¿åº¦è®¡ç®—å¤æ‚åº¦ä¸ºå…ˆæ‹†åˆ†ä¸ºå¤šå¤´æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä¸ºè®¡ç®—å¤æ‚åº¦ä¸ºè®¡ç®—å¤æ‚åº¦ä¸ºåœ¨å°èŒƒå›´å†…åšè‡ªæ³¨æ„åŠ›ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µå†ç»´åº¦éšæœºè®¾ç½®ä¸€å®šæ ·æœ¬ä¸è¿›è¡Œä¸»å¹²è€Œç›´æ¥ç”±åˆ†æ”¯è¿›è¡Œæ’ç­‰æ˜ å°„ç»™æ¯å—åŒºåŸŸåˆ’åˆ†æ ‡è®°åºå·æ¯ä¸€ä¸ªç»´åº¦åˆ†ä¸ºä¸‰éƒ¨åˆ†ç”Ÿæˆæ©ç çª—å£è·å¾—æ©ç çŸ©é˜µå…·ä½“çœ‹è®ºæ–‡ä¸­æ‰€è¯´æ ‡å‡†åŒ–å±‚ä¸ºçª—å£æ•°ç›®çª—å£æ•°ç›®æ¯ä¸ªçª—å£çš„é•¿åº¦é€šé“æ•°ç›®å¤šä¸ªçª—å£å¯ä»¥å¹¶è¡Œåœ°åšæ³¨æ„åŠ›å°†æ‰€æœ‰å°çª—å£åˆå¹¶ä¸ºä¸€ä¸ªå¤§çª—å£å¯¹æ•´ä¸ªçª—å£è¿›è¡Œå·¦ä¸Šçš„å¹³ç§»æ“ä½œä¸‹é‡‡æ ·æ¯è¡Œæ¯åˆ—é—´éš”é‡‡æ ·é€šé“æ•°æ‰©å¢åˆ°å€ç„¶åå†å°†å›åˆ°å€åˆ‡å‰²æˆä¸ªå—è¾“å…¥é€šé“æ•°ç›®ä¸ºåµŒå…¥ç»´åº¦ä¸ºé€šé“æ•°å˜åŒ–å±‚å½’ä¸€åŒ–å¸¸ç”¨äºå› æ­¤å¯èƒ½æ¥æºäºè‡ªå¸¦çš„æ¯ä¸€ä¸ªå±‚çš„æ·±åº¦å› ä¸ºå¯ä»¥éšæ„å †å ç»å¯¹ä½ç½®ç¼–ç å½’ä¸€åŒ–ç»è¿‡æ‰€æœ‰å±‚åçš„é€šé“æ•°é‡éšè—å±‚ç»´åº¦ä¸åµŒå…¥ç»´åº¦çš„å•†å¯¹å›¾ç‰‡è¿›è¡Œåˆ‡å‰²çš„å—å±‚æ•°ä¸€å…±ä¸ºå…¶ä¸­æ¯ä¸€å±‚çš„æ¦‚ç‡ä»å¢åŠ åˆ°æ¯ç»è¿‡ä¸€å±‚å›¾ç‰‡å®½å’Œé«˜å‡åŠåŠ å€åˆå§‹çš„å›¾ç‰‡ä¸ºçª—å£å°ºå¯¸æ˜¯å®˜æ–¹è®­ç»ƒå‚è€ƒä¸­ç»™å‡ºçš„ç›¸å…³ä»£ç å›¾ç‰‡é¢„å¤„ç†ç›¸å…³åˆ†å¸ƒå¼æ•°æ®é‡‡æ ·æ•°æ®å¢å¼ºæªæ–½å’Œæ•°å€¼å¹³æ»‘å‡è½»è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å‡½æ•°æ•°å€¼çš„æŠ–åŠ¨è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°æœ¬æ¬¡åˆ†ç±»æ²¡æœ‰ç”¨å®˜æ–¹ç»™å®šçš„ä¸»å‡½æ•°è®­ç»ƒå’Œè¯„ä¼°å½“ä¸è¶³ä¸€ä¸ªæ—¶å€™ä½¿ä¸¢å¼ƒåé¢ä¸€éƒ¨åˆ†è¿˜æ˜¯éšæœºå¢åŠ ä¸€éƒ¨åˆ†æ ·æœ¬åº“é‡Œé¢è‡ªå¸¦çš„çš„å’Œçš„æ­¤å¤„æ²¡ç”¨åˆ°',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-02 13:27:02',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/blog/" accesskey="h"><div class="title">xiuqhouçš„ä¸ªäººåšå®¢</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> æ–‡ç« </span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> éš§é“</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> åˆ†ç±»</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> æ ‡ç­¾</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> æˆ‘çš„</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> éŸ³ä¹é¦†</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> è¿½ç•ªé¡µ</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> ç›¸å†Œé›†</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> å°ç©ºè°ƒ</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="éšæœºå‰å¾€ä¸€ä¸ªæ–‡ç« " href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="æœç´¢ğŸ”" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> æœç´¢</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="ä¸­æ§å°" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="å¾®ä¿¡" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">å¾®ä¿¡</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="æ”¯ä»˜å®" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">æ”¯ä»˜å®</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">äº’åŠ¨</div><span class="author-content-item-title"> æœ€æ–°è¯„è®º</span></div><div class="aside-list"><span>æ­£åœ¨åŠ è½½ä¸­...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">å…´è¶£ç‚¹</div><span class="author-content-item-title">å¯»æ‰¾ä½ æ„Ÿå…´è¶£çš„é¢†åŸŸ</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 1.05rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 1.05rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 1.05rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 1.05rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 1.05rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 1.05rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 1.05rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 1.05rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 1.05rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 1.05rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 1.05rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 1.05rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 1.05rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 1.05rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 1.05rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 1.05rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 1.05rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 1.05rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">ä¼˜åŒ–ç®—æ³•<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">åšå®¢<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">æœåŠ¡å™¨<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 1.05rem;">ç”»å›¾<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>æ–‡ç« </span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>å½’æ¡£</span><a class="card-more-btn" href="/blog/archives/" title="æŸ¥çœ‹æ›´å¤š">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/08/"><span class="card-archive-list-date">å…«æœˆ 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/07/"><span class="card-archive-list-date">ä¸ƒæœˆ 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/03/"><span class="card-archive-list-date">ä¸‰æœˆ 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/02/"><span class="card-archive-list-date">äºŒæœˆ 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/01/"><span class="card-archive-list-date">ä¸€æœˆ 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/12/"><span class="card-archive-list-date">åäºŒæœˆ 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/11/"><span class="card-archive-list-date">åä¸€æœˆ 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>ç¯‡</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/09/"><span class="card-archive-list-date">ä¹æœˆ 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>ç¯‡</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="æ˜¾ç¤ºæ¨¡å¼åˆ‡æ¢" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="è¾¹æ æ˜¾ç¤ºæ§åˆ¶"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="éŸ³ä¹å¼€å…³"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="åˆ‡æ¢"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">åŸåˆ›</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/blog/categories/%E4%BB%A3%E7%A0%81/" itemprop="url">ä»£ç </a></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Classification/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Classification</span></a></span></div></div><h1 class="post-title" itemprop="name headline">swin transformeråˆ†ç±»MNIST</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-09-28T10:51:50.000Z" title="å‘è¡¨äº 2022-09-28 10:51:50">2022-09-28</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-08-02T13:27:02.849Z" title="æ›´æ–°äº 2024-08-02 13:27:02">2024-08-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="ä½œè€…IPå±åœ°ä¸ºé•¿æ²™"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>é•¿æ²™</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformerç»“æ„.webp"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"><header><a class="post-meta-categories" href="/blog/categories/%E4%BB%A3%E7%A0%81/" itemprop="url">ä»£ç </a><a href="/blog/tags/Classification/" tabindex="-1" itemprop="url">Classification</a><h1 id="CrawlerTitle" itemprop="name headline">swin transformeråˆ†ç±»MNIST</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">xiuqhou</span><time itemprop="dateCreated datePublished" datetime="2022-09-28T10:51:50.000Z" title="å‘è¡¨äº 2022-09-28 10:51:50">2022-09-28</time><time itemprop="dateCreated datePublished" datetime="2024-08-02T13:27:02.849Z" title="æ›´æ–°äº 2024-08-02 13:27:02">2024-08-02</time></header><h2 id="å®‰è£…ä¾èµ–">å®‰è£…ä¾èµ–</h2>
<ul>
<li>
<p>timmåº“ä¸­æä¾›äº†swin transformerä½¿ç”¨çš„DropPathå±‚ç­‰ç»“æ„</p>
</li>
<li>
<p>torchåº“æ˜¯æ„å»ºç¥ç»ç½‘ç»œå’Œå®ç°è‡ªåŠ¨åå‘ä¼ æ’­çš„åŸºç¡€åº“</p>
</li>
<li>
<p>sysåº“æä¾›äº†ä¸€äº›ç³»ç»Ÿä¿¡æ¯å’Œæ“ä½œçš„æ¥å£</p>
</li>
<li>
<p>loggingåº“æä¾›äº†æ—¥å¿—è®°å½•çš„åŠŸèƒ½</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install timm</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Collecting timm</span><br><span class="line">  Downloading timm-0.6.7-py3-none-any.whl (509 kB)</span><br><span class="line">[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m510.0/510.0 kB[0m [31m799.5 kB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m</span><br><span class="line">[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)</span><br><span class="line">Requirement already satisfied: torch&gt;=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)</span><br><span class="line">Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.4-&gt;timm) (4.3.0)</span><br><span class="line">Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (2.28.1)</span><br><span class="line">Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (9.1.1)</span><br><span class="line">Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (1.21.6)</span><br><span class="line">Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2.1.0)</span><br><span class="line">Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (1.26.12)</span><br><span class="line">Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2022.6.15.2)</span><br><span class="line">Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (3.3)</span><br><span class="line">Installing collected packages: timm</span><br><span class="line">Successfully installed timm-0.6.7</span><br><span class="line">[33mWARNING: Running pip as the &#x27;root&#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m</span><br><span class="line">[0m</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">from</span> timm.data <span class="keyword">import</span> Mixup</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint <span class="keyword">as</span> checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> timm.loss <span class="keyword">import</span> LabelSmoothingCrossEntropy, SoftTargetCrossEntropy</span><br><span class="line"><span class="keyword">from</span> timm.utils <span class="keyword">import</span> accuracy, AverageMeter</span><br><span class="line"><span class="keyword">from</span> timm.models.layers <span class="keyword">import</span> DropPath, to_2tuple, trunc_normal_</span><br><span class="line"><span class="keyword">from</span> timm.optim.nadam <span class="keyword">import</span> Nadam</span><br><span class="line"><span class="keyword">from</span> timm.scheduler.cosine_lr <span class="keyword">import</span> CosineLRScheduler</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br></pre></td></tr></table></figure>
<p>åˆ›å»ºæ—¥å¿—è®°å½•çš„åŠŸèƒ½ï¼Œå¯å°†è¾“å‡ºæŒ‰ç…§ä¸€å®šæ ¼å¼é‡å®šå‘åˆ°æ–‡ä»¶ä¸­ï¼Œåœ¨å¤§å‹ç½‘ç»œè°ƒè¯•çš„æ—¶å€™å¯¹äºæŒæ¡ç½‘ç»œä¿¡æ¯å¾ˆæœ‰ç”¨</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@functools.lru_cache()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_logger</span>(<span class="params">output_dir, dist_rank=<span class="number">0</span>, name=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># create loggerï¼Œå¦‚æœåå­—æœ‰å±‚çº§ç»“æ„ï¼Œåˆ™loggerä¹Ÿä¼šå‘å¯¹åº”çš„çˆ¶çº§ç»“æ„ä¼ é€’æ—¥å¿—</span></span><br><span class="line">    logger = logging.getLogger(name)</span><br><span class="line">    <span class="comment"># è®¾ç½®è®°å½•çš„æ—¥å¿—ç­‰çº§ï¼Œæœ‰CRITICALã€ERRORã€WARNINGã€INFOã€DEBUGã€NOTSET</span></span><br><span class="line">    logger.setLevel(logging.DEBUG)</span><br><span class="line">    <span class="comment"># ç¦æ­¢å±‚çº§ä¼ é€’</span></span><br><span class="line">    logger.propagate = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create formatter</span></span><br><span class="line">    <span class="comment"># æ ¼å¼åŒ–è¾“å‡º</span></span><br><span class="line">    fmt = <span class="string">&#x27;[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s&#x27;</span></span><br><span class="line">    color_fmt = colored(<span class="string">&#x27;[%(asctime)s %(name)s]&#x27;</span>, <span class="string">&#x27;green&#x27;</span>) + colored(<span class="string">&#x27;(%(filename)s %(lineno)d)&#x27;</span>,</span><br><span class="line">                                                                     <span class="string">&#x27;yellow&#x27;</span>) + <span class="string">&#x27;: %(levelname)s %(message)s&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create console handlers for master process</span></span><br><span class="line">    <span class="keyword">if</span> dist_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># ä¸»è¿›ç¨‹çš„æ§åˆ¶å¥æŸ„ä¸ºå‘æ ‡å‡†è¾“å‡ºè¾“å‡ºä¿¡æ¯</span></span><br><span class="line">        console_handler = logging.StreamHandler(sys.stdout)</span><br><span class="line">        console_handler.setLevel(logging.DEBUG)</span><br><span class="line">        console_handler.setFormatter(</span><br><span class="line">            logging.Formatter(fmt=color_fmt, datefmt=<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>))</span><br><span class="line">        logger.addHandler(console_handler)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create file handlers</span></span><br><span class="line">    <span class="comment"># æ–‡ä»¶å¥æŸ„ï¼Œå‘æ–‡ä»¶è¿›è¡Œè¾“å‡ºæ—¥å¿—ä¿¡æ¯</span></span><br><span class="line">    file_handler = logging.FileHandler(os.path.join(output_dir, <span class="string">f&#x27;log_rank<span class="subst">&#123;dist_rank&#125;</span>.txt&#x27;</span>), mode=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    file_handler.setLevel(logging.DEBUG)</span><br><span class="line">    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>))</span><br><span class="line">    logger.addHandler(file_handler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure>
<p>swin transformeråŸºæœ¬ç»“æ„ï¼š</p>
<p>åŒ…æ‹¬ä¸€ä¸ªç”¨äºå°†å›¾åƒæ‰“æˆpatchçš„Patch Partitionå±‚ã€å¯¹patchåå›¾åƒchannelæ•°ç›®è¿›è¡Œå˜æ¢çš„Linear Embeddingå±‚ï¼Œä»¥åŠ4ä¸ªBasic Blockç”¨äºå±‚çº§transformerå˜æ¢ã€‚æ•°æ®æµå‘ä¸ºï¼š</p>
<ul>
<li>
<p>Patch Partitionå’ŒLinear Embeddingå±‚åœ¨ä»£ç ä¸­åˆå¹¶ä¸ºPatch Embedå±‚ï¼Œç”¨äºå°†å›¾ç‰‡æ‰“æˆpatchå’Œå˜æ¢channelsæ•°ç›®ã€‚è¾“å…¥å›¾åƒä»(B,H,W,3)å˜æ¢è‡³(B,H//num_patch, W//num_patch, num_patch<em>num_patch</em>3)ï¼Œå†å˜æ¢è‡³(B,H//num_patch, W//num_patch, 96)ï¼Œä»£ç å®ç°æ˜¯é€šè¿‡æ­¥é•¿ä¸ºnum_patchçš„å·ç§¯æ“ä½œä¸€æ¬¡æ€§å®ç°ä¸¤ä¸ªæ“ä½œã€‚æ­¤æ—¶å½¢çŠ¶è®°ä¸º(B, H0, W0, C)ï¼Œæ³¨æ„è¯¥å½¢çŠ¶ä¸ºä»£ç å®ç°ä¸­çš„å†™æ³•ï¼Œå’Œå›¾ä¸­è¡¨ç¤ºæœ‰æ‰€ä¸åŒã€‚</p>
</li>
<li>
<p>BasicBlockå±‚åŒ…æ‹¬å¤šä¸ªSwin Transformer Blockå’Œä¸€ä¸ªPatch Mergingå±‚ã€‚Swin Transformerç½‘ç»œä¸€å…±åŒ…å«4ä¸ªBasic Blockï¼Œæ¯å±‚å…·æœ‰çš„Swin Transformer Blockæ•°ç›®ä¸º[2,2,6,2]ï¼Œæ¯ä¸ªBasic Blockå±‚ä¸­ä»…ç¬¬ä¸€ä¸ªSwin Transformerä¸éœ€è¦å®ç°shifted windowæ“ä½œã€‚</p>
<ol>
<li>
<p>Swin Transformer Blockä¸­é¦–å…ˆé€šè¿‡window_partitionæ“ä½œå°†å›¾ç‰‡åˆ’åˆ†ä¸ºH0//window_size * W0//window_sizeä¸ª(B, window_size, window_size, C)å½¢çŠ¶çš„å°çª—å£ï¼Œå¹¶å°†å…¶ç»„ç»‡æˆ(H0//window_size * W0//window_size * B, window_size * window_size, C)çš„å½¢çŠ¶ï¼Œè®°ä¸º(nW<em>B, N, C)ï¼Œå…¶ä¸­nW=H0//window_size</em>W0//window_sizeä¸ºåˆ’åˆ†çš„å°çª—å£æ•°é‡ï¼Œç„¶åæ‰§è¡Œå¤šå¤´æ³¨æ„åŠ›WindowsAttentionæ“ä½œã€‚</p>
</li>
<li>
<p>WindowsAttentionä¸­å…ˆå¯¹çŸ©é˜µé€šè¿‡å…¨è¿æ¥å±‚ç”Ÿæˆå½¢çŠ¶å‡ä¸ºçš„(nW<em>B, N, C)çš„ä¸‰ä¸ªçŸ©é˜µQ,K,Vï¼Œç„¶åæŒ‰ç…§å¤šå¤´æ³¨æ„åŠ›çš„æ•°ç›®å°†å…¶å½¢çŠ¶å˜ä¸º(nW</em>B, num_heads, N, C//num_heads)ï¼Œå°†å…¶è§†ä¸ºnW<em>B</em>num_headsä¸ªç»´åº¦ä¸º(N,C//num_heads)çš„çŸ©é˜µï¼Œé€šè¿‡ attn=Q K^\top è®¡ç®—å¾—åˆ°å½¢çŠ¶ä¸º(nW<em>B, num_heads, N, N)çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œç„¶åå°†é€šè¿‡attnå’ŒVçŸ©é˜µç›¸ä¹˜å¾—åˆ°å½¢çŠ¶ä¸º(nW</em>B, num_heads, N, C//num_heads)çš„çŸ©é˜µï¼Œæœ€åå°†å…¶ç»„ç»‡ä¸º(nW*B, N, C)å½¢çŠ¶çš„çŸ©é˜µè¿›è¡Œè¾“å‡ºã€‚å› æ­¤WindowsAttentionè¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ç›¸åŒï¼Œå¯ä»¥ä»»æ„å †å å¤šå±‚ã€‚</p>
</li>
<li>
<p>PatchMergingå±‚ä¹‹å‰ï¼Œé¦–å…ˆå°†å¤šå±‚WindowsAttentionå †å è¾“å‡ºå¾—åˆ°çš„(nW<em>B, N, C)çŸ©é˜µå…ˆæŒ‰ç…§å°çª—å£çš„åˆ’åˆ†æ–¹å¼å°†å°çª—å£åˆå¹¶ä¸º(B, N</em>nW, C)ï¼Œç”¨è¾“å…¥å›¾ç‰‡çš„å½¢çŠ¶è¡¨ç¤ºå³ä¸º(B, H0<em>W0, C)ï¼Œç„¶åå°†å…¶å˜æ¢ä¸ºSwin Transformer Blockå±‚æœ€åˆè¾“å…¥çš„å›¾ç‰‡å½¢çŠ¶(B, H0, W0, C)ã€‚PatchMergingå±‚ä¸­é€šè¿‡é—´éš”é‡‡æ ·å¾—åˆ°4ä¸ªå½¢çŠ¶ä¸º(B, H0//2, W0//2, C)çš„çŸ©é˜µï¼Œæ‹¼æ¥å¾—åˆ°çš„(B, H0//2, W0//2, C</em>4)å†é€åˆ°å…¨è¿æ¥é™ç»´æˆ(B, H0//2, W0//2, C<em>2)çš„çŸ©é˜µã€‚è¯¥å±‚è¾“å…¥ä¸º(B, H0, W0, C)ï¼Œè¾“å‡ºä¸º(B, H0//2, W0//2, C</em>2)ä»è€Œå®ç°ä¸‹é‡‡æ ·çš„ç›®çš„ã€‚</p>
</li>
</ol>
</li>
<li>
<p>ç”±äºæœ€åä¸€å±‚Basic Blockæ²¡æœ‰ä¸‹é‡‡æ ·å±‚ï¼Œå› æ­¤ç»è¿‡4å±‚Basic Blockå¾—åˆ°çš„è¾“å‡ºä¸º(B, H0//2^3, W0//2^3, C<em>2^3)ï¼Œç»è¿‡AvgPoolè½¬åŒ–ä¸º(B, 1, C</em>2^3)çš„å½¢å¼ï¼Œç„¶åé€å…¥å…¨è¿æ¥è¿›è¡Œåˆ†ç±»ã€‚</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        <span class="variable language_">self</span>.act = act_layer()</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        <span class="variable language_">self</span>.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.act(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size, H, W</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># å°†nW*Bä¸ªçª—å£åˆå¹¶ä¸ºä¸€ä¸ª</span></span><br><span class="line">    <span class="comment"># B = B*nW/nW = (windows.shape[0]) / (H*W/window_size/window_size)</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        <span class="variable language_">self</span>.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(<span class="variable language_">self</span>.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(<span class="variable language_">self</span>.window_size[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># é¿å…æŠ¥é”™ä¿¡æ¯</span></span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&#x27;ij&#x27;</span>))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += <span class="variable language_">self</span>.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += <span class="variable language_">self</span>.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * <span class="variable language_">self</span>.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        <span class="comment"># æ³¨å†Œä¸ºä¸å¯å˜å‚æ•°ï¼Œåœ¨ä¿å­˜æ¨¡å‹æ—¶è¯¥å‚æ•°ä¹Ÿä¼šè¢«ä¿å­˜</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line">        <span class="comment"># 96 -&gt; 96*3</span></span><br><span class="line">        <span class="variable language_">self</span>.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(dim, dim)</span><br><span class="line">        <span class="variable language_">self</span>.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line">        <span class="comment"># æˆªæ–­åˆ°(\mu-3\sigma,\mu+3\sigma)ä¹‹å†…çš„æ­£æ€åˆ†å¸ƒ</span></span><br><span class="line">        trunc_normal_(<span class="variable language_">self</span>.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        <span class="variable language_">self</span>.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = <span class="variable language_">self</span>.qkv(x).reshape(B_, N, <span class="number">3</span>, <span class="variable language_">self</span>.num_heads, C // <span class="variable language_">self</span>.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># (3, B_, num_heads, N, C//num_heads)</span></span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ³¨æ„åŠ›æœºåˆ¶</span></span><br><span class="line">        q = q * <span class="variable language_">self</span>.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># (B_, num_heads, N, N)</span></span><br><span class="line"></span><br><span class="line">        relative_position_bias = <span class="variable language_">self</span>.relative_position_bias_table[<span class="variable language_">self</span>.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            <span class="variable language_">self</span>.window_size[<span class="number">0</span>] * <span class="variable language_">self</span>.window_size[<span class="number">1</span>], <span class="variable language_">self</span>.window_size[<span class="number">0</span>] * <span class="variable language_">self</span>.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># maskç”¨äºå±è”½ä¸åº”connectçš„æ³¨æ„åŠ›çŸ©é˜µéƒ¨åˆ†</span></span><br><span class="line">            attn = attn.view(B_ // nW, nW, <span class="variable language_">self</span>.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, N, N)</span><br><span class="line">            attn = <span class="variable language_">self</span>.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = <span class="variable language_">self</span>.softmax(attn)</span><br><span class="line">        <span class="comment"># å¯¹æ³¨æ„åŠ›å›¾è¿›è¡Œdropoutæ“ä½œ</span></span><br><span class="line">        attn = <span class="variable language_">self</span>.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&#x27;dim=<span class="subst">&#123;self.dim&#125;</span>, window_size=<span class="subst">&#123;self.window_size&#125;</span>, num_heads=<span class="subst">&#123;self.num_heads&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">flops</span>(<span class="params">self, N</span>):</span><br><span class="line">        <span class="comment"># è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶ä¸æ˜¯æ¯ç§’çš„æµ®ç‚¹è¿ç®—æ•°</span></span><br><span class="line">        <span class="comment"># calculate flops for 1 window with token length of N</span></span><br><span class="line">        flops = <span class="number">0</span></span><br><span class="line">        <span class="comment"># qkv = self.qkv(x)</span></span><br><span class="line">        <span class="comment"># Næ˜¯å°å—æ‹‰ç›´çš„æ•°ç›®(tokené•¿åº¦)ï¼Œdimæ˜¯æ¯ä¸ªåºåˆ—çš„é•¿åº¦</span></span><br><span class="line">        <span class="comment"># (N,dim)(dim,dim*3)-&gt;(N,3)è®¡ç®—å¤æ‚åº¦ä¸º(N*dim*dim*3)</span></span><br><span class="line">        flops += N * <span class="variable language_">self</span>.dim * <span class="number">3</span> * <span class="variable language_">self</span>.dim</span><br><span class="line">        <span class="comment"># attn = (q @ k.transpose(-2, -1))</span></span><br><span class="line">        <span class="comment"># å…ˆæ‹†åˆ†ä¸ºå¤šå¤´æ³¨æ„åŠ›ï¼Œè®¡ç®—å¤æ‚åº¦ä¸ºnum_heads*N*(dim//N)*N</span></span><br><span class="line">        <span class="comment"># (num_heads, N, dim//num_heads) (num_heads, N, dim//num_heads).T -&gt; (num_heads, N, N)</span></span><br><span class="line">        flops += <span class="variable language_">self</span>.num_heads * N * (<span class="variable language_">self</span>.dim // <span class="variable language_">self</span>.num_heads) * N</span><br><span class="line">        <span class="comment">#  x = (attn @ v)ï¼Œè®¡ç®—å¤æ‚åº¦ä¸ºnum_heads*N^2*dim//num_heads</span></span><br><span class="line">        <span class="comment"># (num_heads, N, N)(num_heads, N, dim//num_heads) -&gt; (num_heads, N, dim//num_heads)</span></span><br><span class="line">        flops += <span class="variable language_">self</span>.num_heads * N * N * (<span class="variable language_">self</span>.dim // <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        <span class="comment"># x = self.proj(x)ï¼Œè®¡ç®—å¤æ‚åº¦ä¸º(N, dim, dim)</span></span><br><span class="line">        <span class="comment"># (N, dim) -&gt; (N, dim)</span></span><br><span class="line">        flops += N * <span class="variable language_">self</span>.dim * <span class="variable language_">self</span>.dim</span><br><span class="line">        <span class="keyword">return</span> flops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer Block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Input resulotion.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Window size.</span></span><br><span class="line"><span class="string">        shift_size (int): Shift size for SW-MSA.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, input_resolution, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,</span></span><br><span class="line"><span class="params">                 fused_window_process=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size</span><br><span class="line">        <span class="variable language_">self</span>.shift_size = shift_size</span><br><span class="line">        <span class="variable language_">self</span>.mlp_ratio = mlp_ratio</span><br><span class="line">        <span class="comment"># åœ¨å°èŒƒå›´å†…åšè‡ªæ³¨æ„åŠ›</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">min</span>(<span class="variable language_">self</span>.input_resolution) &lt;= <span class="variable language_">self</span>.window_size:</span><br><span class="line">            <span class="comment"># if window size is larger than input resolution, we don&#x27;t partition windows</span></span><br><span class="line">            <span class="variable language_">self</span>.shift_size = <span class="number">0</span></span><br><span class="line">            <span class="variable language_">self</span>.window_size = <span class="built_in">min</span>(<span class="variable language_">self</span>.input_resolution)</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="variable language_">self</span>.shift_size &lt; <span class="variable language_">self</span>.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = norm_layer(dim)</span><br><span class="line">        <span class="variable language_">self</span>.attn = WindowAttention(</span><br><span class="line">            dim, window_size=to_2tuple(<span class="variable language_">self</span>.window_size), num_heads=num_heads,</span><br><span class="line">            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="comment"># ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µï¼Œå†batchç»´åº¦éšæœºè®¾ç½®ä¸€å®šæ ·æœ¬ä¸è¿›è¡Œä¸»å¹²è€Œç›´æ¥ç”±åˆ†æ”¯è¿›è¡Œæ’ç­‰æ˜ å°„</span></span><br><span class="line">        <span class="variable language_">self</span>.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">            H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">            img_mask = torch.zeros((<span class="number">1</span>, H, W, <span class="number">1</span>))  <span class="comment"># 1 H W 1</span></span><br><span class="line">            h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -<span class="variable language_">self</span>.window_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.window_size, -<span class="variable language_">self</span>.shift_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.shift_size, <span class="literal">None</span>))</span><br><span class="line">            w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -<span class="variable language_">self</span>.window_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.window_size, -<span class="variable language_">self</span>.shift_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.shift_size, <span class="literal">None</span>))</span><br><span class="line">            <span class="comment"># ç»™æ¯å—åŒºåŸŸåˆ’åˆ†æ ‡è®°åºå·ï¼Œæ¯ä¸€ä¸ªç»´åº¦åˆ†ä¸º0:-7,-7:-2,-2:ä¸‰éƒ¨åˆ†</span></span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">                    img_mask[:, h, w, :] = cnt</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">            <span class="comment"># ç”Ÿæˆæ©ç çª—å£ï¼Œ64, 7, 7, 1</span></span><br><span class="line">            mask_windows = window_partition(img_mask, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW, window_size, window_size, 1</span></span><br><span class="line">            <span class="comment"># mask_windows (64, 49)</span></span><br><span class="line">            mask_windows = mask_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size * <span class="variable language_">self</span>.window_size)</span><br><span class="line">            attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_mask = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># è·å¾—æ©ç çŸ©é˜µï¼Œå…·ä½“çœ‹è®ºæ–‡ä¸­æ‰€è¯´</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;attn_mask&quot;</span>, attn_mask)</span><br><span class="line">        <span class="variable language_">self</span>.fused_window_process = fused_window_process</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        shortcut = x</span><br><span class="line">        <span class="comment"># æ ‡å‡†åŒ–å±‚</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            shifted_x = torch.roll(x, shifts=(-<span class="variable language_">self</span>.shift_size, -<span class="variable language_">self</span>.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            <span class="comment"># partition windows</span></span><br><span class="line">            x_windows = window_partition(shifted_x, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = x</span><br><span class="line">            <span class="comment"># partition windowsï¼ŒnWä¸ºçª—å£æ•°ç›®</span></span><br><span class="line">            x_windows = window_partition(shifted_x, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">        <span class="comment"># çª—å£æ•°ç›®*æ¯ä¸ªçª—å£çš„tokené•¿åº¦ï¼Œé€šé“æ•°ç›®</span></span><br><span class="line">        x_windows = x_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size * <span class="variable language_">self</span>.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># W-MSA/SW-MSAï¼Œå¤šä¸ªçª—å£å¯ä»¥å¹¶è¡Œåœ°åšæ³¨æ„åŠ›</span></span><br><span class="line">        attn_windows = <span class="variable language_">self</span>.attn(x_windows, mask=<span class="variable language_">self</span>.attn_mask)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge windows</span></span><br><span class="line">        attn_windows = attn_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size, <span class="variable language_">self</span>.window_size, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># å°†æ‰€æœ‰å°çª—å£åˆå¹¶ä¸ºä¸€ä¸ªå¤§çª—å£</span></span><br><span class="line">            shifted_x = window_reverse(attn_windows, <span class="variable language_">self</span>.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line">            <span class="comment"># å¯¹æ•´ä¸ªçª—å£è¿›è¡Œå·¦ä¸Šçš„å¹³ç§»æ“ä½œ</span></span><br><span class="line">            x = torch.roll(shifted_x, shifts=(<span class="variable language_">self</span>.shift_size, <span class="variable language_">self</span>.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = window_reverse(attn_windows, <span class="variable language_">self</span>.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line">            x = shifted_x</span><br><span class="line">        x = x.view(B, H * W, C)</span><br><span class="line">        x = shortcut + <span class="variable language_">self</span>.drop_path(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.drop_path(<span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.norm2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># ä¸‹é‡‡æ ·ï¼Œæ¯è¡Œã€æ¯åˆ—é—´éš”é‡‡æ ·ï¼Œé€šé“æ•°æ‰©å¢åˆ°4å€ï¼Œç„¶åå†å°†å›åˆ°2å€</span></span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; A basic Swin Transformer layer for one stage.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Input resolution.</span></span><br><span class="line"><span class="string">        depth (int): Number of blocks.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Local window size.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, input_resolution, depth, num_heads, window_size,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 fused_window_process=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.depth = depth</span><br><span class="line">        <span class="variable language_">self</span>.use_checkpoint = use_checkpoint</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build blocks</span></span><br><span class="line">        <span class="variable language_">self</span>.blocks = nn.ModuleList([</span><br><span class="line">            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,</span><br><span class="line">                                 num_heads=num_heads, window_size=window_size,</span><br><span class="line">                                 shift_size=<span class="number">1</span>,</span><br><span class="line">                                 mlp_ratio=mlp_ratio,</span><br><span class="line">                                 qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                                 drop=drop, attn_drop=attn_drop,</span><br><span class="line">                                 drop_path=drop_path[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(drop_path, <span class="built_in">list</span>) <span class="keyword">else</span> drop_path,</span><br><span class="line">                                 norm_layer=norm_layer,</span><br><span class="line">                                 fused_window_process=fused_window_process)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch merging layer</span></span><br><span class="line">        <span class="keyword">if</span> downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.downsample = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> <span class="variable language_">self</span>.blocks:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_checkpoint:</span><br><span class="line">                x = checkpoint.checkpoint(blk, x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = blk(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = <span class="variable language_">self</span>.downsample(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int): Image size.  Default: 224.</span></span><br><span class="line"><span class="string">        patch_size (int): Patch token size. Default: 4.</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3.</span></span><br><span class="line"><span class="string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 224 -&gt; (224, 224)</span></span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        <span class="comment"># 4 -&gt; (4, 4)</span></span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        <span class="comment"># åˆ‡å‰²æˆ[56,56]ä¸ªå—</span></span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        <span class="variable language_">self</span>.img_size = img_size</span><br><span class="line">        <span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">        <span class="variable language_">self</span>.patches_resolution = patches_resolution</span><br><span class="line">        <span class="variable language_">self</span>.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># è¾“å…¥é€šé“æ•°ç›®ä¸º3</span></span><br><span class="line">        <span class="variable language_">self</span>.in_chans = in_chans</span><br><span class="line">        <span class="comment"># åµŒå…¥ç»´åº¦ä¸º96</span></span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">        <span class="comment"># (b,3,224,224) -&gt; (b,96,224//4,224//4)é€šé“æ•°å˜åŒ–</span></span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="comment"># å±‚å½’ä¸€åŒ–ï¼Œå¸¸ç”¨äºnlpï¼Œå› æ­¤å¯èƒ½æ¥æºäºtransformerè‡ªå¸¦çš„</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == <span class="variable language_">self</span>.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == <span class="variable language_">self</span>.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B Ph*Pw C</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer</span></span><br><span class="line"><span class="string">        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -</span></span><br><span class="line"><span class="string">          https://arxiv.org/pdf/2103.14030</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int | tuple(int)): Input image size. Default 224</span></span><br><span class="line"><span class="string">        patch_size (int | tuple(int)): Patch size. Default: 4</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3</span></span><br><span class="line"><span class="string">        num_classes (int): Number of classes for classification head. Default: 1000</span></span><br><span class="line"><span class="string">        embed_dim (int): Patch embedding dimension. Default: 96</span></span><br><span class="line"><span class="string">        depths (tuple(int)): Depth of each Swin Transformer layer.</span></span><br><span class="line"><span class="string">        num_heads (tuple(int)): Number of attention heads in different layers.</span></span><br><span class="line"><span class="string">        window_size (int): Window size. Default: 7</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4</span></span><br><span class="line"><span class="string">        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None</span></span><br><span class="line"><span class="string">        drop_rate (float): Dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        attn_drop_rate (float): Attention dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        drop_path_rate (float): Stochastic depth rate. Default: 0.1</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.</span></span><br><span class="line"><span class="string">        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False</span></span><br><span class="line"><span class="string">        patch_norm (bool): If True, add normalization after patch embedding. Default: True</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">96</span>, depths=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span>], num_heads=[<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>],</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">7</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm, ape=<span class="literal">False</span>, patch_norm=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 use_checkpoint=<span class="literal">False</span>, fused_window_process=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="built_in">len</span>(depths)  <span class="comment"># depthsæ¯ä¸€ä¸ªswin transformerå±‚çš„æ·±åº¦ï¼Œå› ä¸ºswin transformerå¯ä»¥éšæ„å †å </span></span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">        <span class="variable language_">self</span>.ape = ape  <span class="comment"># ç»å¯¹ä½ç½®ç¼–ç </span></span><br><span class="line">        <span class="variable language_">self</span>.patch_norm = patch_norm  <span class="comment"># patchå½’ä¸€åŒ–</span></span><br><span class="line">        <span class="variable language_">self</span>.num_features = <span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** (<span class="variable language_">self</span>.num_layers - <span class="number">1</span>))  <span class="comment"># ç»è¿‡æ‰€æœ‰å±‚åçš„é€šé“æ•°é‡</span></span><br><span class="line">        <span class="variable language_">self</span>.mlp_ratio = mlp_ratio  <span class="comment"># mlpéšè—å±‚ç»´åº¦ä¸åµŒå…¥ç»´åº¦çš„å•†</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># split image into non-overlapping patches</span></span><br><span class="line">        <span class="comment"># å¯¹å›¾ç‰‡è¿›è¡Œåˆ‡å‰²çš„å—</span></span><br><span class="line">        <span class="variable language_">self</span>.patch_embed = PatchEmbed(</span><br><span class="line">            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,</span><br><span class="line">            norm_layer=norm_layer <span class="keyword">if</span> <span class="variable language_">self</span>.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        num_patches = <span class="variable language_">self</span>.patch_embed.num_patches</span><br><span class="line">        patches_resolution = <span class="variable language_">self</span>.patch_embed.patches_resolution</span><br><span class="line">        <span class="variable language_">self</span>.patches_resolution = patches_resolution</span><br><span class="line"></span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.ape:</span><br><span class="line">            <span class="variable language_">self</span>.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            trunc_normal_(<span class="variable language_">self</span>.absolute_pos_embed, std=<span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stochastic depth</span></span><br><span class="line">        <span class="comment"># swin transformerå±‚æ•°ä¸€å…±ä¸º12ï¼Œå…¶ä¸­drop_path_rate=0.2ï¼Œæ¯ä¸€å±‚çš„dropoutæ¦‚ç‡ä»0å¢åŠ åˆ°0.2</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># æ¯ç»è¿‡ä¸€å±‚BasicLayerï¼Œå›¾ç‰‡å®½å’Œé«˜å‡åŠï¼ŒchannelsåŠ å€ï¼Œ96-&gt;96*2-&gt;96*4</span></span><br><span class="line">        <span class="comment"># åˆå§‹çš„å›¾ç‰‡ä¸º(b, 96, 3136)</span></span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers):</span><br><span class="line">            layer = BasicLayer(dim=<span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** i_layer),</span><br><span class="line">                               input_resolution=(patches_resolution[<span class="number">0</span>] // (<span class="number">2</span> ** i_layer),</span><br><span class="line">                                                 patches_resolution[<span class="number">1</span>] // (<span class="number">2</span> ** i_layer)),</span><br><span class="line">                               depth=depths[i_layer],  <span class="comment"># [2, 2, 6, 2]</span></span><br><span class="line">                               num_heads=num_heads[i_layer],</span><br><span class="line">                               window_size=window_size,  <span class="comment"># çª—å£å°ºå¯¸æ˜¯7</span></span><br><span class="line">                               mlp_ratio=<span class="variable language_">self</span>.mlp_ratio,</span><br><span class="line">                               qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                               drop=drop_rate, attn_drop=attn_drop_rate,</span><br><span class="line">                               drop_path=dpr[<span class="built_in">sum</span>(depths[:i_layer]):<span class="built_in">sum</span>(depths[:i_layer + <span class="number">1</span>])],</span><br><span class="line">                               norm_layer=norm_layer,</span><br><span class="line">                               downsample=PatchMerging <span class="keyword">if</span> (i_layer &lt; <span class="variable language_">self</span>.num_layers - <span class="number">1</span>) <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                               use_checkpoint=use_checkpoint,</span><br><span class="line">                               fused_window_process=fused_window_process)</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="variable language_">self</span>.num_features)</span><br><span class="line">        <span class="variable language_">self</span>.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.head = nn.Linear(<span class="variable language_">self</span>.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.apply(<span class="variable language_">self</span>._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            trunc_normal_(m.weight, std=<span class="number">.02</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.patch_embed(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.ape:</span><br><span class="line">            x = x + <span class="variable language_">self</span>.absolute_pos_embed</span><br><span class="line">        x = <span class="variable language_">self</span>.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = <span class="variable language_">self</span>.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.forward_features(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="pytorchå®˜æ–¹è®­ç»ƒå‚è€ƒä¸­ç»™å‡ºçš„ç›¸å…³ä»£ç ">pytorchå®˜æ–¹è®­ç»ƒå‚è€ƒä¸­ç»™å‡ºçš„ç›¸å…³ä»£ç </h2>
<p>å›¾ç‰‡é¢„å¤„ç†ç›¸å…³</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> autoaugment, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationPresetTrain</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        crop_size,</span></span><br><span class="line"><span class="params">        mean=(<span class="params"><span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span></span>),</span></span><br><span class="line"><span class="params">        std=(<span class="params"><span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span></span>),</span></span><br><span class="line"><span class="params">        interpolation=InterpolationMode.BILINEAR,</span></span><br><span class="line"><span class="params">        hflip_prob=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        auto_augment_policy=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        ra_magnitude=<span class="number">9</span>,</span></span><br><span class="line"><span class="params">        augmix_severity=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">        random_erase_prob=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        trans = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]</span><br><span class="line">        <span class="keyword">if</span> hflip_prob &gt; <span class="number">0</span>:</span><br><span class="line">            trans.append(transforms.RandomHorizontalFlip(hflip_prob))</span><br><span class="line">        <span class="keyword">if</span> auto_augment_policy <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> auto_augment_policy == <span class="string">&quot;ra&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.RandAugment(interpolation=interpolation, magnitude=ra_magnitude))</span><br><span class="line">            <span class="keyword">elif</span> auto_augment_policy == <span class="string">&quot;ta_wide&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.TrivialAugmentWide(interpolation=interpolation))</span><br><span class="line">            <span class="keyword">elif</span> auto_augment_policy == <span class="string">&quot;augmix&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.AugMix(interpolation=interpolation, severity=augmix_severity))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                aa_policy = autoaugment.AutoAugmentPolicy(auto_augment_policy)</span><br><span class="line">                trans.append(autoaugment.AutoAugment(policy=aa_policy, interpolation=interpolation))</span><br><span class="line">        trans.extend(</span><br><span class="line">            [</span><br><span class="line">                transforms.PILToTensor(),</span><br><span class="line">                transforms.ConvertImageDtype(torch.<span class="built_in">float</span>),</span><br><span class="line">                transforms.Normalize(mean=mean, std=std),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> random_erase_prob &gt; <span class="number">0</span>:</span><br><span class="line">            trans.append(transforms.RandomErasing(p=random_erase_prob))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transforms = transforms.Compose(trans)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transforms(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationPresetEval</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        crop_size,</span></span><br><span class="line"><span class="params">        resize_size=<span class="number">256</span>,</span></span><br><span class="line"><span class="params">        mean=(<span class="params"><span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span></span>),</span></span><br><span class="line"><span class="params">        std=(<span class="params"><span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span></span>),</span></span><br><span class="line"><span class="params">        interpolation=InterpolationMode.BILINEAR,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transforms = transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.Resize(resize_size, interpolation=interpolation),</span><br><span class="line">                transforms.CenterCrop(crop_size),</span><br><span class="line">                transforms.PILToTensor(),</span><br><span class="line">                transforms.ConvertImageDtype(torch.<span class="built_in">float</span>),</span><br><span class="line">                transforms.Normalize(mean=mean, std=std),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transforms(img)</span><br></pre></td></tr></table></figure>
<p>åˆ†å¸ƒå¼æ•°æ®é‡‡æ ·</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RASampler</span>(torch.utils.data.Sampler):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sampler that restricts data loading to a subset of the dataset for distributed,</span></span><br><span class="line"><span class="string">    with repeated augmentation.</span></span><br><span class="line"><span class="string">    It ensures that different each augmented version of a sample will be visible to a</span></span><br><span class="line"><span class="string">    different process (GPU).</span></span><br><span class="line"><span class="string">    Heavily based on &#x27;torch.utils.data.DistributedSampler&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is borrowed from the DeiT Repo:</span></span><br><span class="line"><span class="string">    https://github.com/facebookresearch/deit/blob/main/samplers.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, num_replicas=<span class="literal">None</span>, rank=<span class="literal">None</span>, shuffle=<span class="literal">True</span>, seed=<span class="number">0</span>, repetitions=<span class="number">3</span></span>):</span><br><span class="line">        <span class="keyword">if</span> num_replicas <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Requires distributed package to be available!&quot;</span>)</span><br><span class="line">            num_replicas = dist.get_world_size()</span><br><span class="line">        <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Requires distributed package to be available!&quot;</span>)</span><br><span class="line">            rank = dist.get_rank()</span><br><span class="line">        <span class="variable language_">self</span>.dataset = dataset</span><br><span class="line">        <span class="variable language_">self</span>.num_replicas = num_replicas</span><br><span class="line">        <span class="variable language_">self</span>.rank = rank</span><br><span class="line">        <span class="variable language_">self</span>.epoch = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.num_samples = <span class="built_in">int</span>(math.ceil(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset) * <span class="built_in">float</span>(repetitions) / <span class="variable language_">self</span>.num_replicas))</span><br><span class="line">        <span class="variable language_">self</span>.total_size = <span class="variable language_">self</span>.num_samples * <span class="variable language_">self</span>.num_replicas</span><br><span class="line">        <span class="variable language_">self</span>.num_selected_samples = <span class="built_in">int</span>(math.floor(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset) // <span class="number">256</span> * <span class="number">256</span> / <span class="variable language_">self</span>.num_replicas))</span><br><span class="line">        <span class="variable language_">self</span>.shuffle = shuffle</span><br><span class="line">        <span class="variable language_">self</span>.seed = seed</span><br><span class="line">        <span class="variable language_">self</span>.repetitions = repetitions</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shuffle:</span><br><span class="line">            <span class="comment"># Deterministically shuffle based on epoch</span></span><br><span class="line">            g = torch.Generator()</span><br><span class="line">            g.manual_seed(<span class="variable language_">self</span>.seed + <span class="variable language_">self</span>.epoch)</span><br><span class="line">            indices = torch.randperm(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset), generator=g).tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add extra samples to make it evenly divisible</span></span><br><span class="line">        indices = [ele <span class="keyword">for</span> ele <span class="keyword">in</span> indices <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.repetitions)]</span><br><span class="line">        indices += indices[: (<span class="variable language_">self</span>.total_size - <span class="built_in">len</span>(indices))]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(indices) == <span class="variable language_">self</span>.total_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Subsample</span></span><br><span class="line">        indices = indices[<span class="variable language_">self</span>.rank : <span class="variable language_">self</span>.total_size : <span class="variable language_">self</span>.num_replicas]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(indices) == <span class="variable language_">self</span>.num_samples</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(indices[: <span class="variable language_">self</span>.num_selected_samples])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.num_selected_samples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_epoch</span>(<span class="params">self, epoch</span>):</span><br><span class="line">        <span class="variable language_">self</span>.epoch = epoch</span><br></pre></td></tr></table></figure>
<p>æ•°æ®å¢å¼ºæªæ–½CutMixå’ŒMixUp</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomMixup</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly apply Mixup to the provided batch and targets.</span></span><br><span class="line"><span class="string">    The class implements the data augmentations as described in the paper</span></span><br><span class="line"><span class="string">    `&quot;mixup: Beyond Empirical Risk Minimization&quot; &lt;https://arxiv.org/abs/1710.09412&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes used for one-hot encoding.</span></span><br><span class="line"><span class="string">        p (float): probability of the batch being transformed. Default value is 0.5.</span></span><br><span class="line"><span class="string">        alpha (float): hyperparameter of the Beta distribution used for mixup.</span></span><br><span class="line"><span class="string">            Default value is 1.0.</span></span><br><span class="line"><span class="string">        inplace (bool): boolean to make this transform inplace. Default set to False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, p: <span class="built_in">float</span> = <span class="number">0.5</span>, alpha: <span class="built_in">float</span> = <span class="number">1.0</span>, inplace: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_classes &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Please provide a valid positive value for the num_classes. Got num_classes=<span class="subst">&#123;num_classes&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> alpha &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Alpha param can&#x27;t be zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.inplace = inplace</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Tensor, target: Tensor</span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch (Tensor): Float tensor of size (B, C, H, W)</span></span><br><span class="line"><span class="string">            target (Tensor): Integer tensor of size (B, )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Randomly transformed batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> batch.ndim != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Batch ndim should be 4. Got <span class="subst">&#123;batch.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim != <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Target ndim should be 1. Got <span class="subst">&#123;target.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch.is_floating_point():</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Batch dtype should be a float tensor. Got <span class="subst">&#123;batch.dtype&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.dtype != torch.int64:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Target dtype should be torch.int64. Got <span class="subst">&#123;target.dtype&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.inplace:</span><br><span class="line">            batch = batch.clone()</span><br><span class="line">            target = target.clone()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">1</span>:</span><br><span class="line">            target = torch.nn.functional.one_hot(target, num_classes=<span class="variable language_">self</span>.num_classes).to(dtype=batch.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.rand(<span class="number">1</span>).item() &gt;= <span class="variable language_">self</span>.p:</span><br><span class="line">            <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">        <span class="comment"># It&#x27;s faster to roll the batch by one instead of shuffling it to create image pairs</span></span><br><span class="line">        batch_rolled = batch.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        target_rolled = target.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Implemented as on mixup paper, page 3.</span></span><br><span class="line">        lambda_param = <span class="built_in">float</span>(torch._sample_dirichlet(torch.tensor([<span class="variable language_">self</span>.alpha, <span class="variable language_">self</span>.alpha]))[<span class="number">0</span>])</span><br><span class="line">        batch_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        batch.mul_(lambda_param).add_(batch_rolled)</span><br><span class="line"></span><br><span class="line">        target_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        target.mul_(lambda_param).add_(target_rolled)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s = (</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;self.__class__.__name__&#125;</span>(&quot;</span></span><br><span class="line">            <span class="string">f&quot;num_classes=<span class="subst">&#123;self.num_classes&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, p=<span class="subst">&#123;self.p&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, alpha=<span class="subst">&#123;self.alpha&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, inplace=<span class="subst">&#123;self.inplace&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;)&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomCutmix</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly apply Cutmix to the provided batch and targets.</span></span><br><span class="line"><span class="string">    The class implements the data augmentations as described in the paper</span></span><br><span class="line"><span class="string">    `&quot;CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features&quot;</span></span><br><span class="line"><span class="string">    &lt;https://arxiv.org/abs/1905.04899&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes used for one-hot encoding.</span></span><br><span class="line"><span class="string">        p (float): probability of the batch being transformed. Default value is 0.5.</span></span><br><span class="line"><span class="string">        alpha (float): hyperparameter of the Beta distribution used for cutmix.</span></span><br><span class="line"><span class="string">            Default value is 1.0.</span></span><br><span class="line"><span class="string">        inplace (bool): boolean to make this transform inplace. Default set to False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, p: <span class="built_in">float</span> = <span class="number">0.5</span>, alpha: <span class="built_in">float</span> = <span class="number">1.0</span>, inplace: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_classes &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Please provide a valid positive value for the num_classes.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> alpha &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Alpha param can&#x27;t be zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.inplace = inplace</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Tensor, target: Tensor</span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch (Tensor): Float tensor of size (B, C, H, W)</span></span><br><span class="line"><span class="string">            target (Tensor): Integer tensor of size (B, )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Randomly transformed batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> batch.ndim != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Batch ndim should be 4. Got <span class="subst">&#123;batch.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim != <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Target ndim should be 1. Got <span class="subst">&#123;target.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch.is_floating_point():</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Batch dtype should be a float tensor. Got <span class="subst">&#123;batch.dtype&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.dtype != torch.int64:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Target dtype should be torch.int64. Got <span class="subst">&#123;target.dtype&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.inplace:</span><br><span class="line">            batch = batch.clone()</span><br><span class="line">            target = target.clone()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">1</span>:</span><br><span class="line">            target = torch.nn.functional.one_hot(target, num_classes=<span class="variable language_">self</span>.num_classes).to(dtype=batch.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.rand(<span class="number">1</span>).item() &gt;= <span class="variable language_">self</span>.p:</span><br><span class="line">            <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">        <span class="comment"># It&#x27;s faster to roll the batch by one instead of shuffling it to create image pairs</span></span><br><span class="line">        batch_rolled = batch.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        target_rolled = target.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Implemented as on cutmix paper, page 12 (with minor corrections on typos).</span></span><br><span class="line">        lambda_param = <span class="built_in">float</span>(torch._sample_dirichlet(torch.tensor([<span class="variable language_">self</span>.alpha, <span class="variable language_">self</span>.alpha]))[<span class="number">0</span>])</span><br><span class="line">        _, H, W = F.get_dimensions(batch)</span><br><span class="line"></span><br><span class="line">        r_x = torch.randint(W, (<span class="number">1</span>,))</span><br><span class="line">        r_y = torch.randint(H, (<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">        r = <span class="number">0.5</span> * math.sqrt(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        r_w_half = <span class="built_in">int</span>(r * W)</span><br><span class="line">        r_h_half = <span class="built_in">int</span>(r * H)</span><br><span class="line"></span><br><span class="line">        x1 = <span class="built_in">int</span>(torch.clamp(r_x - r_w_half, <span class="built_in">min</span>=<span class="number">0</span>))</span><br><span class="line">        y1 = <span class="built_in">int</span>(torch.clamp(r_y - r_h_half, <span class="built_in">min</span>=<span class="number">0</span>))</span><br><span class="line">        x2 = <span class="built_in">int</span>(torch.clamp(r_x + r_w_half, <span class="built_in">max</span>=W))</span><br><span class="line">        y2 = <span class="built_in">int</span>(torch.clamp(r_y + r_h_half, <span class="built_in">max</span>=H))</span><br><span class="line"></span><br><span class="line">        batch[:, :, y1:y2, x1:x2] = batch_rolled[:, :, y1:y2, x1:x2]</span><br><span class="line">        lambda_param = <span class="built_in">float</span>(<span class="number">1.0</span> - (x2 - x1) * (y2 - y1) / (W * H))</span><br><span class="line"></span><br><span class="line">        target_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        target.mul_(lambda_param).add_(target_rolled)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s = (</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;self.__class__.__name__&#125;</span>(&quot;</span></span><br><span class="line">            <span class="string">f&quot;num_classes=<span class="subst">&#123;self.num_classes&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, p=<span class="subst">&#123;self.p&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, alpha=<span class="subst">&#123;self.alpha&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, inplace=<span class="subst">&#123;self.inplace&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;)&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>æ•°å€¼å¹³æ»‘ï¼Œå‡è½»è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å‡½æ•°æ•°å€¼çš„æŠ–åŠ¨</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque, OrderedDict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmoothedValue</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track a series of values and provide access to smoothed values over a</span></span><br><span class="line"><span class="string">    window or the global series average.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, window_size=<span class="number">20</span>, fmt=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> fmt <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            fmt = <span class="string">&quot;&#123;median:.4f&#125; (&#123;global_avg:.4f&#125;)&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.deque = deque(maxlen=window_size)</span><br><span class="line">        <span class="variable language_">self</span>.total = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.count = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.fmt = fmt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, value, n=<span class="number">1</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.deque.append(value)</span><br><span class="line">        <span class="variable language_">self</span>.count += n</span><br><span class="line">        <span class="variable language_">self</span>.total += value * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">synchronize_between_processes</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Warning: does not synchronize the deque!</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        t = reduce_across_processes([<span class="variable language_">self</span>.count, <span class="variable language_">self</span>.total])</span><br><span class="line">        t = t.tolist()</span><br><span class="line">        <span class="variable language_">self</span>.count = <span class="built_in">int</span>(t[<span class="number">0</span>])</span><br><span class="line">        <span class="variable language_">self</span>.total = t[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">median</span>(<span class="params">self</span>):</span><br><span class="line">        d = torch.tensor(<span class="built_in">list</span>(<span class="variable language_">self</span>.deque))</span><br><span class="line">        <span class="keyword">return</span> d.median().item()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">        d = torch.tensor(<span class="built_in">list</span>(<span class="variable language_">self</span>.deque), dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> d.mean().item()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">global_avg</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.total / <span class="variable language_">self</span>.count</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">max</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(<span class="variable language_">self</span>.deque)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">value</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.deque[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fmt.<span class="built_in">format</span>(</span><br><span class="line">            median=<span class="variable language_">self</span>.median, avg=<span class="variable language_">self</span>.avg, global_avg=<span class="variable language_">self</span>.global_avg, <span class="built_in">max</span>=<span class="variable language_">self</span>.<span class="built_in">max</span>, value=<span class="variable language_">self</span>.value</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MetricLogger</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, delimiter=<span class="string">&quot;\t&quot;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.meters = defaultdict(SmoothedValue)</span><br><span class="line">        <span class="variable language_">self</span>.delimiter = delimiter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(v, torch.Tensor):</span><br><span class="line">                v = v.item()</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">isinstance</span>(v, (<span class="built_in">float</span>, <span class="built_in">int</span>))</span><br><span class="line">            <span class="variable language_">self</span>.meters[k].update(v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getattr__</span>(<span class="params">self, attr</span>):</span><br><span class="line">        <span class="keyword">if</span> attr <span class="keyword">in</span> <span class="variable language_">self</span>.meters:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.meters[attr]</span><br><span class="line">        <span class="keyword">if</span> attr <span class="keyword">in</span> <span class="variable language_">self</span>.__dict__:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.__dict__[attr]</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(<span class="string">f&quot;&#x27;<span class="subst">&#123;<span class="built_in">type</span>(self).__name__&#125;</span>&#x27; object has no attribute &#x27;<span class="subst">&#123;attr&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        loss_str = []</span><br><span class="line">        <span class="keyword">for</span> name, meter <span class="keyword">in</span> <span class="variable language_">self</span>.meters.items():</span><br><span class="line">            loss_str.append(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(meter)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.delimiter.join(loss_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">synchronize_between_processes</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> meter <span class="keyword">in</span> <span class="variable language_">self</span>.meters.values():</span><br><span class="line">            meter.synchronize_between_processes()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_meter</span>(<span class="params">self, name, meter</span>):</span><br><span class="line">        <span class="variable language_">self</span>.meters[name] = meter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log_every</span>(<span class="params">self, iterable, print_freq, header=<span class="literal">None</span></span>):</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> header:</span><br><span class="line">            header = <span class="string">&quot;&quot;</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        end = time.time()</span><br><span class="line">        iter_time = SmoothedValue(fmt=<span class="string">&quot;&#123;avg:.4f&#125;&quot;</span>)</span><br><span class="line">        data_time = SmoothedValue(fmt=<span class="string">&quot;&#123;avg:.4f&#125;&quot;</span>)</span><br><span class="line">        space_fmt = <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="built_in">str</span>(<span class="built_in">len</span>(iterable)))) + <span class="string">&quot;d&quot;</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            log_msg = <span class="variable language_">self</span>.delimiter.join(</span><br><span class="line">                [</span><br><span class="line">                    header,</span><br><span class="line">                    <span class="string">&quot;[&#123;0&quot;</span> + space_fmt + <span class="string">&quot;&#125;/&#123;1&#125;]&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;eta: &#123;eta&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;&#123;meters&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;time: &#123;time&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;data: &#123;data&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;max mem: &#123;memory:.0f&#125;&quot;</span>,</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            log_msg = <span class="variable language_">self</span>.delimiter.join(</span><br><span class="line">                [header, <span class="string">&quot;[&#123;0&quot;</span> + space_fmt + <span class="string">&quot;&#125;/&#123;1&#125;]&quot;</span>, <span class="string">&quot;eta: &#123;eta&#125;&quot;</span>, <span class="string">&quot;&#123;meters&#125;&quot;</span>, <span class="string">&quot;time: &#123;time&#125;&quot;</span>, <span class="string">&quot;data: &#123;data&#125;&quot;</span>]</span><br><span class="line">            )</span><br><span class="line">        MB = <span class="number">1024.0</span> * <span class="number">1024.0</span></span><br><span class="line">        <span class="keyword">for</span> obj <span class="keyword">in</span> iterable:</span><br><span class="line">            data_time.update(time.time() - end)</span><br><span class="line">            <span class="keyword">yield</span> obj</span><br><span class="line">            iter_time.update(time.time() - end)</span><br><span class="line">            <span class="keyword">if</span> i % print_freq == <span class="number">0</span>:</span><br><span class="line">                eta_seconds = iter_time.global_avg * (<span class="built_in">len</span>(iterable) - i)</span><br><span class="line">                eta_string = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(eta_seconds)))</span><br><span class="line">                <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        log_msg.<span class="built_in">format</span>(</span><br><span class="line">                            i,</span><br><span class="line">                            <span class="built_in">len</span>(iterable),</span><br><span class="line">                            eta=eta_string,</span><br><span class="line">                            meters=<span class="built_in">str</span>(<span class="variable language_">self</span>),</span><br><span class="line">                            time=<span class="built_in">str</span>(iter_time),</span><br><span class="line">                            data=<span class="built_in">str</span>(data_time),</span><br><span class="line">                            memory=torch.cuda.max_memory_allocated() / MB,</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        log_msg.<span class="built_in">format</span>(</span><br><span class="line">                            i, <span class="built_in">len</span>(iterable), eta=eta_string, meters=<span class="built_in">str</span>(<span class="variable language_">self</span>), time=<span class="built_in">str</span>(iter_time), data=<span class="built_in">str</span>(data_time)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            end = time.time()</span><br><span class="line">        total_time = time.time() - start_time</span><br><span class="line">        total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;header&#125;</span> Total time: <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExponentialMovingAverage</span>(torch.optim.swa_utils.AveragedModel):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Maintains moving averages of model parameters using an exponential decay.</span></span><br><span class="line"><span class="string">    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``</span></span><br><span class="line"><span class="string">    `torch.optim.swa_utils.AveragedModel &lt;https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies&gt;`_</span></span><br><span class="line"><span class="string">    is used to compute the EMA.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, decay, device=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">ema_avg</span>(<span class="params">avg_model_param, model_param, num_averaged</span>):</span><br><span class="line">            <span class="keyword">return</span> decay * avg_model_param + (<span class="number">1</span> - decay) * model_param</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__(model, device, ema_avg, use_buffers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, target, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the accuracy over the k top predictions for the specified values of k&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        maxk = <span class="built_in">max</span>(topk)</span><br><span class="line">        batch_size = target.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">2</span>:</span><br><span class="line">            target = target.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        _, pred = output.topk(maxk, <span class="number">1</span>, <span class="literal">True</span>, <span class="literal">True</span>)</span><br><span class="line">        pred = pred.t()</span><br><span class="line">        correct = pred.eq(target[<span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">            correct_k = correct[:k].flatten().<span class="built_in">sum</span>(dtype=torch.float32)</span><br><span class="line">            res.append(correct_k * (<span class="number">100.0</span> / batch_size))</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mkdir</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">    <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> e.errno != errno.EEXIST:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_for_distributed</span>(<span class="params">is_master</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function disables printing when not in master process</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">import</span> builtins <span class="keyword">as</span> __builtin__</span><br><span class="line"></span><br><span class="line">    builtin_print = __builtin__.<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">print</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        force = kwargs.pop(<span class="string">&quot;force&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> is_master <span class="keyword">or</span> force:</span><br><span class="line">            builtin_print(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    __builtin__.<span class="built_in">print</span> = <span class="built_in">print</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_dist_avail_and_initialized</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_world_size</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> dist.get_world_size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_rank</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dist.get_rank()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_main_process</span>():</span><br><span class="line">    <span class="keyword">return</span> get_rank() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_on_master</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    <span class="keyword">if</span> is_main_process():</span><br><span class="line">        torch.save(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed_mode</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;RANK&quot;</span> <span class="keyword">in</span> os.environ <span class="keyword">and</span> <span class="string">&quot;WORLD_SIZE&quot;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&quot;WORLD_SIZE&quot;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;SLURM_PROCID&quot;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;SLURM_PROCID&quot;</span>])</span><br><span class="line">        args.gpu = args.rank % torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">hasattr</span>(args, <span class="string">&quot;rank&quot;</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Not using distributed mode&quot;</span>)</span><br><span class="line">        args.distributed = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    args.distributed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    args.dist_backend = <span class="string">&quot;nccl&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;| distributed init (rank <span class="subst">&#123;args.rank&#125;</span>): <span class="subst">&#123;args.dist_url&#125;</span>&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.distributed.init_process_group(</span><br><span class="line">        backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank</span><br><span class="line">    )</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    setup_for_distributed(args.rank == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">average_checkpoints</span>(<span class="params">inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads checkpoints from inputs and returns a model with averaged weights. Original implementation taken from:</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/fairseq/blob/a48f235636557b8d3bc4922a6fa90f3a0fa57955/scripts/average_checkpoints.py#L16</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      inputs (List[str]): An iterable of string paths of checkpoints to load from.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A dict of string keys mapping to various values. The &#x27;model&#x27; key</span></span><br><span class="line"><span class="string">      from the returned dict should correspond to an OrderedDict mapping</span></span><br><span class="line"><span class="string">      string parameter names to torch Tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    params_dict = OrderedDict()</span><br><span class="line">    params_keys = <span class="literal">None</span></span><br><span class="line">    new_state = <span class="literal">None</span></span><br><span class="line">    num_models = <span class="built_in">len</span>(inputs)</span><br><span class="line">    <span class="keyword">for</span> fpath <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            state = torch.load(</span><br><span class="line">                f,</span><br><span class="line">                map_location=(<span class="keyword">lambda</span> s, _: torch.serialization.default_restore_location(s, <span class="string">&quot;cpu&quot;</span>)),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># Copies over the settings from the first checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> new_state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            new_state = state</span><br><span class="line">        model_params = state[<span class="string">&quot;model&quot;</span>]</span><br><span class="line">        model_params_keys = <span class="built_in">list</span>(model_params.keys())</span><br><span class="line">        <span class="keyword">if</span> params_keys <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            params_keys = model_params_keys</span><br><span class="line">        <span class="keyword">elif</span> params_keys != model_params_keys:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">f&quot;For checkpoint <span class="subst">&#123;f&#125;</span>, expected list of params: <span class="subst">&#123;params_keys&#125;</span>, but found: <span class="subst">&#123;model_params_keys&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> params_keys:</span><br><span class="line">            p = model_params[k]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(p, torch.HalfTensor):</span><br><span class="line">                p = p.<span class="built_in">float</span>()</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> params_dict:</span><br><span class="line">                params_dict[k] = p.clone()</span><br><span class="line">                <span class="comment"># <span class="doctag">NOTE:</span> clone() is needed in case of p is a shared parameter</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                params_dict[k] += p</span><br><span class="line">    averaged_params = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> params_dict.items():</span><br><span class="line">        averaged_params[k] = v</span><br><span class="line">        <span class="keyword">if</span> averaged_params[k].is_floating_point():</span><br><span class="line">            averaged_params[k].div_(num_models)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            averaged_params[k] //= num_models</span><br><span class="line">    new_state[<span class="string">&quot;model&quot;</span>] = averaged_params</span><br><span class="line">    <span class="keyword">return</span> new_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">store_model_weights</span>(<span class="params">model, checkpoint_path, checkpoint_key=<span class="string">&quot;model&quot;</span>, strict=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This method can be used to prepare weights files for new models. It receives as</span></span><br><span class="line"><span class="string">    input a model architecture and a checkpoint from the training script and produces</span></span><br><span class="line"><span class="string">    a file with the weights ready for release.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples:</span></span><br><span class="line"><span class="string">        from torchvision import models as M</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Classification</span></span><br><span class="line"><span class="string">        model = M.mobilenet_v3_large(weights=None)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./class.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Quantized Classification</span></span><br><span class="line"><span class="string">        model = M.quantization.mobilenet_v3_large(weights=None, quantize=False)</span></span><br><span class="line"><span class="string">        model.fuse_model(is_qat=True)</span></span><br><span class="line"><span class="string">        model.qconfig = torch.ao.quantization.get_default_qat_qconfig(&#x27;qnnpack&#x27;)</span></span><br><span class="line"><span class="string">        _ = torch.ao.quantization.prepare_qat(model, inplace=True)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./qat.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Object Detection</span></span><br><span class="line"><span class="string">        model = M.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=None, weights_backbone=None)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./obj.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Segmentation</span></span><br><span class="line"><span class="string">        model = M.segmentation.deeplabv3_mobilenet_v3_large(weights=None, weights_backbone=None, aux_loss=True)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./segm.pth&#x27;, strict=False))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model (pytorch.nn.Module): The model on which the weights will be loaded for validation purposes.</span></span><br><span class="line"><span class="string">        checkpoint_path (str): The path of the checkpoint we will load.</span></span><br><span class="line"><span class="string">        checkpoint_key (str, optional): The key of the checkpoint where the model weights are stored.</span></span><br><span class="line"><span class="string">            Default: &quot;model&quot;.</span></span><br><span class="line"><span class="string">        strict (bool): whether to strictly enforce that the keys</span></span><br><span class="line"><span class="string">            in :attr:`state_dict` match the keys returned by this module&#x27;s</span></span><br><span class="line"><span class="string">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        output_path (str): The location where the weights are saved.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Store the new model next to the checkpoint_path</span></span><br><span class="line">    checkpoint_path = os.path.abspath(checkpoint_path)</span><br><span class="line">    output_dir = os.path.dirname(checkpoint_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Deep copy to avoid side-effects on the model object.</span></span><br><span class="line">    model = copy.deepcopy(model)</span><br><span class="line">    checkpoint = torch.load(checkpoint_path, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the weights to the model to validate that everything works</span></span><br><span class="line">    <span class="comment"># and remove unnecessary weights (such as auxiliaries, etc)</span></span><br><span class="line">    <span class="keyword">if</span> checkpoint_key == <span class="string">&quot;model_ema&quot;</span>:</span><br><span class="line">        <span class="keyword">del</span> checkpoint[checkpoint_key][<span class="string">&quot;n_averaged&quot;</span>]</span><br><span class="line">        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(checkpoint[checkpoint_key], <span class="string">&quot;module.&quot;</span>)</span><br><span class="line">    model.load_state_dict(checkpoint[checkpoint_key], strict=strict)</span><br><span class="line"></span><br><span class="line">    tmp_path = os.path.join(output_dir, <span class="built_in">str</span>(model.__hash__()))</span><br><span class="line">    torch.save(model.state_dict(), tmp_path)</span><br><span class="line"></span><br><span class="line">    sha256_hash = hashlib.sha256()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(tmp_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># Read and update hash string value in blocks of 4K</span></span><br><span class="line">        <span class="keyword">for</span> byte_block <span class="keyword">in</span> <span class="built_in">iter</span>(<span class="keyword">lambda</span>: f.read(<span class="number">4096</span>), <span class="string">b&quot;&quot;</span>):</span><br><span class="line">            sha256_hash.update(byte_block)</span><br><span class="line">        hh = sha256_hash.hexdigest()</span><br><span class="line"></span><br><span class="line">    output_path = os.path.join(output_dir, <span class="string">&quot;weights-&quot;</span> + <span class="built_in">str</span>(hh[:<span class="number">8</span>]) + <span class="string">&quot;.pth&quot;</span>)</span><br><span class="line">    os.replace(tmp_path, output_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_across_processes</span>(<span class="params">val</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="comment"># nothing to sync, but we still convert to tensor for consistency with the distributed case.</span></span><br><span class="line">        <span class="keyword">return</span> torch.tensor(val)</span><br><span class="line"></span><br><span class="line">    t = torch.tensor(val, device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    dist.barrier()</span><br><span class="line">    dist.all_reduce(t)</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_weight_decay</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model: torch.nn.Module,</span></span><br><span class="line"><span class="params">    weight_decay: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    norm_weight_decay: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    norm_classes: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">type</span>]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    custom_keys_weight_decay: <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">str</span>, <span class="built_in">float</span>]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> norm_classes:</span><br><span class="line">        norm_classes = [</span><br><span class="line">            torch.nn.modules.batchnorm._BatchNorm,</span><br><span class="line">            torch.nn.LayerNorm,</span><br><span class="line">            torch.nn.GroupNorm,</span><br><span class="line">            torch.nn.modules.instancenorm._InstanceNorm,</span><br><span class="line">            torch.nn.LocalResponseNorm,</span><br><span class="line">        ]</span><br><span class="line">    norm_classes = <span class="built_in">tuple</span>(norm_classes)</span><br><span class="line"></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;norm&quot;</span>: [],</span><br><span class="line">    &#125;</span><br><span class="line">    params_weight_decay = &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: weight_decay,</span><br><span class="line">        <span class="string">&quot;norm&quot;</span>: norm_weight_decay,</span><br><span class="line">    &#125;</span><br><span class="line">    custom_keys = []</span><br><span class="line">    <span class="keyword">if</span> custom_keys_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> key, weight_decay <span class="keyword">in</span> custom_keys_weight_decay:</span><br><span class="line">            params[key] = []</span><br><span class="line">            params_weight_decay[key] = weight_decay</span><br><span class="line">            custom_keys.append(key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_params</span>(<span class="params">module, prefix=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, p <span class="keyword">in</span> module.named_parameters(recurse=<span class="literal">False</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> p.requires_grad:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            is_custom_key = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> custom_keys:</span><br><span class="line">                target_name = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;name&#125;</span>&quot;</span> <span class="keyword">if</span> prefix != <span class="string">&quot;&quot;</span> <span class="keyword">and</span> <span class="string">&quot;.&quot;</span> <span class="keyword">in</span> key <span class="keyword">else</span> name</span><br><span class="line">                <span class="keyword">if</span> key == target_name:</span><br><span class="line">                    params[key].append(p)</span><br><span class="line">                    is_custom_key = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_custom_key:</span><br><span class="line">                <span class="keyword">if</span> norm_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(module, norm_classes):</span><br><span class="line">                    params[<span class="string">&quot;norm&quot;</span>].append(p)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    params[<span class="string">&quot;other&quot;</span>].append(p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> child_name, child_module <span class="keyword">in</span> module.named_children():</span><br><span class="line">            child_prefix = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;child_name&#125;</span>&quot;</span> <span class="keyword">if</span> prefix != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> child_name</span><br><span class="line">            _add_params(child_module, prefix=child_prefix)</span><br><span class="line"></span><br><span class="line">    _add_params(model)</span><br><span class="line"></span><br><span class="line">    param_groups = []</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(params[key]) &gt; <span class="number">0</span>:</span><br><span class="line">            param_groups.append(&#123;<span class="string">&quot;params&quot;</span>: params[key], <span class="string">&quot;weight_decay&quot;</span>: params_weight_decay[key]&#125;)</span><br><span class="line">    <span class="keyword">return</span> param_groups</span><br></pre></td></tr></table></figure>
<p>è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°ï¼Œæœ¬æ¬¡åˆ†ç±»æ²¡æœ‰ç”¨å®˜æ–¹ç»™å®šçš„ä¸»å‡½æ•°</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> default_collate</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model, criterion, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>, model_ema=<span class="literal">None</span>, scaler=<span class="literal">None</span>, clip_grad_norm=<span class="literal">None</span>, lr_warmup_epochs=<span class="number">10</span>, model_ema_steps=<span class="number">5</span></span>):</span><br><span class="line">    model.train()</span><br><span class="line">    metric_logger = MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    metric_logger.add_meter(<span class="string">&quot;lr&quot;</span>, SmoothedValue(window_size=<span class="number">1</span>, fmt=<span class="string">&quot;&#123;value&#125;&quot;</span>))</span><br><span class="line">    metric_logger.add_meter(<span class="string">&quot;img/s&quot;</span>, SmoothedValue(window_size=<span class="number">10</span>, fmt=<span class="string">&quot;&#123;value&#125;&quot;</span>))</span><br><span class="line"></span><br><span class="line">    header = <span class="string">f&quot;Epoch: [<span class="subst">&#123;epoch&#125;</span>]&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i, (image, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        image, target = image.to(device), target.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.cuda.amp.autocast(enabled=scaler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">            output = model(image)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">if</span> scaler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scaler.scale(loss).backward()</span><br><span class="line">            <span class="keyword">if</span> clip_grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># we should unscale the gradients of optimizer&#x27;s assigned params if do gradient clipping</span></span><br><span class="line">                scaler.unscale_(optimizer)</span><br><span class="line">                nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)</span><br><span class="line">            scaler.step(optimizer)</span><br><span class="line">            scaler.update()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">if</span> clip_grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> model_ema <span class="keyword">and</span> i % model_ema_steps == <span class="number">0</span>:</span><br><span class="line">            model_ema.update_parameters(model)</span><br><span class="line">            <span class="keyword">if</span> epoch &lt; lr_warmup_epochs:</span><br><span class="line">                <span class="comment"># Reset ema buffer to keep copying weights during warmup period</span></span><br><span class="line">                model_ema.n_averaged.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">        batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>])</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;acc1&quot;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;acc5&quot;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;img/s&quot;</span>].update(batch_size / (time.time() - start_time))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, criterion, data_loader, device, print_freq=<span class="number">100</span>, log_suffix=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    metric_logger = MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    header = <span class="string">f&quot;Test: <span class="subst">&#123;log_suffix&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    num_processed_samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="keyword">for</span> image, target <span class="keyword">in</span> metric_logger.log_every(data_loader, print_freq, header):</span><br><span class="line">            image = image.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            target = target.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            output = model(image)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">            acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">            <span class="comment"># FIXME need to take into account that the datasets</span></span><br><span class="line">            <span class="comment"># could have been padded in distributed setup</span></span><br><span class="line">            batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">            metric_logger.update(loss=loss.item())</span><br><span class="line">            metric_logger.meters[<span class="string">&quot;acc1&quot;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">            metric_logger.meters[<span class="string">&quot;acc5&quot;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">            num_processed_samples += batch_size</span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line"></span><br><span class="line">    num_processed_samples = reduce_across_processes(num_processed_samples)</span><br><span class="line">    <span class="keyword">if</span> (</span><br><span class="line">        <span class="built_in">hasattr</span>(data_loader.dataset, <span class="string">&quot;__len__&quot;</span>)</span><br><span class="line">        <span class="keyword">and</span> <span class="built_in">len</span>(data_loader.dataset) != num_processed_samples</span><br><span class="line">        <span class="keyword">and</span> torch.distributed.get_rank() == <span class="number">0</span></span><br><span class="line">    ):</span><br><span class="line">        <span class="comment"># See FIXME above</span></span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">f&quot;It looks like the dataset has <span class="subst">&#123;<span class="built_in">len</span>(data_loader.dataset)&#125;</span> samples, but <span class="subst">&#123;num_processed_samples&#125;</span> &quot;</span></span><br><span class="line">            <span class="string">&quot;samples were used for the validation, which might bias the results. &quot;</span></span><br><span class="line">            <span class="string">&quot;Try adjusting the batch size and / or the world size. &quot;</span></span><br><span class="line">            <span class="string">&quot;Setting the world size to 1 is always a safe bet.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;header&#125;</span> Acc@1 <span class="subst">&#123;metric_logger.acc1.global_avg:<span class="number">.3</span>f&#125;</span> Acc@5 <span class="subst">&#123;metric_logger.acc5.global_avg:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> metric_logger.acc1.global_avg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_cache_path</span>(<span class="params">filepath</span>):</span><br><span class="line">    <span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line">    h = hashlib.sha1(filepath.encode()).hexdigest()</span><br><span class="line">    cache_path = os.path.join(<span class="string">&quot;~&quot;</span>, <span class="string">&quot;.torch&quot;</span>, <span class="string">&quot;vision&quot;</span>, <span class="string">&quot;datasets&quot;</span>, <span class="string">&quot;imagefolder&quot;</span>, h[:<span class="number">10</span>] + <span class="string">&quot;.pt&quot;</span>)</span><br><span class="line">    cache_path = os.path.expanduser(cache_path)</span><br><span class="line">    <span class="keyword">return</span> cache_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">traindir, valdir, args</span>):</span><br><span class="line">    <span class="comment"># Data loading code</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading data&quot;</span>)</span><br><span class="line">    val_resize_size, val_crop_size, train_crop_size = (</span><br><span class="line">        args.val_resize_size,</span><br><span class="line">        args.val_crop_size,</span><br><span class="line">        args.train_crop_size,</span><br><span class="line">    )</span><br><span class="line">    interpolation = InterpolationMode(args.interpolation)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading training data&quot;</span>)</span><br><span class="line">    st = time.time()</span><br><span class="line">    cache_path = _get_cache_path(traindir)</span><br><span class="line">    <span class="keyword">if</span> args.cache_dataset <span class="keyword">and</span> os.path.exists(cache_path):</span><br><span class="line">        <span class="comment"># Attention, as the transforms are also cached!</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loading dataset_train from <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">        dataset, _ = torch.load(cache_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        auto_augment_policy = <span class="built_in">getattr</span>(args, <span class="string">&quot;auto_augment&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        random_erase_prob = <span class="built_in">getattr</span>(args, <span class="string">&quot;random_erase&quot;</span>, <span class="number">0.0</span>)</span><br><span class="line">        ra_magnitude = args.ra_magnitude</span><br><span class="line">        augmix_severity = args.augmix_severity</span><br><span class="line">        dataset = torchvision.datasets.ImageFolder(</span><br><span class="line">            traindir,</span><br><span class="line">            ClassificationPresetTrain(</span><br><span class="line">                crop_size=train_crop_size,</span><br><span class="line">                interpolation=interpolation,</span><br><span class="line">                auto_augment_policy=auto_augment_policy,</span><br><span class="line">                random_erase_prob=random_erase_prob,</span><br><span class="line">                ra_magnitude=ra_magnitude,</span><br><span class="line">                augmix_severity=augmix_severity,</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> args.cache_dataset:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Saving dataset_train to <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">            mkdir(os.path.dirname(cache_path))</span><br><span class="line">            save_on_master((dataset, traindir), cache_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Took&quot;</span>, time.time() - st)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading validation data&quot;</span>)</span><br><span class="line">    cache_path = _get_cache_path(valdir)</span><br><span class="line">    <span class="keyword">if</span> args.cache_dataset <span class="keyword">and</span> os.path.exists(cache_path):</span><br><span class="line">        <span class="comment"># Attention, as the transforms are also cached!</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loading dataset_test from <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">        dataset_test, _ = torch.load(cache_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> args.weights <span class="keyword">and</span> args.test_only:</span><br><span class="line">            weights = torchvision.models.get_weight(args.weights)</span><br><span class="line">            preprocessing = weights.transforms()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            preprocessing = ClassificationPresetEval(</span><br><span class="line">                crop_size=val_crop_size, resize_size=val_resize_size, interpolation=interpolation</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        dataset_test = torchvision.datasets.ImageFolder(</span><br><span class="line">            valdir,</span><br><span class="line">            preprocessing,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> args.cache_dataset:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Saving dataset_test to <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">            mkdir(os.path.dirname(cache_path))</span><br><span class="line">            save_on_master((dataset_test, valdir), cache_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Creating data loaders&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(args, <span class="string">&quot;ra_sampler&quot;</span>) <span class="keyword">and</span> args.ra_sampler:</span><br><span class="line">            train_sampler = RASampler(dataset, shuffle=<span class="literal">True</span>, repetitions=args.ra_reps)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)</span><br><span class="line">        test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_sampler = torch.utils.data.RandomSampler(dataset)</span><br><span class="line">        test_sampler = torch.utils.data.SequentialSampler(dataset_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset, dataset_test, train_sampler, test_sampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.output_dir:</span><br><span class="line">        mkdir(args.output_dir)</span><br><span class="line"></span><br><span class="line">    init_distributed_mode(args)</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">    device = torch.device(args.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_deterministic_algorithms:</span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">        torch.use_deterministic_algorithms(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    train_dir = os.path.join(args.data_path, <span class="string">&quot;train&quot;</span>)</span><br><span class="line">    val_dir = os.path.join(args.data_path, <span class="string">&quot;val&quot;</span>)</span><br><span class="line">    dataset, dataset_test, train_sampler, test_sampler = load_data(train_dir, val_dir, args)</span><br><span class="line"></span><br><span class="line">    collate_fn = <span class="literal">None</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(dataset.classes)</span><br><span class="line">    mixup_transforms = []</span><br><span class="line">    <span class="keyword">if</span> args.mixup_alpha &gt; <span class="number">0.0</span>:</span><br><span class="line">        mixup_transforms.append(RandomMixup(num_classes, p=<span class="number">1.0</span>, alpha=args.mixup_alpha))</span><br><span class="line">    <span class="keyword">if</span> args.cutmix_alpha &gt; <span class="number">0.0</span>:</span><br><span class="line">        mixup_transforms.append(RandomCutmix(num_classes, p=<span class="number">1.0</span>, alpha=args.cutmix_alpha))</span><br><span class="line">    <span class="keyword">if</span> mixup_transforms:</span><br><span class="line">        mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">            <span class="keyword">return</span> mixupcutmix(*default_collate(batch))</span><br><span class="line"></span><br><span class="line">    data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        num_workers=args.workers,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">        dataset_test, batch_size=args.batch_size, sampler=test_sampler, num_workers=args.workers, pin_memory=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Creating model&quot;</span>)</span><br><span class="line">    model = torchvision.models.get_model(args.model, weights=args.weights, num_classes=num_classes)</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.distributed <span class="keyword">and</span> args.sync_bn:</span><br><span class="line">        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)</span><br><span class="line"></span><br><span class="line">    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)</span><br><span class="line"></span><br><span class="line">    custom_keys_weight_decay = []</span><br><span class="line">    <span class="keyword">if</span> args.bias_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        custom_keys_weight_decay.append((<span class="string">&quot;bias&quot;</span>, args.bias_weight_decay))</span><br><span class="line">    <span class="keyword">if</span> args.transformer_embedding_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">&quot;class_token&quot;</span>, <span class="string">&quot;position_embedding&quot;</span>, <span class="string">&quot;relative_position_bias_table&quot;</span>]:</span><br><span class="line">            custom_keys_weight_decay.append((key, args.transformer_embedding_decay))</span><br><span class="line">    parameters = set_weight_decay(</span><br><span class="line">        model,</span><br><span class="line">        args.weight_decay,</span><br><span class="line">        norm_weight_decay=args.norm_weight_decay,</span><br><span class="line">        custom_keys_weight_decay=custom_keys_weight_decay <span class="keyword">if</span> <span class="built_in">len</span>(custom_keys_weight_decay) &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    opt_name = args.opt.lower()</span><br><span class="line">    <span class="keyword">if</span> opt_name.startswith(<span class="string">&quot;sgd&quot;</span>):</span><br><span class="line">        optimizer = torch.optim.SGD(</span><br><span class="line">            parameters,</span><br><span class="line">            lr=args.lr,</span><br><span class="line">            momentum=args.momentum,</span><br><span class="line">            weight_decay=args.weight_decay,</span><br><span class="line">            nesterov=<span class="string">&quot;nesterov&quot;</span> <span class="keyword">in</span> opt_name,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> opt_name == <span class="string">&quot;rmsprop&quot;</span>:</span><br><span class="line">        optimizer = torch.optim.RMSprop(</span><br><span class="line">            parameters, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, eps=<span class="number">0.0316</span>, alpha=<span class="number">0.9</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> opt_name == <span class="string">&quot;adamw&quot;</span>:</span><br><span class="line">        optimizer = torch.optim.AdamW(parameters, lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Invalid optimizer <span class="subst">&#123;args.opt&#125;</span>. Only SGD, RMSprop and AdamW are supported.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    scaler = torch.cuda.amp.GradScaler() <span class="keyword">if</span> args.amp <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.lr_scheduler = args.lr_scheduler.lower()</span><br><span class="line">    <span class="keyword">if</span> args.lr_scheduler == <span class="string">&quot;steplr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)</span><br><span class="line">    <span class="keyword">elif</span> args.lr_scheduler == <span class="string">&quot;cosineannealinglr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(</span><br><span class="line">            optimizer, T_max=args.epochs - args.lr_warmup_epochs, eta_min=args.lr_min</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> args.lr_scheduler == <span class="string">&quot;exponentiallr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_gamma)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">            <span class="string">f&quot;Invalid lr scheduler &#x27;<span class="subst">&#123;args.lr_scheduler&#125;</span>&#x27;. Only StepLR, CosineAnnealingLR and ExponentialLR &quot;</span></span><br><span class="line">            <span class="string">&quot;are supported.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.lr_warmup_epochs &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> args.lr_warmup_method == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">            warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(</span><br><span class="line">                optimizer, start_factor=args.lr_warmup_decay, total_iters=args.lr_warmup_epochs</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> args.lr_warmup_method == <span class="string">&quot;constant&quot;</span>:</span><br><span class="line">            warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(</span><br><span class="line">                optimizer, factor=args.lr_warmup_decay, total_iters=args.lr_warmup_epochs</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">                <span class="string">f&quot;Invalid warmup lr method &#x27;<span class="subst">&#123;args.lr_warmup_method&#125;</span>&#x27;. Only linear and constant are supported.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(</span><br><span class="line">            optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[args.lr_warmup_epochs]</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        lr_scheduler = main_lr_scheduler</span><br><span class="line"></span><br><span class="line">    model_without_ddp = model</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</span><br><span class="line">        model_without_ddp = model.module</span><br><span class="line"></span><br><span class="line">    model_ema = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> args.model_ema:</span><br><span class="line">        <span class="comment"># Decay adjustment that aims to keep the decay independent from other hyper-parameters originally proposed at:</span></span><br><span class="line">        <span class="comment"># https://github.com/facebookresearch/pycls/blob/f8cd9627/pycls/core/net.py#L123</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># total_ema_updates = (Dataset_size / n_GPUs) * epochs / (batch_size_per_gpu * EMA_steps)</span></span><br><span class="line">        <span class="comment"># We consider constant = Dataset_size for a given dataset/setup and ommit it. Thus:</span></span><br><span class="line">        <span class="comment"># adjust = 1 / total_ema_updates ~= n_GPUs * batch_size_per_gpu * EMA_steps / epochs</span></span><br><span class="line">        adjust = args.world_size * args.batch_size * args.model_ema_steps / args.epochs</span><br><span class="line">        alpha = <span class="number">1.0</span> - args.model_ema_decay</span><br><span class="line">        alpha = <span class="built_in">min</span>(<span class="number">1.0</span>, alpha * adjust)</span><br><span class="line">        model_ema = ExponentialMovingAverage(model_without_ddp, device=device, decay=<span class="number">1.0</span> - alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.resume:</span><br><span class="line">        checkpoint = torch.load(args.resume, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        model_without_ddp.load_state_dict(checkpoint[<span class="string">&quot;model&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args.test_only:</span><br><span class="line">            optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">            lr_scheduler.load_state_dict(checkpoint[<span class="string">&quot;lr_scheduler&quot;</span>])</span><br><span class="line">        args.start_epoch = checkpoint[<span class="string">&quot;epoch&quot;</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            model_ema.load_state_dict(checkpoint[<span class="string">&quot;model_ema&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> scaler:</span><br><span class="line">            scaler.load_state_dict(checkpoint[<span class="string">&quot;scaler&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.test_only:</span><br><span class="line">        <span class="comment"># We disable the cudnn benchmarking because it can noticeably affect the accuracy</span></span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            evaluate(model_ema, criterion, data_loader_test, device=device, log_suffix=<span class="string">&quot;EMA&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            evaluate(model, criterion, data_loader_test, device=device)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start training&quot;</span>)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">        <span class="keyword">if</span> args.distributed:</span><br><span class="line">            train_sampler.set_epoch(epoch)</span><br><span class="line">        train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema, scaler)</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        evaluate(model, criterion, data_loader_test, device=device)</span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            evaluate(model_ema, criterion, data_loader_test, device=device, log_suffix=<span class="string">&quot;EMA&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> args.output_dir:</span><br><span class="line">            checkpoint = &#123;</span><br><span class="line">                <span class="string">&quot;model&quot;</span>: model_without_ddp.state_dict(),</span><br><span class="line">                <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict(),</span><br><span class="line">                <span class="string">&quot;lr_scheduler&quot;</span>: lr_scheduler.state_dict(),</span><br><span class="line">                <span class="string">&quot;epoch&quot;</span>: epoch,</span><br><span class="line">                <span class="string">&quot;args&quot;</span>: args,</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> model_ema:</span><br><span class="line">                checkpoint[<span class="string">&quot;model_ema&quot;</span>] = model_ema.state_dict()</span><br><span class="line">            <span class="keyword">if</span> scaler:</span><br><span class="line">                checkpoint[<span class="string">&quot;scaler&quot;</span>] = scaler.state_dict()</span><br><span class="line">            save_on_master(checkpoint, os.path.join(args.output_dir, <span class="string">f&quot;model_<span class="subst">&#123;epoch&#125;</span>.pth&quot;</span>))</span><br><span class="line">            save_on_master(checkpoint, os.path.join(args.output_dir, <span class="string">&quot;checkpoint.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line">    total_time = time.time() - start_time</span><br><span class="line">    total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training time <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>è®­ç»ƒå’Œè¯„ä¼°</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> Compose, ToTensor, PILToTensor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data_train = MNIST(<span class="string">&#x27;.data/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=ToTensor())</span><br><span class="line">data_val = MNIST(<span class="string">&#x27;./data/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=ToTensor())</span><br><span class="line">num_classes = <span class="built_in">len</span>(data_train.classes)</span><br><span class="line">sampler_train = torch.utils.data.RandomSampler(data_train, replacement=<span class="literal">False</span>)</span><br><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    data_train, sampler=sampler_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>,</span><br><span class="line">    pin_memory=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">True</span>,  <span class="comment"># å½“ä¸è¶³ä¸€ä¸ªbatchæ—¶å€™ï¼Œä½¿ä¸¢å¼ƒåé¢ä¸€éƒ¨åˆ†è¿˜æ˜¯éšæœºå¢åŠ ä¸€éƒ¨åˆ†æ ·æœ¬</span></span><br><span class="line">)</span><br><span class="line">sampler_val = torch.utils.data.SequentialSampler(data_val)</span><br><span class="line">data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">    data_val, sampler=sampler_val,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>,</span><br><span class="line">    pin_memory=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># timmåº“é‡Œé¢è‡ªå¸¦çš„mixupï¼Œ0.8çš„MIXUPå’Œ0.8çš„CUTMIXï¼Œæ­¤å¤„æ²¡ç”¨åˆ°</span></span><br><span class="line">mixup_fn = Mixup(</span><br><span class="line">    <span class="comment"># mixup=0.8,cutmix=0.8,cutmix_minmax is None,mixup_prob=1, mixup_switch_prob=0.5, mixup_mode=&#x27;batch&#x27;, label_smoothing=0.1</span></span><br><span class="line">    mixup_alpha=<span class="number">0.8</span>, cutmix_alpha=<span class="number">0.8</span>, prob=<span class="number">0.8</span>, switch_prob=<span class="number">0.5</span>, mode=<span class="string">&#x27;batch&#x27;</span>,</span><br><span class="line">    label_smoothing=<span class="number">0.1</span>, num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model = SwinTransformer(img_size=<span class="number">28</span>, patch_size=<span class="number">7</span>, in_chans=<span class="number">1</span>, num_classes=<span class="number">10</span>, embed_dim=<span class="number">96</span>, depths=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>], num_heads=[<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>], window_size=<span class="number">2</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">True</span>, drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.</span>, ape=<span class="literal">False</span>, patch_norm=<span class="literal">True</span>, use_checkpoint=<span class="literal">False</span>, fused_window_process=<span class="literal">False</span>).to(device)</span><br><span class="line">optimizer = Nadam(model.parameters())</span><br><span class="line">loss_fn = LabelSmoothingCrossEntropy()</span><br><span class="line">lr_scheduler = CosineLRScheduler(optimizer, t_initial=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start training&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train_one_epoch(model, loss_fn, optimizer, data_loader_train, device, epoch)</span><br><span class="line">    lr_scheduler.step(epoch)</span><br><span class="line">    evaluate(model, loss_fn, data_loader_val, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">total_time = time.time() - start_time</span><br><span class="line">total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training time <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br></pre></td><td class="code"><pre><span class="line">Start training</span><br><span class="line">Epoch: [0]  [  0/468]  eta: 0:53:11  lr: 0.002  img/s: 19.040995153557684  loss: 2.3678 (2.3678)  acc1: 8.5938 (8.5938)  acc5: 49.2188 (49.2188)  time: 6.8184  data: 0.0960  max mem: 138</span><br><span class="line">Epoch: [0]  [ 10/468]  eta: 0:05:01  lr: 0.002  img/s: 3501.2711431105545  loss: 2.6789 (2.8085)  acc1: 11.7188 (12.9972)  acc5: 52.3438 (52.9830)  time: 0.6580  data: 0.0090  max mem: 190</span><br><span class="line">Epoch: [0]  [ 20/468]  eta: 0:02:43  lr: 0.002  img/s: 3585.1146043405674  loss: 2.4055 (2.5948)  acc1: 11.7188 (12.7232)  acc5: 53.9062 (56.5848)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 30/468]  eta: 0:01:53  lr: 0.002  img/s: 3649.6010441592343  loss: 2.2687 (2.4918)  acc1: 12.5000 (13.7853)  acc5: 64.0625 (59.5010)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 40/468]  eta: 0:01:27  lr: 0.002  img/s: 3546.464652336473  loss: 2.2370 (2.4254)  acc1: 17.1875 (14.8438)  acc5: 67.1875 (61.1852)  time: 0.0361  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 50/468]  eta: 0:01:11  lr: 0.002  img/s: 3591.8786094682473  loss: 2.2039 (2.3840)  acc1: 17.1875 (15.4105)  acc5: 67.1875 (62.3775)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 60/468]  eta: 0:01:00  lr: 0.002  img/s: 3516.176414340542  loss: 2.0975 (2.3266)  acc1: 21.0938 (17.5717)  acc5: 74.2188 (65.4201)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 70/468]  eta: 0:00:53  lr: 0.002  img/s: 3595.655457400995  loss: 1.9138 (2.2534)  acc1: 32.8125 (20.4005)  acc5: 88.2812 (69.0361)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 80/468]  eta: 0:00:47  lr: 0.002  img/s: 3523.9311585165738  loss: 1.6661 (2.1731)  acc1: 46.0938 (24.2091)  acc5: 92.9688 (72.1065)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 90/468]  eta: 0:00:42  lr: 0.002  img/s: 3560.978423374125  loss: 1.5143 (2.0945)  acc1: 54.6875 (27.6700)  acc5: 94.5312 (74.6738)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [100/468]  eta: 0:00:38  lr: 0.002  img/s: 3397.5516052070348  loss: 1.3297 (2.0148)  acc1: 61.7188 (31.3738)  acc5: 96.8750 (76.8487)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [110/468]  eta: 0:00:35  lr: 0.002  img/s: 3430.11246062728  loss: 1.2297 (1.9384)  acc1: 65.6250 (34.9733)  acc5: 97.6562 (78.7796)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [120/468]  eta: 0:00:32  lr: 0.002  img/s: 3561.07290346973  loss: 1.1007 (1.8659)  acc1: 75.0000 (38.4362)  acc5: 98.4375 (80.3525)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [130/468]  eta: 0:00:30  lr: 0.002  img/s: 3507.3555366825635  loss: 0.9622 (1.7933)  acc1: 80.4688 (41.7820)  acc5: 99.2188 (81.7987)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [140/468]  eta: 0:00:27  lr: 0.002  img/s: 3631.1864186675684  loss: 0.9119 (1.7340)  acc1: 82.0312 (44.5867)  acc5: 99.2188 (83.0286)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [150/468]  eta: 0:00:26  lr: 0.002  img/s: 3552.660252253206  loss: 0.8685 (1.6751)  acc1: 83.5938 (47.2941)  acc5: 99.2188 (84.1060)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [160/468]  eta: 0:00:24  lr: 0.002  img/s: 3542.01905365107  loss: 0.8460 (1.6237)  acc1: 86.7188 (49.7234)  acc5: 99.2188 (85.0398)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [170/468]  eta: 0:00:22  lr: 0.002  img/s: 3549.3250826391645  loss: 0.8027 (1.5745)  acc1: 89.0625 (52.0514)  acc5: 99.2188 (85.8872)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [180/468]  eta: 0:00:21  lr: 0.002  img/s: 3582.865593550626  loss: 0.7574 (1.5300)  acc1: 89.0625 (54.1523)  acc5: 99.2188 (86.6238)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [190/468]  eta: 0:00:20  lr: 0.002  img/s: 3565.306025952637  loss: 0.7399 (1.4878)  acc1: 89.8438 (56.1109)  acc5: 99.2188 (87.3037)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [200/468]  eta: 0:00:18  lr: 0.002  img/s: 3605.6771975069846  loss: 0.7423 (1.4527)  acc1: 89.8438 (57.7231)  acc5: 99.2188 (87.9120)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [210/468]  eta: 0:00:17  lr: 0.002  img/s: 3401.2969342954707  loss: 0.7311 (1.4173)  acc1: 90.6250 (59.3676)  acc5: 99.2188 (88.4590)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [220/468]  eta: 0:00:16  lr: 0.002  img/s: 3480.7953422632554  loss: 0.7156 (1.3896)  acc1: 91.4062 (60.6582)  acc5: 99.2188 (88.9352)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [230/468]  eta: 0:00:15  lr: 0.002  img/s: 3562.7034746370077  loss: 0.7577 (1.3609)  acc1: 89.8438 (61.9690)  acc5: 99.2188 (89.3939)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [240/468]  eta: 0:00:14  lr: 0.002  img/s: 3469.951602895553  loss: 0.6970 (1.3335)  acc1: 92.1875 (63.1905)  acc5: 100.0000 (89.8178)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [250/468]  eta: 0:00:14  lr: 0.002  img/s: 2312.135436719682  loss: 0.6869 (1.3075)  acc1: 92.9688 (64.3800)  acc5: 100.0000 (90.2141)  time: 0.0424  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [0]  [260/468]  eta: 0:00:13  lr: 0.002  img/s: 3446.49529764465  loss: 0.6936 (1.2856)  acc1: 92.9688 (65.3766)  acc5: 100.0000 (90.5741)  time: 0.0492  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [0]  [270/468]  eta: 0:00:12  lr: 0.002  img/s: 3455.8797038944317  loss: 0.6986 (1.2633)  acc1: 91.4062 (66.3947)  acc5: 100.0000 (90.9162)  time: 0.0451  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [0]  [280/468]  eta: 0:00:11  lr: 0.002  img/s: 3372.855566863935  loss: 0.6628 (1.2424)  acc1: 92.9688 (67.3349)  acc5: 100.0000 (91.2339)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [290/468]  eta: 0:00:10  lr: 0.002  img/s: 3639.0379784587644  loss: 0.6992 (1.2249)  acc1: 92.1875 (68.1352)  acc5: 100.0000 (91.5190)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [300/468]  eta: 0:00:10  lr: 0.002  img/s: 3276.9800098882383  loss: 0.7055 (1.2073)  acc1: 91.4062 (68.9369)  acc5: 100.0000 (91.7956)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [310/468]  eta: 0:00:09  lr: 0.002  img/s: 3596.450327576736  loss: 0.6860 (1.1903)  acc1: 92.1875 (69.7171)  acc5: 100.0000 (92.0418)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [320/468]  eta: 0:00:08  lr: 0.002  img/s: 3440.2452452965604  loss: 0.6749 (1.1755)  acc1: 92.9688 (70.3831)  acc5: 100.0000 (92.2873)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [330/468]  eta: 0:00:08  lr: 0.002  img/s: 3599.681596309607  loss: 0.6807 (1.1608)  acc1: 92.9688 (71.0583)  acc5: 100.0000 (92.5038)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [340/468]  eta: 0:00:07  lr: 0.002  img/s: 3264.029960907339  loss: 0.6687 (1.1463)  acc1: 93.7500 (71.7238)  acc5: 100.0000 (92.7144)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [350/468]  eta: 0:00:06  lr: 0.002  img/s: 2509.434432857657  loss: 0.6476 (1.1321)  acc1: 93.7500 (72.3669)  acc5: 100.0000 (92.9153)  time: 0.0386  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [0]  [360/468]  eta: 0:00:06  lr: 0.002  img/s: 2476.787392566006  loss: 0.6356 (1.1186)  acc1: 93.7500 (72.9787)  acc5: 100.0000 (93.1073)  time: 0.0489  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [0]  [370/468]  eta: 0:00:05  lr: 0.002  img/s: 2908.151346900747  loss: 0.6492 (1.1062)  acc1: 93.7500 (73.5428)  acc5: 100.0000 (93.2867)  time: 0.0485  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [0]  [380/468]  eta: 0:00:04  lr: 0.002  img/s: 3440.5098049268154  loss: 0.6544 (1.0938)  acc1: 93.7500 (74.0957)  acc5: 100.0000 (93.4568)  time: 0.0406  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [0]  [390/468]  eta: 0:00:04  lr: 0.002  img/s: 3392.3991482209317  loss: 0.6606 (1.0837)  acc1: 92.9688 (74.5524)  acc5: 100.0000 (93.6141)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [400/468]  eta: 0:00:03  lr: 0.002  img/s: 3594.0427104392884  loss: 0.6609 (1.0728)  acc1: 93.7500 (75.0487)  acc5: 100.0000 (93.7695)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [410/468]  eta: 0:00:03  lr: 0.002  img/s: 3482.601694365521  loss: 0.6539 (1.0631)  acc1: 92.9688 (75.4733)  acc5: 100.0000 (93.9154)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [420/468]  eta: 0:00:02  lr: 0.002  img/s: 3460.6245576490455  loss: 0.6279 (1.0527)  acc1: 95.3125 (75.9538)  acc5: 100.0000 (94.0525)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [430/468]  eta: 0:00:02  lr: 0.002  img/s: 3554.1184196589343  loss: 0.6099 (1.0425)  acc1: 95.3125 (76.4084)  acc5: 100.0000 (94.1905)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [440/468]  eta: 0:00:01  lr: 0.002  img/s: 3696.159833667238  loss: 0.6209 (1.0333)  acc1: 94.5312 (76.8282)  acc5: 100.0000 (94.3187)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [450/468]  eta: 0:00:00  lr: 0.002  img/s: 3577.42225065302  loss: 0.6355 (1.0245)  acc1: 94.5312 (77.2208)  acc5: 100.0000 (94.4429)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [460/468]  eta: 0:00:00  lr: 0.002  img/s: 3550.005699889573  loss: 0.6098 (1.0152)  acc1: 95.3125 (77.6318)  acc5: 100.0000 (94.5618)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0] Total time: 0:00:24</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5895 (0.5895)  acc1: 96.0938 (96.0938)  acc5: 100.0000 (100.0000)  time: 0.1171  data: 0.0978  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.010 Acc@5 99.930</span><br><span class="line">Epoch: [1]  [  0/468]  eta: 0:01:13  lr: 0.002  img/s: 2390.055122491953  loss: 0.6364 (0.6364)  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.1565  data: 0.1029  max mem: 190</span><br><span class="line">Epoch: [1]  [ 10/468]  eta: 0:00:21  lr: 0.002  img/s: 3516.7061566980865  loss: 0.5902 (0.6113)  acc1: 96.0938 (95.3125)  acc5: 100.0000 (99.9290)  time: 0.0476  data: 0.0096  max mem: 190</span><br><span class="line">Epoch: [1]  [ 20/468]  eta: 0:00:18  lr: 0.002  img/s: 3492.6156808660126  loss: 0.6155 (0.6309)  acc1: 94.5312 (94.4568)  acc5: 100.0000 (99.8140)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [ 30/468]  eta: 0:00:21  lr: 0.002  img/s: 1832.5428361749698  loss: 0.6361 (0.6283)  acc1: 94.5312 (94.7329)  acc5: 100.0000 (99.8236)  time: 0.0500  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [1]  [ 40/468]  eta: 0:00:20  lr: 0.002  img/s: 3413.4288221156903  loss: 0.6217 (0.6312)  acc1: 94.5312 (94.6837)  acc5: 100.0000 (99.8285)  time: 0.0549  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [1]  [ 50/468]  eta: 0:00:19  lr: 0.002  img/s: 3562.2306916503  loss: 0.6316 (0.6319)  acc1: 94.5312 (94.5925)  acc5: 100.0000 (99.7702)  time: 0.0420  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [1]  [ 60/468]  eta: 0:00:18  lr: 0.002  img/s: 3474.66773671607  loss: 0.6233 (0.6315)  acc1: 94.5312 (94.6337)  acc5: 100.0000 (99.8079)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [ 70/468]  eta: 0:00:17  lr: 0.002  img/s: 3406.649398775342  loss: 0.6054 (0.6279)  acc1: 95.3125 (94.8393)  acc5: 100.0000 (99.8349)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [ 80/468]  eta: 0:00:16  lr: 0.002  img/s: 3441.9214771124502  loss: 0.6031 (0.6286)  acc1: 95.3125 (94.7531)  acc5: 100.0000 (99.8553)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [ 90/468]  eta: 0:00:16  lr: 0.002  img/s: 3423.2448431751377  loss: 0.6496 (0.6324)  acc1: 94.5312 (94.5999)  acc5: 100.0000 (99.8369)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [100/468]  eta: 0:00:15  lr: 0.002  img/s: 3509.3960164987807  loss: 0.6414 (0.6306)  acc1: 93.7500 (94.6473)  acc5: 100.0000 (99.8453)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [110/468]  eta: 0:00:14  lr: 0.002  img/s: 3536.1166606290135  loss: 0.6056 (0.6282)  acc1: 95.3125 (94.7494)  acc5: 100.0000 (99.8592)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [120/468]  eta: 0:00:14  lr: 0.002  img/s: 3551.7201338996283  loss: 0.6056 (0.6265)  acc1: 96.0938 (94.8670)  acc5: 100.0000 (99.8580)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [130/468]  eta: 0:00:13  lr: 0.002  img/s: 3513.990038028289  loss: 0.6123 (0.6287)  acc1: 95.3125 (94.7758)  acc5: 100.0000 (99.8628)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [140/468]  eta: 0:00:13  lr: 0.002  img/s: 2365.0075857025804  loss: 0.6116 (0.6272)  acc1: 95.3125 (94.8637)  acc5: 100.0000 (99.8670)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [150/468]  eta: 0:00:13  lr: 0.002  img/s: 2116.48142804204  loss: 0.6052 (0.6264)  acc1: 96.0938 (94.9141)  acc5: 100.0000 (99.8707)  time: 0.0510  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [1]  [160/468]  eta: 0:00:13  lr: 0.002  img/s: 2346.6791619860214  loss: 0.6072 (0.6246)  acc1: 95.3125 (94.9874)  acc5: 100.0000 (99.8690)  time: 0.0622  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [1]  [170/468]  eta: 0:00:12  lr: 0.002  img/s: 3253.033634881875  loss: 0.5855 (0.6233)  acc1: 95.3125 (95.0612)  acc5: 100.0000 (99.8629)  time: 0.0514  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [1]  [180/468]  eta: 0:00:12  lr: 0.002  img/s: 3258.898336773097  loss: 0.6087 (0.6239)  acc1: 95.3125 (95.0233)  acc5: 100.0000 (99.8576)  time: 0.0408  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [1]  [190/468]  eta: 0:00:11  lr: 0.002  img/s: 3504.1734623945067  loss: 0.6345 (0.6238)  acc1: 94.5312 (95.0344)  acc5: 100.0000 (99.8527)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [200/468]  eta: 0:00:11  lr: 0.002  img/s: 3558.052024998509  loss: 0.6168 (0.6241)  acc1: 95.3125 (95.0249)  acc5: 100.0000 (99.8445)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [210/468]  eta: 0:00:10  lr: 0.002  img/s: 3303.73967410033  loss: 0.6099 (0.6240)  acc1: 95.3125 (95.0163)  acc5: 100.0000 (99.8408)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [220/468]  eta: 0:00:10  lr: 0.002  img/s: 3516.1533856844394  loss: 0.6165 (0.6246)  acc1: 94.5312 (94.9625)  acc5: 100.0000 (99.8374)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [230/468]  eta: 0:00:09  lr: 0.002  img/s: 3532.231380598979  loss: 0.6224 (0.6240)  acc1: 94.5312 (95.0081)  acc5: 100.0000 (99.8444)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [240/468]  eta: 0:00:09  lr: 0.002  img/s: 3485.9936626668746  loss: 0.6139 (0.6245)  acc1: 95.3125 (94.9721)  acc5: 100.0000 (99.8412)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [250/468]  eta: 0:00:09  lr: 0.002  img/s: 3558.971905866755  loss: 0.6313 (0.6250)  acc1: 95.3125 (94.9608)  acc5: 100.0000 (99.8381)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [260/468]  eta: 0:00:08  lr: 0.002  img/s: 3356.2403070729297  loss: 0.6108 (0.6250)  acc1: 95.3125 (94.9683)  acc5: 100.0000 (99.8414)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [270/468]  eta: 0:00:08  lr: 0.002  img/s: 3548.9027618027735  loss: 0.6029 (0.6244)  acc1: 96.0938 (94.9983)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [280/468]  eta: 0:00:07  lr: 0.002  img/s: 3527.7286478388287  loss: 0.6134 (0.6240)  acc1: 96.0938 (95.0317)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [290/468]  eta: 0:00:07  lr: 0.002  img/s: 3131.92185230342  loss: 0.6139 (0.6236)  acc1: 95.3125 (95.0279)  acc5: 100.0000 (99.8443)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [300/468]  eta: 0:00:06  lr: 0.002  img/s: 2843.4905061782665  loss: 0.6021 (0.6229)  acc1: 95.3125 (95.0607)  acc5: 100.0000 (99.8469)  time: 0.0470  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [1]  [310/468]  eta: 0:00:06  lr: 0.002  img/s: 3476.4452214905036  loss: 0.6042 (0.6231)  acc1: 95.3125 (95.0437)  acc5: 100.0000 (99.8493)  time: 0.0499  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [1]  [320/468]  eta: 0:00:06  lr: 0.002  img/s: 3499.6506808685394  loss: 0.6194 (0.6227)  acc1: 95.3125 (95.0594)  acc5: 100.0000 (99.8491)  time: 0.0403  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [1]  [330/468]  eta: 0:00:05  lr: 0.002  img/s: 3577.5414448213132  loss: 0.6167 (0.6226)  acc1: 95.3125 (95.0600)  acc5: 100.0000 (99.8513)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [340/468]  eta: 0:00:05  lr: 0.002  img/s: 3553.3421493292035  loss: 0.6114 (0.6226)  acc1: 94.5312 (95.0536)  acc5: 100.0000 (99.8534)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [350/468]  eta: 0:00:04  lr: 0.002  img/s: 3620.0947519605133  loss: 0.6088 (0.6230)  acc1: 94.5312 (95.0521)  acc5: 100.0000 (99.8442)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [360/468]  eta: 0:00:04  lr: 0.002  img/s: 3531.1394576391585  loss: 0.6052 (0.6223)  acc1: 96.0938 (95.0918)  acc5: 100.0000 (99.8442)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [370/468]  eta: 0:00:03  lr: 0.002  img/s: 3390.620891751926  loss: 0.5937 (0.6220)  acc1: 96.0938 (95.0998)  acc5: 100.0000 (99.8484)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [380/468]  eta: 0:00:03  lr: 0.002  img/s: 3559.184253618048  loss: 0.5937 (0.6215)  acc1: 95.3125 (95.1013)  acc5: 100.0000 (99.8483)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [390/468]  eta: 0:00:03  lr: 0.002  img/s: 3451.502838370396  loss: 0.6005 (0.6211)  acc1: 96.0938 (95.1247)  acc5: 100.0000 (99.8461)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [400/468]  eta: 0:00:02  lr: 0.002  img/s: 3515.7389214498544  loss: 0.6005 (0.6209)  acc1: 95.3125 (95.1391)  acc5: 100.0000 (99.8402)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [410/468]  eta: 0:00:02  lr: 0.002  img/s: 3433.4687779795863  loss: 0.6419 (0.6247)  acc1: 94.5312 (94.9856)  acc5: 100.0000 (99.8270)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [420/468]  eta: 0:00:01  lr: 0.002  img/s: 3529.1896162973385  loss: 0.6978 (0.6257)  acc1: 90.6250 (94.9358)  acc5: 100.0000 (99.8256)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [430/468]  eta: 0:00:01  lr: 0.002  img/s: 3377.545010160236  loss: 0.6389 (0.6257)  acc1: 94.5312 (94.9355)  acc5: 100.0000 (99.8242)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [440/468]  eta: 0:00:01  lr: 0.002  img/s: 3556.0723573089226  loss: 0.6291 (0.6259)  acc1: 94.5312 (94.9157)  acc5: 100.0000 (99.8228)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [450/468]  eta: 0:00:00  lr: 0.002  img/s: 3517.881372369145  loss: 0.6064 (0.6255)  acc1: 95.3125 (94.9349)  acc5: 100.0000 (99.8268)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [460/468]  eta: 0:00:00  lr: 0.002  img/s: 3552.3546592029434  loss: 0.6011 (0.6259)  acc1: 94.5312 (94.9109)  acc5: 100.0000 (99.8288)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5358 (0.5358)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1236  data: 0.0995  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 96.310 Acc@5 99.900</span><br><span class="line">Epoch: [2]  [  0/468]  eta: 0:01:13  lr: 0.0019781476007338056  img/s: 2821.315311811909  loss: 0.5854 (0.5854)  acc1: 96.0938 (96.0938)  acc5: 100.0000 (100.0000)  time: 0.1570  data: 0.1116  max mem: 190</span><br><span class="line">Epoch: [2]  [ 10/468]  eta: 0:00:22  lr: 0.0019781476007338056  img/s: 3476.2426314426316  loss: 0.6098 (0.6028)  acc1: 96.0938 (95.8097)  acc5: 100.0000 (99.9290)  time: 0.0488  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [2]  [ 20/468]  eta: 0:00:20  lr: 0.0019781476007338056  img/s: 3316.966389877422  loss: 0.6098 (0.6142)  acc1: 95.3125 (95.2381)  acc5: 100.0000 (99.8140)  time: 0.0404  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [2]  [ 30/468]  eta: 0:00:19  lr: 0.0019781476007338056  img/s: 3366.1095596671953  loss: 0.6261 (0.6167)  acc1: 94.5312 (95.1613)  acc5: 100.0000 (99.8236)  time: 0.0425  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [ 40/468]  eta: 0:00:18  lr: 0.0019781476007338056  img/s: 3201.510569910491  loss: 0.6153 (0.6128)  acc1: 95.3125 (95.4649)  acc5: 100.0000 (99.8095)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 50/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 3468.8081875803605  loss: 0.5954 (0.6109)  acc1: 96.0938 (95.6342)  acc5: 100.0000 (99.8315)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 60/468]  eta: 0:00:16  lr: 0.0019781476007338056  img/s: 3455.4570860338936  loss: 0.5947 (0.6105)  acc1: 96.0938 (95.6327)  acc5: 100.0000 (99.8335)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 70/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 2209.2453098830915  loss: 0.6161 (0.6150)  acc1: 95.3125 (95.4445)  acc5: 100.0000 (99.8129)  time: 0.0469  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [2]  [ 80/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 3254.7888546693503  loss: 0.6118 (0.6127)  acc1: 95.3125 (95.4572)  acc5: 100.0000 (99.8264)  time: 0.0529  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [2]  [ 90/468]  eta: 0:00:16  lr: 0.0019781476007338056  img/s: 3281.928012519562  loss: 0.6002 (0.6121)  acc1: 96.0938 (95.5100)  acc5: 100.0000 (99.7940)  time: 0.0450  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [2]  [100/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 3343.969205663069  loss: 0.6085 (0.6118)  acc1: 96.0938 (95.5523)  acc5: 100.0000 (99.7912)  time: 0.0400  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [2]  [110/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 2319.247087284273  loss: 0.5869 (0.6091)  acc1: 96.0938 (95.6503)  acc5: 100.0000 (99.8029)  time: 0.0431  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [120/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 2519.728123078656  loss: 0.5794 (0.6075)  acc1: 96.0938 (95.6999)  acc5: 100.0000 (99.8063)  time: 0.0471  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [130/468]  eta: 0:00:14  lr: 0.0019781476007338056  img/s: 2833.570376000169  loss: 0.5886 (0.6080)  acc1: 96.0938 (95.6763)  acc5: 100.0000 (99.8151)  time: 0.0471  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [140/468]  eta: 0:00:14  lr: 0.0019781476007338056  img/s: 3309.3399577140954  loss: 0.5939 (0.6070)  acc1: 96.0938 (95.7170)  acc5: 100.0000 (99.8227)  time: 0.0441  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [150/468]  eta: 0:00:13  lr: 0.0019781476007338056  img/s: 3178.707086017431  loss: 0.6062 (0.6089)  acc1: 96.0938 (95.6178)  acc5: 100.0000 (99.8241)  time: 0.0411  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [160/468]  eta: 0:00:13  lr: 0.0019781476007338056  img/s: 3495.025792591628  loss: 0.6187 (0.6087)  acc1: 95.3125 (95.6619)  acc5: 100.0000 (99.8205)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [170/468]  eta: 0:00:12  lr: 0.0019781476007338056  img/s: 3553.836100299203  loss: 0.6058 (0.6102)  acc1: 96.0938 (95.6232)  acc5: 100.0000 (99.8173)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [180/468]  eta: 0:00:12  lr: 0.0019781476007338056  img/s: 3498.419220518568  loss: 0.6069 (0.6115)  acc1: 95.3125 (95.5413)  acc5: 100.0000 (99.8187)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [190/468]  eta: 0:00:11  lr: 0.0019781476007338056  img/s: 3494.0022257655137  loss: 0.6069 (0.6111)  acc1: 95.3125 (95.5334)  acc5: 100.0000 (99.8282)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [200/468]  eta: 0:00:11  lr: 0.0019781476007338056  img/s: 3423.7469516861383  loss: 0.5874 (0.6098)  acc1: 96.0938 (95.5729)  acc5: 100.0000 (99.8368)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [210/468]  eta: 0:00:10  lr: 0.0019781476007338056  img/s: 3504.1963343950706  loss: 0.5874 (0.6095)  acc1: 96.0938 (95.5791)  acc5: 100.0000 (99.8408)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [220/468]  eta: 0:00:10  lr: 0.0019781476007338056  img/s: 3503.6932193434704  loss: 0.5977 (0.6110)  acc1: 95.3125 (95.5246)  acc5: 100.0000 (99.8409)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [230/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3421.2596831546884  loss: 0.6120 (0.6113)  acc1: 94.5312 (95.4917)  acc5: 100.0000 (99.8410)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [240/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3483.8672567520216  loss: 0.6175 (0.6122)  acc1: 94.5312 (95.4487)  acc5: 100.0000 (99.8476)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [250/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3258.5818543785963  loss: 0.6107 (0.6115)  acc1: 95.3125 (95.4681)  acc5: 100.0000 (99.8444)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [260/468]  eta: 0:00:08  lr: 0.0019781476007338056  img/s: 1981.9730431155099  loss: 0.6107 (0.6116)  acc1: 95.3125 (95.4622)  acc5: 100.0000 (99.8443)  time: 0.0385  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [270/468]  eta: 0:00:08  lr: 0.0019781476007338056  img/s: 3555.4600494043007  loss: 0.6134 (0.6117)  acc1: 95.3125 (95.4595)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [280/468]  eta: 0:00:07  lr: 0.0019781476007338056  img/s: 3548.410181164449  loss: 0.5971 (0.6111)  acc1: 96.0938 (95.4932)  acc5: 100.0000 (99.8443)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [290/468]  eta: 0:00:07  lr: 0.0019781476007338056  img/s: 3422.0447458664253  loss: 0.5899 (0.6107)  acc1: 96.0938 (95.4924)  acc5: 100.0000 (99.8416)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [300/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3448.1997739184053  loss: 0.6042 (0.6106)  acc1: 96.0938 (95.5124)  acc5: 100.0000 (99.8443)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [310/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3380.6714607760414  loss: 0.6186 (0.6109)  acc1: 95.3125 (95.5110)  acc5: 100.0000 (99.8468)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [320/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3389.9786070594178  loss: 0.6108 (0.6104)  acc1: 95.3125 (95.5242)  acc5: 100.0000 (99.8515)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [330/468]  eta: 0:00:05  lr: 0.0019781476007338056  img/s: 3506.7369837423335  loss: 0.5978 (0.6102)  acc1: 96.0938 (95.5249)  acc5: 100.0000 (99.8513)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [340/468]  eta: 0:00:05  lr: 0.0019781476007338056  img/s: 1529.856215929718  loss: 0.6067 (0.6103)  acc1: 95.3125 (95.5187)  acc5: 100.0000 (99.8534)  time: 0.0402  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [2]  [350/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 2602.5688343796205  loss: 0.6111 (0.6111)  acc1: 94.5312 (95.4950)  acc5: 100.0000 (99.8397)  time: 0.0504  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [2]  [360/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 3450.659845100749  loss: 0.6074 (0.6105)  acc1: 95.3125 (95.5181)  acc5: 100.0000 (99.8377)  time: 0.0477  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [2]  [370/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 3490.935119318551  loss: 0.6074 (0.6108)  acc1: 95.3125 (95.5041)  acc5: 100.0000 (99.8357)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [380/468]  eta: 0:00:03  lr: 0.0019781476007338056  img/s: 3499.1032581420964  loss: 0.5872 (0.6100)  acc1: 96.0938 (95.5504)  acc5: 100.0000 (99.8380)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [390/468]  eta: 0:00:03  lr: 0.0019781476007338056  img/s: 3348.140069473461  loss: 0.5739 (0.6090)  acc1: 96.8750 (95.5942)  acc5: 100.0000 (99.8362)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [400/468]  eta: 0:00:02  lr: 0.0019781476007338056  img/s: 3369.2571543327645  loss: 0.5725 (0.6083)  acc1: 96.8750 (95.6242)  acc5: 100.0000 (99.8363)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [410/468]  eta: 0:00:02  lr: 0.0019781476007338056  img/s: 3479.306511820821  loss: 0.5725 (0.6077)  acc1: 96.8750 (95.6490)  acc5: 100.0000 (99.8384)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [420/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 3559.3966267105125  loss: 0.5968 (0.6074)  acc1: 96.0938 (95.6632)  acc5: 100.0000 (99.8404)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [430/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 2184.8355973726834  loss: 0.5968 (0.6067)  acc1: 96.0938 (95.6805)  acc5: 100.0000 (99.8423)  time: 0.0443  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [2]  [440/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 2506.7746442045495  loss: 0.5753 (0.6061)  acc1: 96.0938 (95.7022)  acc5: 100.0000 (99.8441)  time: 0.0546  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [2]  [450/468]  eta: 0:00:00  lr: 0.0019781476007338056  img/s: 3444.5714872321314  loss: 0.5954 (0.6062)  acc1: 96.0938 (95.7023)  acc5: 100.0000 (99.8424)  time: 0.0545  data: 0.0039  max mem: 190</span><br><span class="line">Epoch: [2]  [460/468]  eta: 0:00:00  lr: 0.0019781476007338056  img/s: 3485.7446938364747  loss: 0.5979 (0.6060)  acc1: 95.3125 (95.7074)  acc5: 100.0000 (99.8424)  time: 0.0444  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [2] Total time: 0:00:19</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5610 (0.5610)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 0.1196  data: 0.0968  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 96.960 Acc@5 99.950</span><br><span class="line">Epoch: [3]  [  0/468]  eta: 0:01:21  lr: 0.001913545457642601  img/s: 2668.437331318684  loss: 0.6060 (0.6060)  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.1742  data: 0.1262  max mem: 190</span><br><span class="line">Epoch: [3]  [ 10/468]  eta: 0:00:23  lr: 0.001913545457642601  img/s: 3215.315721703509  loss: 0.6060 (0.5948)  acc1: 95.3125 (96.0227)  acc5: 100.0000 (99.9290)  time: 0.0504  data: 0.0117  max mem: 190</span><br><span class="line">Epoch: [3]  [ 20/468]  eta: 0:00:19  lr: 0.001913545457642601  img/s: 3411.194916923468  loss: 0.5929 (0.5942)  acc1: 96.0938 (96.0193)  acc5: 100.0000 (99.9256)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 30/468]  eta: 0:00:18  lr: 0.001913545457642601  img/s: 3474.3529289948488  loss: 0.5953 (0.5982)  acc1: 96.0938 (96.1190)  acc5: 100.0000 (99.8236)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 40/468]  eta: 0:00:17  lr: 0.001913545457642601  img/s: 3487.4429143065013  loss: 0.6071 (0.5976)  acc1: 95.3125 (96.0938)  acc5: 100.0000 (99.8666)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 50/468]  eta: 0:00:16  lr: 0.001913545457642601  img/s: 3424.161848088833  loss: 0.5653 (0.5907)  acc1: 97.6562 (96.3848)  acc5: 100.0000 (99.8928)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 60/468]  eta: 0:00:16  lr: 0.001913545457642601  img/s: 3444.3946929453127  loss: 0.5667 (0.5880)  acc1: 96.8750 (96.4267)  acc5: 100.0000 (99.9103)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 70/468]  eta: 0:00:15  lr: 0.001913545457642601  img/s: 3553.5538257876624  loss: 0.5666 (0.5839)  acc1: 96.8750 (96.6329)  acc5: 100.0000 (99.9120)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 80/468]  eta: 0:00:15  lr: 0.001913545457642601  img/s: 3473.8358687001364  loss: 0.5538 (0.5815)  acc1: 97.6562 (96.7207)  acc5: 100.0000 (99.9228)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 90/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3484.319466258226  loss: 0.5678 (0.5810)  acc1: 96.8750 (96.7119)  acc5: 100.0000 (99.9227)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [100/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3542.579987858632  loss: 0.5647 (0.5788)  acc1: 96.8750 (96.7899)  acc5: 100.0000 (99.9149)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [110/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 2912.3950960182274  loss: 0.5647 (0.5789)  acc1: 96.8750 (96.7483)  acc5: 100.0000 (99.9226)  time: 0.0464  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [3]  [120/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3437.117709573746  loss: 0.5790 (0.5800)  acc1: 96.0938 (96.7265)  acc5: 100.0000 (99.9290)  time: 0.0512  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [3]  [130/468]  eta: 0:00:13  lr: 0.001913545457642601  img/s: 3497.894972765891  loss: 0.6027 (0.5813)  acc1: 96.0938 (96.6961)  acc5: 100.0000 (99.9165)  time: 0.0423  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [3]  [140/468]  eta: 0:00:13  lr: 0.001913545457642601  img/s: 3335.4720610345557  loss: 0.5973 (0.5822)  acc1: 96.0938 (96.6201)  acc5: 100.0000 (99.9113)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [150/468]  eta: 0:00:12  lr: 0.001913545457642601  img/s: 3523.792381002389  loss: 0.5937 (0.5826)  acc1: 96.0938 (96.6215)  acc5: 100.0000 (99.9120)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [160/468]  eta: 0:00:12  lr: 0.001913545457642601  img/s: 3514.9793240712856  loss: 0.6063 (0.5881)  acc1: 96.0938 (96.4431)  acc5: 100.0000 (99.8932)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [170/468]  eta: 0:00:11  lr: 0.001913545457642601  img/s: 3534.068262755656  loss: 0.6085 (0.5878)  acc1: 96.0938 (96.4638)  acc5: 100.0000 (99.8995)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [180/468]  eta: 0:00:11  lr: 0.001913545457642601  img/s: 3485.1111154387945  loss: 0.5896 (0.5887)  acc1: 96.8750 (96.4132)  acc5: 100.0000 (99.9050)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [190/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3553.4832641660546  loss: 0.5913 (0.5890)  acc1: 95.3125 (96.3719)  acc5: 100.0000 (99.9059)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [200/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3421.3905019245967  loss: 0.5847 (0.5893)  acc1: 95.3125 (96.3386)  acc5: 100.0000 (99.9067)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [210/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3595.2701922626184  loss: 0.5847 (0.5894)  acc1: 96.0938 (96.3344)  acc5: 100.0000 (99.9111)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [220/468]  eta: 0:00:09  lr: 0.001913545457642601  img/s: 3537.258275353152  loss: 0.5805 (0.5894)  acc1: 96.8750 (96.3518)  acc5: 100.0000 (99.9152)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [230/468]  eta: 0:00:09  lr: 0.001913545457642601  img/s: 3386.7281008314303  loss: 0.5732 (0.5886)  acc1: 96.8750 (96.3846)  acc5: 100.0000 (99.9154)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [240/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3616.8023821393444  loss: 0.5657 (0.5874)  acc1: 96.8750 (96.4341)  acc5: 100.0000 (99.9157)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [250/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3618.533177863002  loss: 0.5598 (0.5867)  acc1: 96.8750 (96.4704)  acc5: 100.0000 (99.9128)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [260/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3545.551224731048  loss: 0.5767 (0.5865)  acc1: 96.8750 (96.4559)  acc5: 100.0000 (99.9162)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [270/468]  eta: 0:00:07  lr: 0.001913545457642601  img/s: 3522.613213303851  loss: 0.5849 (0.5870)  acc1: 96.0938 (96.4310)  acc5: 100.0000 (99.9193)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [280/468]  eta: 0:00:07  lr: 0.001913545457642601  img/s: 3410.609813737199  loss: 0.5820 (0.5864)  acc1: 96.8750 (96.4552)  acc5: 100.0000 (99.9166)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [290/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3503.327408219464  loss: 0.5733 (0.5860)  acc1: 96.8750 (96.4696)  acc5: 100.0000 (99.9168)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [300/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3625.130232212671  loss: 0.5620 (0.5857)  acc1: 96.8750 (96.4805)  acc5: 100.0000 (99.9195)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [310/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3454.3232016471497  loss: 0.5660 (0.5855)  acc1: 96.8750 (96.4957)  acc5: 100.0000 (99.9196)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [320/468]  eta: 0:00:05  lr: 0.001913545457642601  img/s: 3379.0330746524173  loss: 0.6028 (0.5871)  acc1: 96.0938 (96.4393)  acc5: 100.0000 (99.9124)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [330/468]  eta: 0:00:05  lr: 0.001913545457642601  img/s: 3490.503884688152  loss: 0.6150 (0.5877)  acc1: 95.3125 (96.4171)  acc5: 100.0000 (99.9103)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [340/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3462.0081379977432  loss: 0.5826 (0.5875)  acc1: 96.0938 (96.4214)  acc5: 100.0000 (99.9084)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [350/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3005.1716606306222  loss: 0.5865 (0.5878)  acc1: 96.0938 (96.4009)  acc5: 100.0000 (99.9110)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [360/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3514.818239549576  loss: 0.5899 (0.5879)  acc1: 96.0938 (96.4140)  acc5: 100.0000 (99.9134)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [370/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 3496.049959300622  loss: 0.5930 (0.5880)  acc1: 96.0938 (96.3907)  acc5: 100.0000 (99.9137)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [380/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 3580.0702315935478  loss: 0.6009 (0.5906)  acc1: 94.5312 (96.2803)  acc5: 100.0000 (99.9118)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [390/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 2281.0239118981663  loss: 0.6759 (0.5998)  acc1: 92.1875 (95.9359)  acc5: 100.0000 (99.8441)  time: 0.0419  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [3]  [400/468]  eta: 0:00:02  lr: 0.001913545457642601  img/s: 3228.949118290952  loss: 0.8094 (0.6080)  acc1: 86.7188 (95.6223)  acc5: 99.2188 (99.8208)  time: 0.0525  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [3]  [410/468]  eta: 0:00:02  lr: 0.001913545457642601  img/s: 3199.450015196572  loss: 0.7277 (0.6102)  acc1: 89.0625 (95.5254)  acc5: 100.0000 (99.8213)  time: 0.0491  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [3]  [420/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3448.9086237023334  loss: 0.6970 (0.6125)  acc1: 91.4062 (95.4313)  acc5: 100.0000 (99.8126)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [430/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3289.6703533722634  loss: 0.6822 (0.6141)  acc1: 92.1875 (95.3705)  acc5: 100.0000 (99.8133)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [440/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3540.1505552185267  loss: 0.6592 (0.6149)  acc1: 93.7500 (95.3426)  acc5: 100.0000 (99.8104)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [450/468]  eta: 0:00:00  lr: 0.001913545457642601  img/s: 3581.431529512221  loss: 0.6415 (0.6156)  acc1: 93.7500 (95.3090)  acc5: 100.0000 (99.8095)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [460/468]  eta: 0:00:00  lr: 0.001913545457642601  img/s: 3241.6413290906125  loss: 0.6232 (0.6155)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (99.8119)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5786 (0.5786)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1213  data: 0.0950  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.390 Acc@5 99.820</span><br><span class="line">Epoch: [4]  [  0/468]  eta: 0:01:13  lr: 0.0018090169943749475  img/s: 2566.57445811701  loss: 0.6653 (0.6653)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.1563  data: 0.1064  max mem: 190</span><br><span class="line">Epoch: [4]  [ 10/468]  eta: 0:00:24  lr: 0.0018090169943749475  img/s: 3494.7982814737666  loss: 0.6109 (0.6174)  acc1: 96.0938 (95.3125)  acc5: 100.0000 (99.8580)  time: 0.0527  data: 0.0101  max mem: 190</span><br><span class="line">Epoch: [4]  [ 20/468]  eta: 0:00:21  lr: 0.0018090169943749475  img/s: 3517.005646904684  loss: 0.6020 (0.6088)  acc1: 96.0938 (95.7217)  acc5: 100.0000 (99.9256)  time: 0.0418  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [4]  [ 30/468]  eta: 0:00:19  lr: 0.0018090169943749475  img/s: 3463.392825117893  loss: 0.5943 (0.6025)  acc1: 96.0938 (95.9173)  acc5: 100.0000 (99.9244)  time: 0.0390  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [4]  [ 40/468]  eta: 0:00:18  lr: 0.0018090169943749475  img/s: 3438.5706453513694  loss: 0.5790 (0.5999)  acc1: 96.0938 (95.9794)  acc5: 100.0000 (99.9428)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [ 50/468]  eta: 0:00:17  lr: 0.0018090169943749475  img/s: 3453.900964365442  loss: 0.5844 (0.5983)  acc1: 96.0938 (96.0478)  acc5: 100.0000 (99.9081)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [ 60/468]  eta: 0:00:16  lr: 0.0018090169943749475  img/s: 3443.908602219514  loss: 0.6074 (0.6035)  acc1: 95.3125 (95.7992)  acc5: 100.0000 (99.8975)  time: 0.0384  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [4]  [ 70/468]  eta: 0:00:16  lr: 0.0018090169943749475  img/s: 3476.0850777289297  loss: 0.6247 (0.6063)  acc1: 94.5312 (95.7416)  acc5: 100.0000 (99.9010)  time: 0.0386  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [4]  [ 80/468]  eta: 0:00:15  lr: 0.0018090169943749475  img/s: 3564.5721949632502  loss: 0.6247 (0.6091)  acc1: 94.5312 (95.6019)  acc5: 100.0000 (99.8939)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [ 90/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3499.81037809648  loss: 0.6081 (0.6080)  acc1: 95.3125 (95.6731)  acc5: 100.0000 (99.8884)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [100/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3433.578572387902  loss: 0.6030 (0.6081)  acc1: 95.3125 (95.6142)  acc5: 100.0000 (99.8917)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [110/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3602.9912151778103  loss: 0.6003 (0.6076)  acc1: 94.5312 (95.6011)  acc5: 100.0000 (99.8944)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [120/468]  eta: 0:00:13  lr: 0.0018090169943749475  img/s: 3575.6114766763462  loss: 0.5878 (0.6064)  acc1: 96.0938 (95.6353)  acc5: 100.0000 (99.9032)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [130/468]  eta: 0:00:13  lr: 0.0018090169943749475  img/s: 3571.3301048374224  loss: 0.5812 (0.6045)  acc1: 96.8750 (95.7180)  acc5: 100.0000 (99.9046)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [140/468]  eta: 0:00:12  lr: 0.0018090169943749475  img/s: 3436.017817828068  loss: 0.5775 (0.6025)  acc1: 96.8750 (95.7890)  acc5: 100.0000 (99.9058)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [150/468]  eta: 0:00:12  lr: 0.0018090169943749475  img/s: 3567.675283422602  loss: 0.5696 (0.6018)  acc1: 96.8750 (95.8506)  acc5: 100.0000 (99.9069)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [160/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3576.2307456601966  loss: 0.5856 (0.6003)  acc1: 96.8750 (95.9045)  acc5: 100.0000 (99.9078)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [170/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 2341.4799443492216  loss: 0.5834 (0.5991)  acc1: 96.8750 (95.9430)  acc5: 100.0000 (99.9086)  time: 0.0456  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [180/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3425.844300372658  loss: 0.5871 (0.5986)  acc1: 96.0938 (95.9384)  acc5: 100.0000 (99.9007)  time: 0.0495  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [4]  [190/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3642.025045790652  loss: 0.5852 (0.5970)  acc1: 96.0938 (96.0079)  acc5: 100.0000 (99.9059)  time: 0.0403  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [4]  [200/468]  eta: 0:00:10  lr: 0.0018090169943749475  img/s: 3513.277177185038  loss: 0.5691 (0.5967)  acc1: 96.8750 (96.0199)  acc5: 100.0000 (99.9028)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [210/468]  eta: 0:00:10  lr: 0.0018090169943749475  img/s: 3409.310302783987  loss: 0.5779 (0.5966)  acc1: 96.0938 (96.0086)  acc5: 100.0000 (99.9000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [220/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 2335.2163617541387  loss: 0.5842 (0.5966)  acc1: 96.0938 (96.0230)  acc5: 100.0000 (99.9010)  time: 0.0430  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [4]  [230/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 2321.1723311989554  loss: 0.5957 (0.5973)  acc1: 96.0938 (96.0058)  acc5: 100.0000 (99.8985)  time: 0.0547  data: 0.0040  max mem: 190</span><br><span class="line">Epoch: [4]  [240/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 3229.8426923030647  loss: 0.5963 (0.5970)  acc1: 96.0938 (96.0095)  acc5: 100.0000 (99.8930)  time: 0.0570  data: 0.0054  max mem: 190</span><br><span class="line">Epoch: [4]  [250/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3209.798589023078  loss: 0.5701 (0.5961)  acc1: 96.8750 (96.0471)  acc5: 100.0000 (99.8911)  time: 0.0471  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [4]  [260/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3546.6755101636354  loss: 0.5701 (0.6003)  acc1: 96.8750 (95.8752)  acc5: 100.0000 (99.8743)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [270/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3492.1840309623703  loss: 0.6600 (0.6041)  acc1: 92.9688 (95.7074)  acc5: 100.0000 (99.8645)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [280/468]  eta: 0:00:07  lr: 0.0018090169943749475  img/s: 3504.7453519949863  loss: 0.6688 (0.6056)  acc1: 92.1875 (95.6350)  acc5: 100.0000 (99.8638)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [290/468]  eta: 0:00:07  lr: 0.0018090169943749475  img/s: 3551.9786168432056  loss: 0.6537 (0.6070)  acc1: 93.7500 (95.5944)  acc5: 100.0000 (99.8523)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [300/468]  eta: 0:00:06  lr: 0.0018090169943749475  img/s: 3557.674775521023  loss: 0.6653 (0.6094)  acc1: 93.7500 (95.5046)  acc5: 99.2188 (99.8391)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [310/468]  eta: 0:00:06  lr: 0.0018090169943749475  img/s: 3551.7906255168537  loss: 0.6758 (0.6117)  acc1: 92.9688 (95.4130)  acc5: 100.0000 (99.8342)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [320/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3595.39057874928  loss: 0.6588 (0.6142)  acc1: 92.9688 (95.3149)  acc5: 100.0000 (99.8248)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [330/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3503.7846840614516  loss: 0.6509 (0.6150)  acc1: 93.7500 (95.2960)  acc5: 100.0000 (99.8230)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [340/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3458.952349045177  loss: 0.6186 (0.6149)  acc1: 95.3125 (95.3033)  acc5: 100.0000 (99.8236)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [350/468]  eta: 0:00:04  lr: 0.0018090169943749475  img/s: 3440.5318533993836  loss: 0.5989 (0.6143)  acc1: 96.0938 (95.3281)  acc5: 100.0000 (99.8264)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [360/468]  eta: 0:00:04  lr: 0.0018090169943749475  img/s: 3385.6815685087436  loss: 0.5974 (0.6144)  acc1: 96.0938 (95.3276)  acc5: 100.0000 (99.8247)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [370/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3553.3421493292035  loss: 0.5979 (0.6139)  acc1: 96.0938 (95.3651)  acc5: 100.0000 (99.8273)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [380/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3550.9214244139903  loss: 0.5998 (0.6138)  acc1: 96.0938 (95.3494)  acc5: 100.0000 (99.8319)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [390/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3523.838638959266  loss: 0.6247 (0.6144)  acc1: 94.5312 (95.3165)  acc5: 100.0000 (99.8342)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [400/468]  eta: 0:00:02  lr: 0.0018090169943749475  img/s: 3421.870256351422  loss: 0.6127 (0.6145)  acc1: 94.5312 (95.3281)  acc5: 100.0000 (99.8266)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [410/468]  eta: 0:00:02  lr: 0.0018090169943749475  img/s: 3300.7132484491526  loss: 0.6100 (0.6147)  acc1: 94.5312 (95.3125)  acc5: 100.0000 (99.8251)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [420/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3615.414067813731  loss: 0.6168 (0.6146)  acc1: 95.3125 (95.3106)  acc5: 100.0000 (99.8200)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [430/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3585.21036955912  loss: 0.5991 (0.6142)  acc1: 95.3125 (95.3252)  acc5: 100.0000 (99.8187)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [440/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3062.4962893243205  loss: 0.5911 (0.6138)  acc1: 95.3125 (95.3444)  acc5: 100.0000 (99.8211)  time: 0.0415  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [4]  [450/468]  eta: 0:00:00  lr: 0.0018090169943749475  img/s: 2291.2944530513128  loss: 0.5909 (0.6136)  acc1: 95.3125 (95.3419)  acc5: 100.0000 (99.8233)  time: 0.0487  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [4]  [460/468]  eta: 0:00:00  lr: 0.0018090169943749475  img/s: 3341.2845069020027  loss: 0.5903 (0.6131)  acc1: 95.3125 (95.3498)  acc5: 100.0000 (99.8271)  time: 0.0447  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [4] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.6161 (0.6161)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 0.1243  data: 0.0979  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.730 Acc@5 99.930</span><br><span class="line">Epoch: [5]  [  0/468]  eta: 0:01:14  lr: 0.0016691306063588583  img/s: 2564.9313560617643  loss: 0.6228 (0.6228)  acc1: 96.0938 (96.0938)  acc5: 99.2188 (99.2188)  time: 0.1600  data: 0.1101  max mem: 190</span><br><span class="line">Epoch: [5]  [ 10/468]  eta: 0:00:24  lr: 0.0016691306063588583  img/s: 3533.300725257657  loss: 0.6159 (0.6111)  acc1: 96.0938 (95.5966)  acc5: 100.0000 (99.7869)  time: 0.0539  data: 0.0111  max mem: 190</span><br><span class="line">Epoch: [5]  [ 20/468]  eta: 0:00:21  lr: 0.0016691306063588583  img/s: 3536.978628086542  loss: 0.5970 (0.5997)  acc1: 96.0938 (95.9077)  acc5: 100.0000 (99.8140)  time: 0.0420  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [5]  [ 30/468]  eta: 0:00:19  lr: 0.0016691306063588583  img/s: 3382.5245370749562  loss: 0.5810 (0.6025)  acc1: 96.0938 (95.8417)  acc5: 100.0000 (99.7984)  time: 0.0390  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [5]  [ 40/468]  eta: 0:00:18  lr: 0.0016691306063588583  img/s: 3408.271406805485  loss: 0.6031 (0.6021)  acc1: 95.3125 (95.8460)  acc5: 100.0000 (99.8285)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 50/468]  eta: 0:00:17  lr: 0.0016691306063588583  img/s: 3385.361330760597  loss: 0.5901 (0.6014)  acc1: 96.0938 (95.8946)  acc5: 100.0000 (99.8468)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 60/468]  eta: 0:00:16  lr: 0.0016691306063588583  img/s: 3415.5135444632474  loss: 0.5825 (0.5997)  acc1: 96.8750 (96.0041)  acc5: 100.0000 (99.8719)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 70/468]  eta: 0:00:16  lr: 0.0016691306063588583  img/s: 3336.840315242523  loss: 0.5662 (0.5967)  acc1: 97.6562 (96.0827)  acc5: 100.0000 (99.8790)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 80/468]  eta: 0:00:15  lr: 0.0016691306063588583  img/s: 3558.1935141798613  loss: 0.5666 (0.5931)  acc1: 97.6562 (96.2288)  acc5: 100.0000 (99.8939)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 90/468]  eta: 0:00:15  lr: 0.0016691306063588583  img/s: 3518.388570679599  loss: 0.5748 (0.5911)  acc1: 96.8750 (96.2826)  acc5: 100.0000 (99.9056)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [100/468]  eta: 0:00:14  lr: 0.0016691306063588583  img/s: 3507.9972295186944  loss: 0.5749 (0.5911)  acc1: 96.0938 (96.2871)  acc5: 100.0000 (99.9149)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [110/468]  eta: 0:00:14  lr: 0.0016691306063588583  img/s: 3445.898023106547  loss: 0.5652 (0.5887)  acc1: 96.8750 (96.3612)  acc5: 100.0000 (99.9155)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [120/468]  eta: 0:00:13  lr: 0.0016691306063588583  img/s: 3503.6703539101095  loss: 0.5570 (0.5868)  acc1: 98.4375 (96.4941)  acc5: 100.0000 (99.9161)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [130/468]  eta: 0:00:13  lr: 0.0016691306063588583  img/s: 3515.8770653376905  loss: 0.5673 (0.5860)  acc1: 97.6562 (96.5649)  acc5: 100.0000 (99.9105)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [140/468]  eta: 0:00:12  lr: 0.0016691306063588583  img/s: 3546.5115074646583  loss: 0.5682 (0.5845)  acc1: 97.6562 (96.6312)  acc5: 100.0000 (99.9169)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [150/468]  eta: 0:00:12  lr: 0.0016691306063588583  img/s: 3463.392825117893  loss: 0.5682 (0.5837)  acc1: 96.8750 (96.6474)  acc5: 100.0000 (99.9224)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [160/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3087.843005532997  loss: 0.5680 (0.5827)  acc1: 96.8750 (96.7052)  acc5: 100.0000 (99.9175)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [170/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3348.808373410181  loss: 0.5629 (0.5817)  acc1: 97.6562 (96.7608)  acc5: 100.0000 (99.9132)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [180/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3404.726617792547  loss: 0.5687 (0.5814)  acc1: 97.6562 (96.7800)  acc5: 100.0000 (99.9050)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [190/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3387.732525634958  loss: 0.5621 (0.5802)  acc1: 97.6562 (96.8136)  acc5: 100.0000 (99.9018)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [200/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3349.100840283713  loss: 0.5525 (0.5793)  acc1: 96.8750 (96.8595)  acc5: 100.0000 (99.8989)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [210/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3505.157228105454  loss: 0.5715 (0.5793)  acc1: 96.8750 (96.8269)  acc5: 100.0000 (99.9037)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [220/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 2838.7243924620884  loss: 0.5872 (0.5800)  acc1: 96.8750 (96.8149)  acc5: 100.0000 (99.9081)  time: 0.0469  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [5]  [230/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 3548.808926375907  loss: 0.5927 (0.5808)  acc1: 96.0938 (96.7803)  acc5: 100.0000 (99.9087)  time: 0.0489  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [5]  [240/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 3558.052024998509  loss: 0.5956 (0.5810)  acc1: 96.0938 (96.7680)  acc5: 100.0000 (99.9060)  time: 0.0394  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [5]  [250/468]  eta: 0:00:08  lr: 0.0016691306063588583  img/s: 3512.0361100571745  loss: 0.5814 (0.5812)  acc1: 96.8750 (96.7661)  acc5: 100.0000 (99.9035)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [260/468]  eta: 0:00:08  lr: 0.0016691306063588583  img/s: 3292.7977233139522  loss: 0.5690 (0.5806)  acc1: 96.8750 (96.7762)  acc5: 100.0000 (99.9072)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [270/468]  eta: 0:00:07  lr: 0.0016691306063588583  img/s: 3578.6384039567793  loss: 0.5666 (0.5808)  acc1: 96.8750 (96.7626)  acc5: 100.0000 (99.9106)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [280/468]  eta: 0:00:07  lr: 0.0016691306063588583  img/s: 3536.978628086542  loss: 0.5828 (0.5810)  acc1: 96.0938 (96.7471)  acc5: 100.0000 (99.9083)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [290/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3547.8473992717563  loss: 0.5722 (0.5804)  acc1: 96.8750 (96.7730)  acc5: 100.0000 (99.9114)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [300/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3528.3778177946606  loss: 0.5726 (0.5804)  acc1: 97.6562 (96.7712)  acc5: 100.0000 (99.9143)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [310/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3517.6047803753013  loss: 0.5726 (0.5806)  acc1: 96.8750 (96.7620)  acc5: 100.0000 (99.9146)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [320/468]  eta: 0:00:05  lr: 0.0016691306063588583  img/s: 3548.433634284657  loss: 0.5642 (0.5801)  acc1: 96.8750 (96.7825)  acc5: 100.0000 (99.9173)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [330/468]  eta: 0:00:05  lr: 0.0016691306063588583  img/s: 3526.8480134538577  loss: 0.5545 (0.5798)  acc1: 96.8750 (96.7782)  acc5: 100.0000 (99.9198)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [340/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3395.2740082087994  loss: 0.5802 (0.5803)  acc1: 96.0938 (96.7536)  acc5: 100.0000 (99.9198)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [350/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3578.161382555435  loss: 0.5988 (0.5803)  acc1: 96.0938 (96.7548)  acc5: 100.0000 (99.9176)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [360/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3519.4264774328885  loss: 0.5805 (0.5803)  acc1: 96.8750 (96.7430)  acc5: 100.0000 (99.9178)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [370/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3484.5908483157004  loss: 0.5814 (0.5806)  acc1: 96.8750 (96.7297)  acc5: 100.0000 (99.9179)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [380/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3446.738691079981  loss: 0.5775 (0.5806)  acc1: 96.8750 (96.7315)  acc5: 100.0000 (99.9180)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [390/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3408.812419441887  loss: 0.5775 (0.5808)  acc1: 96.8750 (96.7291)  acc5: 100.0000 (99.9161)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [400/468]  eta: 0:00:02  lr: 0.0016691306063588583  img/s: 3489.619052571369  loss: 0.5905 (0.5811)  acc1: 96.8750 (96.7191)  acc5: 100.0000 (99.9104)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [410/468]  eta: 0:00:02  lr: 0.0016691306063588583  img/s: 3493.979486645494  loss: 0.5783 (0.5811)  acc1: 96.8750 (96.7115)  acc5: 100.0000 (99.9088)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [420/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3499.240097767639  loss: 0.5662 (0.5808)  acc1: 96.8750 (96.7191)  acc5: 100.0000 (99.9109)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [430/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3484.4777673211097  loss: 0.5600 (0.5802)  acc1: 96.8750 (96.7391)  acc5: 100.0000 (99.9112)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [440/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3520.742038717801  loss: 0.5597 (0.5799)  acc1: 97.6562 (96.7528)  acc5: 100.0000 (99.9114)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [450/468]  eta: 0:00:00  lr: 0.0016691306063588583  img/s: 3472.4877398824115  loss: 0.5598 (0.5798)  acc1: 96.8750 (96.7503)  acc5: 100.0000 (99.9134)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [460/468]  eta: 0:00:00  lr: 0.0016691306063588583  img/s: 3504.310698876654  loss: 0.5630 (0.5794)  acc1: 97.6562 (96.7615)  acc5: 100.0000 (99.9153)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5471 (0.5471)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1204  data: 0.1010  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.440 Acc@5 99.980</span><br><span class="line">Epoch: [6]  [  0/468]  eta: 0:01:42  lr: 0.0015  img/s: 1579.9847319707471  loss: 0.6669 (0.6669)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.2198  data: 0.1388  max mem: 190</span><br><span class="line">Epoch: [6]  [ 10/468]  eta: 0:00:37  lr: 0.0015  img/s: 3150.1517482558515  loss: 0.5509 (0.5569)  acc1: 98.4375 (97.9403)  acc5: 100.0000 (100.0000)  time: 0.0820  data: 0.0176  max mem: 190</span><br><span class="line">Epoch: [6]  [ 20/468]  eta: 0:00:32  lr: 0.0015  img/s: 2385.509817600142  loss: 0.5436 (0.5496)  acc1: 98.4375 (98.1027)  acc5: 100.0000 (99.9256)  time: 0.0648  data: 0.0048  max mem: 190</span><br><span class="line">Epoch: [6]  [ 30/468]  eta: 0:00:28  lr: 0.0015  img/s: 3463.6386111146953  loss: 0.5402 (0.5503)  acc1: 97.6562 (98.0091)  acc5: 100.0000 (99.9496)  time: 0.0559  data: 0.0043  max mem: 190</span><br><span class="line">Epoch: [6]  [ 40/468]  eta: 0:00:24  lr: 0.0015  img/s: 3532.8589609449546  loss: 0.5531 (0.5517)  acc1: 97.6562 (97.8849)  acc5: 100.0000 (99.9428)  time: 0.0438  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [6]  [ 50/468]  eta: 0:00:22  lr: 0.0015  img/s: 3459.219793814433  loss: 0.5620 (0.5530)  acc1: 97.6562 (97.8248)  acc5: 100.0000 (99.9540)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [ 60/468]  eta: 0:00:20  lr: 0.0015  img/s: 3523.537983946655  loss: 0.5487 (0.5516)  acc1: 97.6562 (97.8996)  acc5: 100.0000 (99.9616)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 70/468]  eta: 0:00:19  lr: 0.0015  img/s: 3478.6978118459674  loss: 0.5407 (0.5519)  acc1: 98.4375 (97.9093)  acc5: 100.0000 (99.9670)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 80/468]  eta: 0:00:18  lr: 0.0015  img/s: 3461.6509791025915  loss: 0.5368 (0.5515)  acc1: 97.6562 (97.9649)  acc5: 100.0000 (99.9614)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 90/468]  eta: 0:00:17  lr: 0.0015  img/s: 3463.593920156899  loss: 0.5368 (0.5517)  acc1: 98.4375 (97.9653)  acc5: 100.0000 (99.9657)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [100/468]  eta: 0:00:16  lr: 0.0015  img/s: 3570.3563367449406  loss: 0.5376 (0.5519)  acc1: 97.6562 (97.9502)  acc5: 100.0000 (99.9691)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [110/468]  eta: 0:00:16  lr: 0.0015  img/s: 3560.765861488055  loss: 0.5520 (0.5527)  acc1: 97.6562 (97.8885)  acc5: 100.0000 (99.9578)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [120/468]  eta: 0:00:15  lr: 0.0015  img/s: 2785.958465226822  loss: 0.5322 (0.5515)  acc1: 97.6562 (97.9468)  acc5: 100.0000 (99.9548)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [130/468]  eta: 0:00:15  lr: 0.0015  img/s: 2802.054875025444  loss: 0.5300 (0.5520)  acc1: 97.6562 (97.8948)  acc5: 100.0000 (99.9523)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [140/468]  eta: 0:00:14  lr: 0.0015  img/s: 3535.5810547389497  loss: 0.5530 (0.5526)  acc1: 97.6562 (97.8723)  acc5: 100.0000 (99.9501)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [150/468]  eta: 0:00:13  lr: 0.0015  img/s: 3535.7207623714125  loss: 0.5523 (0.5529)  acc1: 97.6562 (97.8477)  acc5: 100.0000 (99.9483)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [160/468]  eta: 0:00:13  lr: 0.0015  img/s: 3605.6287660008866  loss: 0.5583 (0.5537)  acc1: 97.6562 (97.8164)  acc5: 100.0000 (99.9515)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [170/468]  eta: 0:00:12  lr: 0.0015  img/s: 3558.688814943458  loss: 0.5609 (0.5539)  acc1: 97.6562 (97.8207)  acc5: 100.0000 (99.9543)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [180/468]  eta: 0:00:12  lr: 0.0015  img/s: 3531.743416681468  loss: 0.5429 (0.5535)  acc1: 97.6562 (97.8289)  acc5: 100.0000 (99.9568)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [190/468]  eta: 0:00:11  lr: 0.0015  img/s: 3577.5176053522405  loss: 0.5429 (0.5534)  acc1: 97.6562 (97.8321)  acc5: 100.0000 (99.9550)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [200/468]  eta: 0:00:11  lr: 0.0015  img/s: 3349.2470928781754  loss: 0.5524 (0.5535)  acc1: 97.6562 (97.8234)  acc5: 100.0000 (99.9572)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [210/468]  eta: 0:00:10  lr: 0.0015  img/s: 3514.5421290022714  loss: 0.5593 (0.5540)  acc1: 97.6562 (97.8044)  acc5: 100.0000 (99.9556)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [220/468]  eta: 0:00:10  lr: 0.0015  img/s: 3602.0967767907464  loss: 0.5576 (0.5543)  acc1: 97.6562 (97.7835)  acc5: 100.0000 (99.9540)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [230/468]  eta: 0:00:09  lr: 0.0015  img/s: 3557.816234700031  loss: 0.5456 (0.5544)  acc1: 97.6562 (97.7746)  acc5: 100.0000 (99.9527)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [240/468]  eta: 0:00:09  lr: 0.0015  img/s: 3482.5791033932496  loss: 0.5582 (0.5549)  acc1: 96.8750 (97.7470)  acc5: 100.0000 (99.9546)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [250/468]  eta: 0:00:08  lr: 0.0015  img/s: 3449.9502753555203  loss: 0.5683 (0.5558)  acc1: 96.8750 (97.7185)  acc5: 100.0000 (99.9533)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [260/468]  eta: 0:00:08  lr: 0.0015  img/s: 3443.1134769057117  loss: 0.5683 (0.5562)  acc1: 96.8750 (97.7011)  acc5: 100.0000 (99.9551)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [270/468]  eta: 0:00:08  lr: 0.0015  img/s: 3272.545531017415  loss: 0.5686 (0.5569)  acc1: 96.8750 (97.6764)  acc5: 100.0000 (99.9539)  time: 0.0466  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [6]  [280/468]  eta: 0:00:07  lr: 0.0015  img/s: 3548.8793024808465  loss: 0.5686 (0.5571)  acc1: 96.8750 (97.6646)  acc5: 100.0000 (99.9555)  time: 0.0494  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [6]  [290/468]  eta: 0:00:07  lr: 0.0015  img/s: 3544.848908227744  loss: 0.5674 (0.5572)  acc1: 97.6562 (97.6616)  acc5: 100.0000 (99.9570)  time: 0.0401  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [6]  [300/468]  eta: 0:00:06  lr: 0.0015  img/s: 3525.4352825294677  loss: 0.5674 (0.5573)  acc1: 97.6562 (97.6640)  acc5: 100.0000 (99.9533)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [310/468]  eta: 0:00:06  lr: 0.0015  img/s: 3477.2558178697495  loss: 0.5506 (0.5572)  acc1: 97.6562 (97.6613)  acc5: 100.0000 (99.9548)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [320/468]  eta: 0:00:06  lr: 0.0015  img/s: 3415.2310892562928  loss: 0.5397 (0.5569)  acc1: 97.6562 (97.6830)  acc5: 100.0000 (99.9538)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [330/468]  eta: 0:00:05  lr: 0.0015  img/s: 3478.066792347709  loss: 0.5433 (0.5572)  acc1: 97.6562 (97.6657)  acc5: 100.0000 (99.9528)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [340/468]  eta: 0:00:05  lr: 0.0015  img/s: 2952.0947976751477  loss: 0.5600 (0.5575)  acc1: 96.8750 (97.6631)  acc5: 100.0000 (99.9496)  time: 0.0409  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [6]  [350/468]  eta: 0:00:04  lr: 0.0015  img/s: 2809.577425871074  loss: 0.5623 (0.5582)  acc1: 96.8750 (97.6295)  acc5: 100.0000 (99.9510)  time: 0.0456  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [6]  [360/468]  eta: 0:00:04  lr: 0.0015  img/s: 3442.8264385432767  loss: 0.5646 (0.5587)  acc1: 97.6562 (97.6151)  acc5: 100.0000 (99.9459)  time: 0.0457  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [6]  [370/468]  eta: 0:00:04  lr: 0.0015  img/s: 3640.8888888888887  loss: 0.5475 (0.5584)  acc1: 97.6562 (97.6183)  acc5: 100.0000 (99.9474)  time: 0.0406  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [6]  [380/468]  eta: 0:00:03  lr: 0.0015  img/s: 3591.614286956696  loss: 0.5605 (0.5589)  acc1: 96.8750 (97.5968)  acc5: 100.0000 (99.9467)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [390/468]  eta: 0:00:03  lr: 0.0015  img/s: 3500.0157245209953  loss: 0.5546 (0.5586)  acc1: 97.6562 (97.6143)  acc5: 100.0000 (99.9480)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [400/468]  eta: 0:00:02  lr: 0.0015  img/s: 3382.9508188457394  loss: 0.5446 (0.5587)  acc1: 97.6562 (97.6114)  acc5: 100.0000 (99.9493)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [410/468]  eta: 0:00:02  lr: 0.0015  img/s: 3362.062260074522  loss: 0.5734 (0.5591)  acc1: 96.8750 (97.5840)  acc5: 100.0000 (99.9487)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [420/468]  eta: 0:00:01  lr: 0.0015  img/s: 3414.861795237126  loss: 0.5567 (0.5587)  acc1: 96.8750 (97.5987)  acc5: 100.0000 (99.9499)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [430/468]  eta: 0:00:01  lr: 0.0015  img/s: 3449.7507614408905  loss: 0.5381 (0.5586)  acc1: 98.4375 (97.5964)  acc5: 100.0000 (99.9511)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [440/468]  eta: 0:00:01  lr: 0.0015  img/s: 3363.6210035649174  loss: 0.5545 (0.5587)  acc1: 97.6562 (97.5942)  acc5: 100.0000 (99.9504)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [450/468]  eta: 0:00:00  lr: 0.0015  img/s: 3469.7497689508755  loss: 0.5516 (0.5587)  acc1: 97.6562 (97.5956)  acc5: 100.0000 (99.9498)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [460/468]  eta: 0:00:00  lr: 0.0015  img/s: 3430.682352339751  loss: 0.5530 (0.5589)  acc1: 97.6562 (97.5851)  acc5: 100.0000 (99.9492)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5535 (0.5535)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1226  data: 0.0983  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.780 Acc@5 99.980</span><br><span class="line">Epoch: [7]  [  0/468]  eta: 0:01:16  lr: 0.0013090169943749475  img/s: 2489.0627005174047  loss: 0.5265 (0.5265)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1624  data: 0.1110  max mem: 190</span><br><span class="line">Epoch: [7]  [ 10/468]  eta: 0:00:25  lr: 0.0013090169943749475  img/s: 3451.502838370396  loss: 0.5363 (0.5397)  acc1: 97.6562 (98.2955)  acc5: 100.0000 (100.0000)  time: 0.0547  data: 0.0117  max mem: 190</span><br><span class="line">Epoch: [7]  [ 20/468]  eta: 0:00:21  lr: 0.0013090169943749475  img/s: 3377.842518198806  loss: 0.5363 (0.5402)  acc1: 98.4375 (98.2887)  acc5: 100.0000 (100.0000)  time: 0.0429  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [7]  [ 30/468]  eta: 0:00:20  lr: 0.0013090169943749475  img/s: 1929.3512011931073  loss: 0.5474 (0.5520)  acc1: 97.6562 (97.7571)  acc5: 100.0000 (99.9496)  time: 0.0422  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [7]  [ 40/468]  eta: 0:00:20  lr: 0.0013090169943749475  img/s: 3102.421348866506  loss: 0.5683 (0.5610)  acc1: 96.0938 (97.3133)  acc5: 100.0000 (99.9428)  time: 0.0472  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [7]  [ 50/468]  eta: 0:00:19  lr: 0.0013090169943749475  img/s: 3340.3282148279036  loss: 0.5635 (0.5618)  acc1: 96.0938 (97.3039)  acc5: 100.0000 (99.9540)  time: 0.0484  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [7]  [ 60/468]  eta: 0:00:18  lr: 0.0013090169943749475  img/s: 3524.926050673968  loss: 0.5575 (0.5615)  acc1: 97.6562 (97.3361)  acc5: 100.0000 (99.9616)  time: 0.0415  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [7]  [ 70/468]  eta: 0:00:17  lr: 0.0013090169943749475  img/s: 3519.4264774328885  loss: 0.5377 (0.5585)  acc1: 98.4375 (97.5022)  acc5: 100.0000 (99.9670)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [ 80/468]  eta: 0:00:16  lr: 0.0013090169943749475  img/s: 3478.787976180448  loss: 0.5332 (0.5582)  acc1: 98.4375 (97.5598)  acc5: 100.0000 (99.9711)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [ 90/468]  eta: 0:00:16  lr: 0.0013090169943749475  img/s: 3446.9821188949027  loss: 0.5633 (0.5601)  acc1: 97.6562 (97.4931)  acc5: 100.0000 (99.9657)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [100/468]  eta: 0:00:15  lr: 0.0013090169943749475  img/s: 3419.908474749019  loss: 0.5747 (0.5619)  acc1: 97.6562 (97.4629)  acc5: 100.0000 (99.9536)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [110/468]  eta: 0:00:15  lr: 0.0013090169943749475  img/s: 3515.669853576761  loss: 0.5706 (0.5618)  acc1: 97.6562 (97.4944)  acc5: 100.0000 (99.9507)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [120/468]  eta: 0:00:14  lr: 0.0013090169943749475  img/s: 3549.489345665871  loss: 0.5600 (0.5613)  acc1: 97.6562 (97.5465)  acc5: 100.0000 (99.9548)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [130/468]  eta: 0:00:14  lr: 0.0013090169943749475  img/s: 2857.048879522753  loss: 0.5624 (0.5615)  acc1: 97.6562 (97.5310)  acc5: 100.0000 (99.9404)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [140/468]  eta: 0:00:13  lr: 0.0013090169943749475  img/s: 3495.7767894932185  loss: 0.5542 (0.5605)  acc1: 97.6562 (97.5842)  acc5: 100.0000 (99.9446)  time: 0.0405  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [150/468]  eta: 0:00:13  lr: 0.0013090169943749475  img/s: 3554.0007811413934  loss: 0.5517 (0.5603)  acc1: 98.4375 (97.6407)  acc5: 100.0000 (99.9483)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [160/468]  eta: 0:00:12  lr: 0.0013090169943749475  img/s: 3441.9214771124502  loss: 0.5565 (0.5605)  acc1: 98.4375 (97.6271)  acc5: 100.0000 (99.9369)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [170/468]  eta: 0:00:12  lr: 0.0013090169943749475  img/s: 3572.7798651733247  loss: 0.5565 (0.5605)  acc1: 97.6562 (97.6243)  acc5: 100.0000 (99.9269)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [180/468]  eta: 0:00:11  lr: 0.0013090169943749475  img/s: 3475.117561007185  loss: 0.5568 (0.5610)  acc1: 97.6562 (97.6088)  acc5: 100.0000 (99.9309)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [190/468]  eta: 0:00:11  lr: 0.0013090169943749475  img/s: 3443.1797233249745  loss: 0.5642 (0.5609)  acc1: 97.6562 (97.6194)  acc5: 100.0000 (99.9346)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [200/468]  eta: 0:00:10  lr: 0.0013090169943749475  img/s: 3543.702389438944  loss: 0.5639 (0.5611)  acc1: 97.6562 (97.6096)  acc5: 100.0000 (99.9378)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [210/468]  eta: 0:00:10  lr: 0.0013090169943749475  img/s: 3572.684762861763  loss: 0.5502 (0.5611)  acc1: 97.6562 (97.5970)  acc5: 100.0000 (99.9408)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [220/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3566.6087280022853  loss: 0.5558 (0.5613)  acc1: 96.8750 (97.5785)  acc5: 100.0000 (99.9434)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [230/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3526.6626727626253  loss: 0.5505 (0.5606)  acc1: 97.6562 (97.6089)  acc5: 100.0000 (99.9459)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [240/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3495.754027269531  loss: 0.5505 (0.5605)  acc1: 98.4375 (97.6109)  acc5: 100.0000 (99.9449)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [250/468]  eta: 0:00:08  lr: 0.0013090169943749475  img/s: 3506.9660519835124  loss: 0.5430 (0.5600)  acc1: 97.6562 (97.6251)  acc5: 100.0000 (99.9471)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [260/468]  eta: 0:00:08  lr: 0.0013090169943749475  img/s: 3502.4589128676184  loss: 0.5449 (0.5598)  acc1: 98.4375 (97.6503)  acc5: 100.0000 (99.9461)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [270/468]  eta: 0:00:07  lr: 0.0013090169943749475  img/s: 3502.9845296585563  loss: 0.5531 (0.5596)  acc1: 97.6562 (97.6534)  acc5: 100.0000 (99.9452)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [280/468]  eta: 0:00:07  lr: 0.0013090169943749475  img/s: 3531.3484970071695  loss: 0.5551 (0.5595)  acc1: 97.6562 (97.6590)  acc5: 100.0000 (99.9472)  time: 0.0379  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [290/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 3533.8589012782877  loss: 0.5554 (0.5593)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (99.9463)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [300/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 3543.655608506818  loss: 0.5470 (0.5591)  acc1: 97.6562 (97.6537)  acc5: 100.0000 (99.9481)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [310/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 1957.5682104910047  loss: 0.5547 (0.5592)  acc1: 97.6562 (97.6437)  acc5: 100.0000 (99.9498)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [320/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 1364.188460815254  loss: 0.5524 (0.5589)  acc1: 98.4375 (97.6611)  acc5: 100.0000 (99.9489)  time: 0.0678  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [7]  [330/468]  eta: 0:00:05  lr: 0.0013090169943749475  img/s: 3207.1908050371576  loss: 0.5477 (0.5592)  acc1: 97.6562 (97.6586)  acc5: 100.0000 (99.9410)  time: 0.0775  data: 0.0048  max mem: 190</span><br><span class="line">Epoch: [7]  [340/468]  eta: 0:00:05  lr: 0.0013090169943749475  img/s: 3364.8437321767688  loss: 0.5512 (0.5589)  acc1: 97.6562 (97.6631)  acc5: 100.0000 (99.9427)  time: 0.0515  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [7]  [350/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3425.0574935565364  loss: 0.5448 (0.5588)  acc1: 97.6562 (97.6629)  acc5: 100.0000 (99.9444)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [360/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3450.127640432109  loss: 0.5366 (0.5583)  acc1: 98.4375 (97.6887)  acc5: 100.0000 (99.9459)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [370/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3478.201991538875  loss: 0.5509 (0.5586)  acc1: 97.6562 (97.6773)  acc5: 100.0000 (99.9410)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [380/468]  eta: 0:00:03  lr: 0.0013090169943749475  img/s: 3473.7684373989  loss: 0.5645 (0.5588)  acc1: 96.8750 (97.6686)  acc5: 100.0000 (99.9426)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [390/468]  eta: 0:00:03  lr: 0.0013090169943749475  img/s: 3551.344225858944  loss: 0.5611 (0.5590)  acc1: 96.8750 (97.6443)  acc5: 100.0000 (99.9441)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [400/468]  eta: 0:00:02  lr: 0.0013090169943749475  img/s: 3478.990863023108  loss: 0.5611 (0.5593)  acc1: 96.8750 (97.6387)  acc5: 100.0000 (99.9454)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [410/468]  eta: 0:00:02  lr: 0.0013090169943749475  img/s: 3477.931603666634  loss: 0.5499 (0.5592)  acc1: 97.6562 (97.6448)  acc5: 100.0000 (99.9468)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [420/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3323.3109373742627  loss: 0.5408 (0.5588)  acc1: 98.4375 (97.6581)  acc5: 100.0000 (99.9480)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [430/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3464.0632327416556  loss: 0.5350 (0.5586)  acc1: 97.6562 (97.6581)  acc5: 100.0000 (99.9492)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [440/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3507.7221895540138  loss: 0.5480 (0.5584)  acc1: 97.6562 (97.6704)  acc5: 100.0000 (99.9504)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [450/468]  eta: 0:00:00  lr: 0.0013090169943749475  img/s: 3492.6611239054346  loss: 0.5523 (0.5584)  acc1: 97.6562 (97.6649)  acc5: 100.0000 (99.9515)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [460/468]  eta: 0:00:00  lr: 0.0013090169943749475  img/s: 3556.143021792409  loss: 0.5525 (0.5584)  acc1: 97.6562 (97.6613)  acc5: 100.0000 (99.9509)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5532 (0.5532)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1250  data: 0.1056  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.530 Acc@5 99.950</span><br><span class="line">Epoch: [8]  [  0/468]  eta: 0:01:36  lr: 0.0011045284632676536  img/s: 2262.6431328916537  loss: 0.5521 (0.5521)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.2056  data: 0.1490  max mem: 190</span><br><span class="line">Epoch: [8]  [ 10/468]  eta: 0:00:23  lr: 0.0011045284632676536  img/s: 3470.736735947248  loss: 0.5344 (0.5478)  acc1: 98.4375 (97.8693)  acc5: 100.0000 (100.0000)  time: 0.0523  data: 0.0138  max mem: 190</span><br><span class="line">Epoch: [8]  [ 20/468]  eta: 0:00:20  lr: 0.0011045284632676536  img/s: 3469.368591110594  loss: 0.5449 (0.5517)  acc1: 97.6562 (97.7679)  acc5: 100.0000 (99.9256)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 30/468]  eta: 0:00:18  lr: 0.0011045284632676536  img/s: 3585.66532422342  loss: 0.5611 (0.5518)  acc1: 97.6562 (97.7823)  acc5: 100.0000 (99.9496)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 40/468]  eta: 0:00:17  lr: 0.0011045284632676536  img/s: 3444.195821064044  loss: 0.5630 (0.5562)  acc1: 96.8750 (97.5800)  acc5: 100.0000 (99.9619)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 50/468]  eta: 0:00:16  lr: 0.0011045284632676536  img/s: 3318.9554337007526  loss: 0.5700 (0.5597)  acc1: 96.8750 (97.4877)  acc5: 100.0000 (99.9387)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [ 60/468]  eta: 0:00:16  lr: 0.0011045284632676536  img/s: 3511.8293507767785  loss: 0.5537 (0.5556)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (99.9488)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [ 70/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3525.57418948115  loss: 0.5447 (0.5569)  acc1: 97.6562 (97.6012)  acc5: 100.0000 (99.9450)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 80/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 2674.5390018631624  loss: 0.5500 (0.5547)  acc1: 97.6562 (97.6852)  acc5: 100.0000 (99.9518)  time: 0.0420  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [8]  [ 90/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3554.4243162543116  loss: 0.5306 (0.5527)  acc1: 98.4375 (97.7850)  acc5: 100.0000 (99.9571)  time: 0.0483  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [8]  [100/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3394.6514239465832  loss: 0.5306 (0.5515)  acc1: 98.4375 (97.8342)  acc5: 100.0000 (99.9536)  time: 0.0435  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [8]  [110/468]  eta: 0:00:14  lr: 0.0011045284632676536  img/s: 3309.278761280142  loss: 0.5341 (0.5502)  acc1: 98.4375 (97.8885)  acc5: 100.0000 (99.9578)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [120/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3566.2059730045703  loss: 0.5408 (0.5502)  acc1: 98.4375 (97.8758)  acc5: 100.0000 (99.9613)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [130/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3405.6121235957194  loss: 0.5434 (0.5499)  acc1: 97.6562 (97.8769)  acc5: 100.0000 (99.9642)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [140/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3592.7439370415973  loss: 0.5348 (0.5492)  acc1: 97.6562 (97.9000)  acc5: 100.0000 (99.9612)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [150/468]  eta: 0:00:12  lr: 0.0011045284632676536  img/s: 3486.876656989394  loss: 0.5285 (0.5483)  acc1: 98.4375 (97.9408)  acc5: 100.0000 (99.9534)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [160/468]  eta: 0:00:12  lr: 0.0011045284632676536  img/s: 3574.635372763651  loss: 0.5372 (0.5475)  acc1: 98.4375 (97.9862)  acc5: 100.0000 (99.9515)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [170/468]  eta: 0:00:11  lr: 0.0011045284632676536  img/s: 3459.509572322424  loss: 0.5390 (0.5475)  acc1: 98.4375 (97.9989)  acc5: 100.0000 (99.9543)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [180/468]  eta: 0:00:11  lr: 0.0011045284632676536  img/s: 3559.8686576664986  loss: 0.5453 (0.5478)  acc1: 98.4375 (98.0016)  acc5: 100.0000 (99.9568)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [190/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3072.609495902202  loss: 0.5619 (0.5487)  acc1: 97.6562 (97.9917)  acc5: 100.0000 (99.9591)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [200/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3079.9600254717143  loss: 0.5619 (0.5487)  acc1: 97.6562 (97.9711)  acc5: 100.0000 (99.9611)  time: 0.0421  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [210/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3520.095675207847  loss: 0.5388 (0.5484)  acc1: 97.6562 (98.0006)  acc5: 100.0000 (99.9630)  time: 0.0402  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [220/468]  eta: 0:00:09  lr: 0.0011045284632676536  img/s: 3515.3705908159322  loss: 0.5334 (0.5481)  acc1: 98.4375 (98.0098)  acc5: 100.0000 (99.9646)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [230/468]  eta: 0:00:09  lr: 0.0011045284632676536  img/s: 3547.4488700938286  loss: 0.5408 (0.5483)  acc1: 98.4375 (97.9978)  acc5: 100.0000 (99.9628)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [240/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3518.2271735355216  loss: 0.5406 (0.5481)  acc1: 97.6562 (97.9999)  acc5: 100.0000 (99.9611)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [250/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3403.798411178809  loss: 0.5354 (0.5477)  acc1: 98.4375 (98.0142)  acc5: 100.0000 (99.9626)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [260/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3418.7324851309872  loss: 0.5354 (0.5476)  acc1: 98.4375 (98.0244)  acc5: 100.0000 (99.9581)  time: 0.0415  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [8]  [270/468]  eta: 0:00:07  lr: 0.0011045284632676536  img/s: 3439.9366438136735  loss: 0.5399 (0.5473)  acc1: 98.4375 (98.0339)  acc5: 100.0000 (99.9596)  time: 0.0417  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [8]  [280/468]  eta: 0:00:07  lr: 0.0011045284632676536  img/s: 3443.4005631345685  loss: 0.5378 (0.5470)  acc1: 98.4375 (98.0344)  acc5: 100.0000 (99.9611)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [290/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3593.2248547640083  loss: 0.5373 (0.5468)  acc1: 97.6562 (98.0402)  acc5: 100.0000 (99.9597)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [300/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3540.967780657842  loss: 0.5535 (0.5469)  acc1: 97.6562 (98.0456)  acc5: 100.0000 (99.9611)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [310/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3562.325238208987  loss: 0.5366 (0.5469)  acc1: 98.4375 (98.0481)  acc5: 100.0000 (99.9623)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [320/468]  eta: 0:00:05  lr: 0.0011045284632676536  img/s: 3435.7319612699266  loss: 0.5502 (0.5472)  acc1: 96.8750 (98.0262)  acc5: 100.0000 (99.9635)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [330/468]  eta: 0:00:05  lr: 0.0011045284632676536  img/s: 3478.8105179943755  loss: 0.5403 (0.5468)  acc1: 97.6562 (98.0433)  acc5: 100.0000 (99.9646)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [340/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3482.1725160043325  loss: 0.5320 (0.5465)  acc1: 98.4375 (98.0595)  acc5: 100.0000 (99.9656)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [350/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3570.237620865309  loss: 0.5320 (0.5463)  acc1: 98.4375 (98.0747)  acc5: 100.0000 (99.9666)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [360/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3473.813391308849  loss: 0.5514 (0.5467)  acc1: 98.4375 (98.0674)  acc5: 100.0000 (99.9675)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [370/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3036.1365176132604  loss: 0.5527 (0.5467)  acc1: 97.6562 (98.0711)  acc5: 100.0000 (99.9684)  time: 0.0452  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [8]  [380/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3556.378590355061  loss: 0.5467 (0.5466)  acc1: 97.6562 (98.0746)  acc5: 100.0000 (99.9692)  time: 0.0491  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [8]  [390/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3614.294450690382  loss: 0.5329 (0.5464)  acc1: 98.4375 (98.0818)  acc5: 100.0000 (99.9700)  time: 0.0408  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [8]  [400/468]  eta: 0:00:02  lr: 0.0011045284632676536  img/s: 3452.4350471045946  loss: 0.5323 (0.5460)  acc1: 99.2188 (98.1024)  acc5: 100.0000 (99.9708)  time: 0.0368  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [410/468]  eta: 0:00:02  lr: 0.0011045284632676536  img/s: 3487.9640334977034  loss: 0.5392 (0.5461)  acc1: 98.4375 (98.1049)  acc5: 100.0000 (99.9696)  time: 0.0366  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [420/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3611.0611942908645  loss: 0.5458 (0.5460)  acc1: 98.4375 (98.1072)  acc5: 100.0000 (99.9703)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [430/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3563.412884469873  loss: 0.5407 (0.5459)  acc1: 98.4375 (98.1130)  acc5: 100.0000 (99.9710)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [440/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3511.278111694648  loss: 0.5311 (0.5457)  acc1: 98.4375 (98.1257)  acc5: 100.0000 (99.9699)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [450/468]  eta: 0:00:00  lr: 0.0011045284632676536  img/s: 3517.4895465475106  loss: 0.5311 (0.5458)  acc1: 99.2188 (98.1257)  acc5: 100.0000 (99.9688)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [460/468]  eta: 0:00:00  lr: 0.0011045284632676536  img/s: 3581.455421172358  loss: 0.5253 (0.5455)  acc1: 99.2188 (98.1375)  acc5: 100.0000 (99.9695)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5504 (0.5504)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1249  data: 0.1060  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.000 Acc@5 99.970</span><br><span class="line">Epoch: [9]  [  0/468]  eta: 0:01:14  lr: 0.0008954715367323467  img/s: 2598.3995043946256  loss: 0.5086 (0.5086)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1588  data: 0.1095  max mem: 190</span><br><span class="line">Epoch: [9]  [ 10/468]  eta: 0:00:23  lr: 0.0008954715367323467  img/s: 2439.9008898462994  loss: 0.5330 (0.5354)  acc1: 98.4375 (98.5085)  acc5: 100.0000 (99.9290)  time: 0.0515  data: 0.0102  max mem: 190</span><br><span class="line">Epoch: [9]  [ 20/468]  eta: 0:00:20  lr: 0.0008954715367323467  img/s: 2203.5869723151436  loss: 0.5330 (0.5314)  acc1: 98.4375 (98.7351)  acc5: 100.0000 (99.9628)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [ 30/468]  eta: 0:00:19  lr: 0.0008954715367323467  img/s: 3566.5376469806683  loss: 0.5352 (0.5309)  acc1: 98.4375 (98.6895)  acc5: 100.0000 (99.9748)  time: 0.0395  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [9]  [ 40/468]  eta: 0:00:18  lr: 0.0008954715367323467  img/s: 3491.2756429848805  loss: 0.5292 (0.5306)  acc1: 98.4375 (98.7424)  acc5: 100.0000 (99.9809)  time: 0.0376  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [9]  [ 50/468]  eta: 0:00:17  lr: 0.0008954715367323467  img/s: 3387.283666464769  loss: 0.5334 (0.5317)  acc1: 98.4375 (98.6520)  acc5: 100.0000 (99.9847)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 60/468]  eta: 0:00:16  lr: 0.0008954715367323467  img/s: 3530.6517953439434  loss: 0.5342 (0.5321)  acc1: 98.4375 (98.6680)  acc5: 100.0000 (99.9872)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 70/468]  eta: 0:00:15  lr: 0.0008954715367323467  img/s: 3613.8078768990517  loss: 0.5293 (0.5325)  acc1: 98.4375 (98.6356)  acc5: 100.0000 (99.9890)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 80/468]  eta: 0:00:15  lr: 0.0008954715367323467  img/s: 3584.276876856828  loss: 0.5376 (0.5332)  acc1: 98.4375 (98.5822)  acc5: 100.0000 (99.9904)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 90/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 3455.723990550796  loss: 0.5376 (0.5334)  acc1: 98.4375 (98.5834)  acc5: 100.0000 (99.9914)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [100/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 3509.9925599032395  loss: 0.5252 (0.5334)  acc1: 99.2188 (98.5767)  acc5: 100.0000 (99.9845)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [110/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2140.45439576431  loss: 0.5335 (0.5337)  acc1: 98.4375 (98.5642)  acc5: 100.0000 (99.9859)  time: 0.0437  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [9]  [120/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2856.7904303775913  loss: 0.5341 (0.5336)  acc1: 98.4375 (98.5731)  acc5: 100.0000 (99.9871)  time: 0.0544  data: 0.0045  max mem: 190</span><br><span class="line">Epoch: [9]  [130/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 1459.1936682403655  loss: 0.5305 (0.5335)  acc1: 99.2188 (98.5866)  acc5: 100.0000 (99.9881)  time: 0.0578  data: 0.0061  max mem: 190</span><br><span class="line">Epoch: [9]  [140/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2368.293897860965  loss: 0.5229 (0.5326)  acc1: 99.2188 (98.6370)  acc5: 100.0000 (99.9889)  time: 0.0550  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [9]  [150/468]  eta: 0:00:13  lr: 0.0008954715367323467  img/s: 3473.1841424283202  loss: 0.5229 (0.5323)  acc1: 99.2188 (98.6445)  acc5: 100.0000 (99.9897)  time: 0.0454  data: 0.0028  max mem: 190</span><br><span class="line">Epoch: [9]  [160/468]  eta: 0:00:13  lr: 0.0008954715367323467  img/s: 3528.6097221126797  loss: 0.5357 (0.5330)  acc1: 98.4375 (98.6122)  acc5: 100.0000 (99.9903)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [170/468]  eta: 0:00:12  lr: 0.0008954715367323467  img/s: 3583.1286298745936  loss: 0.5247 (0.5322)  acc1: 98.4375 (98.6431)  acc5: 100.0000 (99.9909)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [180/468]  eta: 0:00:12  lr: 0.0008954715367323467  img/s: 3462.253727493164  loss: 0.5234 (0.5326)  acc1: 99.2188 (98.6360)  acc5: 100.0000 (99.9871)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [190/468]  eta: 0:00:11  lr: 0.0008954715367323467  img/s: 3527.1955797620376  loss: 0.5289 (0.5326)  acc1: 98.4375 (98.6420)  acc5: 100.0000 (99.9877)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [200/468]  eta: 0:00:11  lr: 0.0008954715367323467  img/s: 3514.059039914124  loss: 0.5289 (0.5330)  acc1: 98.4375 (98.6435)  acc5: 100.0000 (99.9883)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [210/468]  eta: 0:00:10  lr: 0.0008954715367323467  img/s: 3603.982868574037  loss: 0.5418 (0.5334)  acc1: 98.4375 (98.6226)  acc5: 100.0000 (99.9852)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [220/468]  eta: 0:00:10  lr: 0.0008954715367323467  img/s: 3533.626306505542  loss: 0.5405 (0.5335)  acc1: 98.4375 (98.6178)  acc5: 100.0000 (99.9859)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [230/468]  eta: 0:00:09  lr: 0.0008954715367323467  img/s: 3551.649645080411  loss: 0.5366 (0.5336)  acc1: 98.4375 (98.6134)  acc5: 100.0000 (99.9831)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [240/468]  eta: 0:00:09  lr: 0.0008954715367323467  img/s: 3531.1626830134574  loss: 0.5348 (0.5340)  acc1: 98.4375 (98.5996)  acc5: 100.0000 (99.9838)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [250/468]  eta: 0:00:08  lr: 0.0008954715367323467  img/s: 3386.557194221914  loss: 0.5304 (0.5338)  acc1: 98.4375 (98.6118)  acc5: 100.0000 (99.9844)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [260/468]  eta: 0:00:08  lr: 0.0008954715367323467  img/s: 3556.143021792409  loss: 0.5290 (0.5342)  acc1: 98.4375 (98.5961)  acc5: 100.0000 (99.9790)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [270/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3552.77780204218  loss: 0.5240 (0.5341)  acc1: 98.4375 (98.6076)  acc5: 100.0000 (99.9798)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [280/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3469.7273444063853  loss: 0.5242 (0.5341)  acc1: 98.4375 (98.6043)  acc5: 100.0000 (99.9778)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [290/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3458.595820341691  loss: 0.5285 (0.5343)  acc1: 98.4375 (98.6013)  acc5: 100.0000 (99.9758)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [300/468]  eta: 0:00:06  lr: 0.0008954715367323467  img/s: 3508.5245100281663  loss: 0.5416 (0.5351)  acc1: 98.4375 (98.5751)  acc5: 100.0000 (99.9740)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [310/468]  eta: 0:00:06  lr: 0.0008954715367323467  img/s: 3411.8886325649974  loss: 0.5515 (0.5356)  acc1: 97.6562 (98.5480)  acc5: 100.0000 (99.9749)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [320/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3536.535943665312  loss: 0.5444 (0.5360)  acc1: 97.6562 (98.5349)  acc5: 100.0000 (99.9757)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [330/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3589.9091407556  loss: 0.5402 (0.5362)  acc1: 98.4375 (98.5272)  acc5: 100.0000 (99.9740)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [340/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3572.2093272384905  loss: 0.5437 (0.5365)  acc1: 98.4375 (98.5085)  acc5: 100.0000 (99.9725)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [350/468]  eta: 0:00:04  lr: 0.0008954715367323467  img/s: 3516.4758143221134  loss: 0.5372 (0.5365)  acc1: 98.4375 (98.4976)  acc5: 100.0000 (99.9733)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [360/468]  eta: 0:00:04  lr: 0.0008954715367323467  img/s: 3577.255392160129  loss: 0.5323 (0.5364)  acc1: 98.4375 (98.4981)  acc5: 100.0000 (99.9740)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [370/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3451.6137892016304  loss: 0.5394 (0.5369)  acc1: 97.6562 (98.4712)  acc5: 100.0000 (99.9747)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [380/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3403.3884345720335  loss: 0.5448 (0.5369)  acc1: 98.4375 (98.4724)  acc5: 100.0000 (99.9754)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [390/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3540.5707954680347  loss: 0.5286 (0.5366)  acc1: 98.4375 (98.4895)  acc5: 100.0000 (99.9760)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [400/468]  eta: 0:00:02  lr: 0.0008954715367323467  img/s: 3487.0804884385557  loss: 0.5253 (0.5365)  acc1: 99.2188 (98.4901)  acc5: 100.0000 (99.9747)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [410/468]  eta: 0:00:02  lr: 0.0008954715367323467  img/s: 3550.0291741056667  loss: 0.5383 (0.5368)  acc1: 98.4375 (98.4793)  acc5: 100.0000 (99.9734)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [420/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 1396.214262494181  loss: 0.5412 (0.5369)  acc1: 98.4375 (98.4765)  acc5: 100.0000 (99.9740)  time: 0.0427  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [430/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 3218.3997170484313  loss: 0.5426 (0.5372)  acc1: 97.6562 (98.4665)  acc5: 100.0000 (99.9746)  time: 0.0484  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [9]  [440/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 3407.51427755387  loss: 0.5414 (0.5373)  acc1: 97.6562 (98.4588)  acc5: 100.0000 (99.9752)  time: 0.0437  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [9]  [450/468]  eta: 0:00:00  lr: 0.0008954715367323467  img/s: 3551.7906255168537  loss: 0.5339 (0.5370)  acc1: 98.4375 (98.4618)  acc5: 100.0000 (99.9757)  time: 0.0384  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [9]  [460/468]  eta: 0:00:00  lr: 0.0008954715367323467  img/s: 3539.6604009942444  loss: 0.5245 (0.5369)  acc1: 98.4375 (98.4697)  acc5: 100.0000 (99.9746)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:11  loss: 0.5483 (0.5483)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1414  data: 0.1171  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.280 Acc@5 99.960</span><br><span class="line">Epoch: [10]  [  0/468]  eta: 0:01:13  lr: 0.0006909830056250527  img/s: 2420.8236928016163  loss: 0.5375 (0.5375)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1576  data: 0.1046  max mem: 190</span><br><span class="line">Epoch: [10]  [ 10/468]  eta: 0:00:24  lr: 0.0006909830056250527  img/s: 3436.0398087643284  loss: 0.5144 (0.5181)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.0530  data: 0.0104  max mem: 190</span><br><span class="line">Epoch: [10]  [ 20/468]  eta: 0:00:21  lr: 0.0006909830056250527  img/s: 3524.069946962138  loss: 0.5215 (0.5234)  acc1: 99.2188 (99.1071)  acc5: 100.0000 (100.0000)  time: 0.0421  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [10]  [ 30/468]  eta: 0:00:19  lr: 0.0006909830056250527  img/s: 3538.447269731422  loss: 0.5242 (0.5217)  acc1: 99.2188 (99.1431)  acc5: 100.0000 (100.0000)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 40/468]  eta: 0:00:18  lr: 0.0006909830056250527  img/s: 3512.8633906955442  loss: 0.5161 (0.5239)  acc1: 99.2188 (99.0091)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 50/468]  eta: 0:00:17  lr: 0.0006909830056250527  img/s: 3348.3280029936386  loss: 0.5179 (0.5234)  acc1: 99.2188 (99.0656)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [ 60/468]  eta: 0:00:16  lr: 0.0006909830056250527  img/s: 3320.2280314415234  loss: 0.5163 (0.5226)  acc1: 99.2188 (99.0907)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 70/468]  eta: 0:00:16  lr: 0.0006909830056250527  img/s: 3438.5706453513694  loss: 0.5123 (0.5220)  acc1: 99.2188 (99.1087)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 80/468]  eta: 0:00:15  lr: 0.0006909830056250527  img/s: 3534.6034103627626  loss: 0.5209 (0.5231)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 90/468]  eta: 0:00:15  lr: 0.0006909830056250527  img/s: 3577.0170498837356  loss: 0.5209 (0.5231)  acc1: 99.2188 (99.0556)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [100/468]  eta: 0:00:14  lr: 0.0006909830056250527  img/s: 3524.069946962138  loss: 0.5268 (0.5248)  acc1: 98.4375 (99.0022)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [110/468]  eta: 0:00:14  lr: 0.0006909830056250527  img/s: 3550.2639333421507  loss: 0.5263 (0.5245)  acc1: 99.2188 (99.0217)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [120/468]  eta: 0:00:13  lr: 0.0006909830056250527  img/s: 3530.9072206985907  loss: 0.5166 (0.5249)  acc1: 99.2188 (99.0121)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [130/468]  eta: 0:00:13  lr: 0.0006909830056250527  img/s: 3610.624055093751  loss: 0.5242 (0.5256)  acc1: 99.2188 (98.9742)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [140/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 3567.912382370142  loss: 0.5208 (0.5253)  acc1: 99.2188 (99.0082)  acc5: 100.0000 (99.9945)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [150/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 2762.3779244768484  loss: 0.5192 (0.5255)  acc1: 99.2188 (99.0014)  acc5: 100.0000 (99.9948)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [160/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 3191.291160910658  loss: 0.5192 (0.5253)  acc1: 99.2188 (99.0149)  acc5: 100.0000 (99.9951)  time: 0.0430  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [170/468]  eta: 0:00:11  lr: 0.0006909830056250527  img/s: 3518.8267232960393  loss: 0.5229 (0.5257)  acc1: 98.4375 (98.9766)  acc5: 100.0000 (99.9954)  time: 0.0413  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [180/468]  eta: 0:00:11  lr: 0.0006909830056250527  img/s: 3413.4939311669073  loss: 0.5251 (0.5263)  acc1: 98.4375 (98.9555)  acc5: 100.0000 (99.9957)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [190/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 1556.440813486599  loss: 0.5210 (0.5262)  acc1: 99.2188 (98.9570)  acc5: 100.0000 (99.9959)  time: 0.0407  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [200/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 2556.966489493437  loss: 0.5269 (0.5272)  acc1: 98.4375 (98.9195)  acc5: 100.0000 (99.9961)  time: 0.0485  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [10]  [210/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 3502.276126608042  loss: 0.5357 (0.5273)  acc1: 98.4375 (98.9114)  acc5: 100.0000 (99.9963)  time: 0.0458  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [10]  [220/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3562.79804629433  loss: 0.5338 (0.5279)  acc1: 98.4375 (98.8758)  acc5: 100.0000 (99.9965)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [230/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3597.125038525963  loss: 0.5187 (0.5277)  acc1: 99.2188 (98.8907)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [240/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3483.7994354498555  loss: 0.5226 (0.5282)  acc1: 99.2188 (98.8654)  acc5: 100.0000 (99.9968)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [250/468]  eta: 0:00:08  lr: 0.0006909830056250527  img/s: 3499.057647311856  loss: 0.5308 (0.5282)  acc1: 98.4375 (98.8546)  acc5: 100.0000 (99.9969)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [260/468]  eta: 0:00:08  lr: 0.0006909830056250527  img/s: 3570.4988062222756  loss: 0.5202 (0.5281)  acc1: 99.2188 (98.8685)  acc5: 100.0000 (99.9940)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [270/468]  eta: 0:00:07  lr: 0.0006909830056250527  img/s: 3405.525712509594  loss: 0.5211 (0.5282)  acc1: 99.2188 (98.8555)  acc5: 100.0000 (99.9942)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [280/468]  eta: 0:00:07  lr: 0.0006909830056250527  img/s: 3477.5711518904527  loss: 0.5305 (0.5286)  acc1: 98.4375 (98.8295)  acc5: 100.0000 (99.9944)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [290/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3542.112530349414  loss: 0.5300 (0.5288)  acc1: 98.4375 (98.8187)  acc5: 100.0000 (99.9946)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [300/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3544.1234734159834  loss: 0.5232 (0.5288)  acc1: 99.2188 (98.8268)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [310/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3441.08315707162  loss: 0.5262 (0.5292)  acc1: 99.2188 (98.8118)  acc5: 100.0000 (99.9950)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [320/468]  eta: 0:00:05  lr: 0.0006909830056250527  img/s: 3481.9466751412247  loss: 0.5417 (0.5296)  acc1: 98.4375 (98.7977)  acc5: 100.0000 (99.9951)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [330/468]  eta: 0:00:05  lr: 0.0006909830056250527  img/s: 3582.1245170975812  loss: 0.5399 (0.5302)  acc1: 97.6562 (98.7632)  acc5: 100.0000 (99.9953)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [340/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3493.865795484866  loss: 0.5399 (0.5307)  acc1: 97.6562 (98.7422)  acc5: 100.0000 (99.9954)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [350/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3524.671489909269  loss: 0.5373 (0.5309)  acc1: 98.4375 (98.7313)  acc5: 100.0000 (99.9955)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [360/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3499.719120753044  loss: 0.5248 (0.5307)  acc1: 98.4375 (98.7383)  acc5: 100.0000 (99.9957)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [370/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3413.9063461783035  loss: 0.5168 (0.5305)  acc1: 99.2188 (98.7449)  acc5: 100.0000 (99.9958)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [380/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3589.9571508813224  loss: 0.5213 (0.5303)  acc1: 99.2188 (98.7533)  acc5: 100.0000 (99.9959)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [390/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3470.4450736273257  loss: 0.5237 (0.5301)  acc1: 99.2188 (98.7612)  acc5: 100.0000 (99.9960)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [400/468]  eta: 0:00:02  lr: 0.0006909830056250527  img/s: 3534.463791014905  loss: 0.5256 (0.5300)  acc1: 99.2188 (98.7629)  acc5: 100.0000 (99.9961)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [410/468]  eta: 0:00:02  lr: 0.0006909830056250527  img/s: 2385.032927587739  loss: 0.5269 (0.5300)  acc1: 98.4375 (98.7625)  acc5: 100.0000 (99.9962)  time: 0.0418  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [10]  [420/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 1669.8991660938293  loss: 0.5245 (0.5301)  acc1: 98.4375 (98.7622)  acc5: 100.0000 (99.9963)  time: 0.0561  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [10]  [430/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 3159.8113780561016  loss: 0.5219 (0.5300)  acc1: 98.4375 (98.7583)  acc5: 100.0000 (99.9964)  time: 0.0600  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [10]  [440/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 3148.636799230539  loss: 0.5261 (0.5301)  acc1: 98.4375 (98.7564)  acc5: 100.0000 (99.9929)  time: 0.0488  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [10]  [450/468]  eta: 0:00:00  lr: 0.0006909830056250527  img/s: 3290.860071104573  loss: 0.5279 (0.5301)  acc1: 98.4375 (98.7562)  acc5: 100.0000 (99.9931)  time: 0.0413  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [10]  [460/468]  eta: 0:00:00  lr: 0.0006909830056250527  img/s: 1988.366599260757  loss: 0.5279 (0.5301)  acc1: 99.2188 (98.7578)  acc5: 100.0000 (99.9932)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5544 (0.5544)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1328  data: 0.1155  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.270 Acc@5 99.970</span><br><span class="line">Epoch: [11]  [  0/468]  eta: 0:01:20  lr: 0.0005000000000000002  img/s: 2074.494628974173  loss: 0.5164 (0.5164)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1717  data: 0.1100  max mem: 190</span><br><span class="line">Epoch: [11]  [ 10/468]  eta: 0:00:25  lr: 0.0005000000000000002  img/s: 3200.5181197651195  loss: 0.5164 (0.5185)  acc1: 99.2188 (99.1477)  acc5: 100.0000 (100.0000)  time: 0.0554  data: 0.0103  max mem: 190</span><br><span class="line">Epoch: [11]  [ 20/468]  eta: 0:00:21  lr: 0.0005000000000000002  img/s: 3149.2278254544603  loss: 0.5217 (0.5232)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (100.0000)  time: 0.0423  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [ 30/468]  eta: 0:00:20  lr: 0.0005000000000000002  img/s: 3227.5514728868584  loss: 0.5288 (0.5235)  acc1: 99.2188 (98.9919)  acc5: 100.0000 (100.0000)  time: 0.0406  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [ 40/468]  eta: 0:00:19  lr: 0.0005000000000000002  img/s: 3470.4226400946354  loss: 0.5152 (0.5219)  acc1: 99.2188 (99.0663)  acc5: 100.0000 (99.9809)  time: 0.0402  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 50/468]  eta: 0:00:18  lr: 0.0005000000000000002  img/s: 3297.773387879458  loss: 0.5106 (0.5212)  acc1: 99.2188 (99.0656)  acc5: 100.0000 (99.9847)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 60/468]  eta: 0:00:17  lr: 0.0005000000000000002  img/s: 3524.5095158378467  loss: 0.5191 (0.5218)  acc1: 99.2188 (99.0523)  acc5: 100.0000 (99.9872)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 70/468]  eta: 0:00:16  lr: 0.0005000000000000002  img/s: 3443.8423277504444  loss: 0.5181 (0.5218)  acc1: 99.2188 (99.0427)  acc5: 100.0000 (99.9890)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 80/468]  eta: 0:00:15  lr: 0.0005000000000000002  img/s: 3546.980126849894  loss: 0.5163 (0.5221)  acc1: 99.2188 (99.0355)  acc5: 100.0000 (99.9904)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 90/468]  eta: 0:00:15  lr: 0.0005000000000000002  img/s: 3493.1610753975483  loss: 0.5179 (0.5222)  acc1: 99.2188 (99.0299)  acc5: 100.0000 (99.9914)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [100/468]  eta: 0:00:14  lr: 0.0005000000000000002  img/s: 3373.745770806626  loss: 0.5201 (0.5224)  acc1: 99.2188 (99.0176)  acc5: 100.0000 (99.9923)  time: 0.0377  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [110/468]  eta: 0:00:14  lr: 0.0005000000000000002  img/s: 3415.90471342767  loss: 0.5171 (0.5224)  acc1: 99.2188 (99.0146)  acc5: 100.0000 (99.9930)  time: 0.0377  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [120/468]  eta: 0:00:13  lr: 0.0005000000000000002  img/s: 3553.130498087334  loss: 0.5166 (0.5222)  acc1: 99.2188 (99.0121)  acc5: 100.0000 (99.9935)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [130/468]  eta: 0:00:13  lr: 0.0005000000000000002  img/s: 3494.0249651815116  loss: 0.5192 (0.5222)  acc1: 99.2188 (99.0160)  acc5: 100.0000 (99.9940)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [140/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3481.5402354009275  loss: 0.5231 (0.5224)  acc1: 99.2188 (99.0193)  acc5: 100.0000 (99.9945)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [150/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3100.5042389521586  loss: 0.5230 (0.5225)  acc1: 99.2188 (99.0273)  acc5: 100.0000 (99.9948)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [160/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3358.0876940590715  loss: 0.5219 (0.5226)  acc1: 99.2188 (99.0392)  acc5: 100.0000 (99.9951)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [170/468]  eta: 0:00:11  lr: 0.0005000000000000002  img/s: 3427.9004456703574  loss: 0.5155 (0.5224)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (99.9954)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [180/468]  eta: 0:00:11  lr: 0.0005000000000000002  img/s: 3247.5632096301  loss: 0.5159 (0.5225)  acc1: 99.2188 (99.0331)  acc5: 100.0000 (99.9957)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [190/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3533.370487617067  loss: 0.5161 (0.5224)  acc1: 99.2188 (99.0388)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [200/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3471.7018145135216  loss: 0.5194 (0.5228)  acc1: 98.4375 (99.0244)  acc5: 100.0000 (99.9961)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [210/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3124.103347143988  loss: 0.5306 (0.5233)  acc1: 98.4375 (99.0077)  acc5: 100.0000 (99.9963)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [220/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 3118.1337344709223  loss: 0.5267 (0.5233)  acc1: 99.2188 (99.0243)  acc5: 100.0000 (99.9965)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [230/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 2732.584679594849  loss: 0.5109 (0.5232)  acc1: 100.0000 (99.0294)  acc5: 100.0000 (99.9966)  time: 0.0512  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [11]  [240/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 3494.184149381374  loss: 0.5244 (0.5235)  acc1: 98.4375 (99.0113)  acc5: 100.0000 (99.9935)  time: 0.0530  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [11]  [250/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3441.568450473089  loss: 0.5169 (0.5231)  acc1: 99.2188 (99.0382)  acc5: 100.0000 (99.9938)  time: 0.0464  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [11]  [260/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3567.485626951957  loss: 0.5087 (0.5230)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (99.9940)  time: 0.0440  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [11]  [270/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3464.443245615167  loss: 0.5222 (0.5231)  acc1: 99.2188 (99.0515)  acc5: 100.0000 (99.9942)  time: 0.0395  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [11]  [280/468]  eta: 0:00:07  lr: 0.0005000000000000002  img/s: 3437.4478144228246  loss: 0.5222 (0.5231)  acc1: 99.2188 (99.0547)  acc5: 100.0000 (99.9944)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [290/468]  eta: 0:00:07  lr: 0.0005000000000000002  img/s: 3604.3216069606315  loss: 0.5156 (0.5230)  acc1: 99.2188 (99.0630)  acc5: 100.0000 (99.9946)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [300/468]  eta: 0:00:06  lr: 0.0005000000000000002  img/s: 3499.4225672513476  loss: 0.5130 (0.5227)  acc1: 99.2188 (99.0734)  acc5: 100.0000 (99.9922)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [310/468]  eta: 0:00:06  lr: 0.0005000000000000002  img/s: 3521.6196261069204  loss: 0.5125 (0.5226)  acc1: 99.2188 (99.0756)  acc5: 100.0000 (99.9925)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [320/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3499.6506808685394  loss: 0.5142 (0.5223)  acc1: 99.2188 (99.0873)  acc5: 100.0000 (99.9927)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [330/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3439.870522127466  loss: 0.5142 (0.5224)  acc1: 99.2188 (99.0889)  acc5: 100.0000 (99.9906)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [340/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3406.044244811988  loss: 0.5148 (0.5224)  acc1: 99.2188 (99.0927)  acc5: 100.0000 (99.9908)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [350/468]  eta: 0:00:04  lr: 0.0005000000000000002  img/s: 3536.6524288217547  loss: 0.5167 (0.5225)  acc1: 99.2188 (99.0919)  acc5: 100.0000 (99.9889)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [360/468]  eta: 0:00:04  lr: 0.0005000000000000002  img/s: 3499.1944833699413  loss: 0.5167 (0.5223)  acc1: 99.2188 (99.1041)  acc5: 100.0000 (99.9870)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [11]  [370/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3468.449624322456  loss: 0.5129 (0.5222)  acc1: 99.2188 (99.1029)  acc5: 100.0000 (99.9874)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [380/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3447.4469402170425  loss: 0.5251 (0.5225)  acc1: 99.2188 (99.0957)  acc5: 100.0000 (99.9877)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [390/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3585.473750292183  loss: 0.5270 (0.5227)  acc1: 98.4375 (99.0869)  acc5: 100.0000 (99.9880)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [400/468]  eta: 0:00:02  lr: 0.0005000000000000002  img/s: 3455.968689248516  loss: 0.5184 (0.5230)  acc1: 99.2188 (99.0804)  acc5: 100.0000 (99.9864)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [410/468]  eta: 0:00:02  lr: 0.0005000000000000002  img/s: 3584.205089860336  loss: 0.5204 (0.5232)  acc1: 98.4375 (99.0705)  acc5: 100.0000 (99.9867)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [420/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3539.0304021094266  loss: 0.5257 (0.5233)  acc1: 98.4375 (99.0684)  acc5: 100.0000 (99.9852)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [430/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3503.738951105542  loss: 0.5203 (0.5233)  acc1: 99.2188 (99.0683)  acc5: 100.0000 (99.9855)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [440/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3516.6140161266026  loss: 0.5149 (0.5232)  acc1: 99.2188 (99.0699)  acc5: 100.0000 (99.9858)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [450/468]  eta: 0:00:00  lr: 0.0005000000000000002  img/s: 3491.661650125522  loss: 0.5159 (0.5232)  acc1: 99.2188 (99.0698)  acc5: 100.0000 (99.9861)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [11]  [460/468]  eta: 0:00:00  lr: 0.0005000000000000002  img/s: 3486.265305592353  loss: 0.5175 (0.5231)  acc1: 99.2188 (99.0747)  acc5: 100.0000 (99.9864)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5518 (0.5518)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1369  data: 0.1105  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.430 Acc@5 99.990</span><br><span class="line">Epoch: [12]  [  0/468]  eta: 0:02:14  lr: 0.0003308693936411421  img/s: 948.6122759093495  loss: 0.5233 (0.5233)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.2872  data: 0.1522  max mem: 190</span><br><span class="line">Epoch: [12]  [ 10/468]  eta: 0:00:36  lr: 0.0003308693936411421  img/s: 3477.0756526751425  loss: 0.5233 (0.5273)  acc1: 99.2188 (98.9347)  acc5: 100.0000 (100.0000)  time: 0.0804  data: 0.0196  max mem: 190</span><br><span class="line">Epoch: [12]  [ 20/468]  eta: 0:00:26  lr: 0.0003308693936411421  img/s: 3368.8977353304763  loss: 0.5224 (0.5260)  acc1: 99.2188 (99.0327)  acc5: 100.0000 (100.0000)  time: 0.0488  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [12]  [ 30/468]  eta: 0:00:23  lr: 0.0003308693936411421  img/s: 3471.9487812922375  loss: 0.5112 (0.5222)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 40/468]  eta: 0:00:21  lr: 0.0003308693936411421  img/s: 3426.893938620232  loss: 0.5088 (0.5198)  acc1: 99.2188 (99.2950)  acc5: 100.0000 (100.0000)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 50/468]  eta: 0:00:19  lr: 0.0003308693936411421  img/s: 3567.248584717608  loss: 0.5111 (0.5197)  acc1: 99.2188 (99.3107)  acc5: 100.0000 (99.9694)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 60/468]  eta: 0:00:18  lr: 0.0003308693936411421  img/s: 3598.4511009082075  loss: 0.5193 (0.5200)  acc1: 99.2188 (99.2572)  acc5: 100.0000 (99.9744)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 70/468]  eta: 0:00:17  lr: 0.0003308693936411421  img/s: 3365.2866634907105  loss: 0.5134 (0.5187)  acc1: 99.2188 (99.2958)  acc5: 100.0000 (99.9780)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 80/468]  eta: 0:00:16  lr: 0.0003308693936411421  img/s: 3487.42026048264  loss: 0.5116 (0.5181)  acc1: 99.2188 (99.3248)  acc5: 100.0000 (99.9807)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 90/468]  eta: 0:00:16  lr: 0.0003308693936411421  img/s: 2787.694391077233  loss: 0.5146 (0.5188)  acc1: 99.2188 (99.2788)  acc5: 100.0000 (99.9742)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [100/468]  eta: 0:00:15  lr: 0.0003308693936411421  img/s: 3409.505166293034  loss: 0.5101 (0.5178)  acc1: 99.2188 (99.3038)  acc5: 100.0000 (99.9768)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [110/468]  eta: 0:00:14  lr: 0.0003308693936411421  img/s: 3557.1090512757655  loss: 0.5063 (0.5172)  acc1: 100.0000 (99.3243)  acc5: 100.0000 (99.9789)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [120/468]  eta: 0:00:14  lr: 0.0003308693936411421  img/s: 3579.1871358284775  loss: 0.5063 (0.5173)  acc1: 100.0000 (99.3285)  acc5: 100.0000 (99.9806)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [130/468]  eta: 0:00:13  lr: 0.0003308693936411421  img/s: 3457.0591318570223  loss: 0.5063 (0.5167)  acc1: 99.2188 (99.3380)  acc5: 100.0000 (99.9821)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [140/468]  eta: 0:00:13  lr: 0.0003308693936411421  img/s: 3483.822042257177  loss: 0.5083 (0.5166)  acc1: 99.2188 (99.3351)  acc5: 100.0000 (99.9834)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [150/468]  eta: 0:00:12  lr: 0.0003308693936411421  img/s: 3503.2359673735727  loss: 0.5090 (0.5161)  acc1: 99.2188 (99.3429)  acc5: 100.0000 (99.9845)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [160/468]  eta: 0:00:12  lr: 0.0003308693936411421  img/s: 3429.1046543563934  loss: 0.5088 (0.5158)  acc1: 99.2188 (99.3595)  acc5: 100.0000 (99.9854)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [170/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 3477.0981723034674  loss: 0.5088 (0.5158)  acc1: 99.2188 (99.3604)  acc5: 100.0000 (99.9863)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [180/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 2182.446439968292  loss: 0.5171 (0.5164)  acc1: 99.2188 (99.3482)  acc5: 100.0000 (99.9871)  time: 0.0454  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [190/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 2737.196131315037  loss: 0.5142 (0.5163)  acc1: 99.2188 (99.3496)  acc5: 100.0000 (99.9877)  time: 0.0563  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [12]  [200/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 3381.0972755784514  loss: 0.5142 (0.5168)  acc1: 99.2188 (99.3276)  acc5: 100.0000 (99.9883)  time: 0.0535  data: 0.0055  max mem: 190</span><br><span class="line">Epoch: [12]  [210/468]  eta: 0:00:10  lr: 0.0003308693936411421  img/s: 3422.4374123466864  loss: 0.5139 (0.5166)  acc1: 99.2188 (99.3372)  acc5: 100.0000 (99.9889)  time: 0.0428  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [12]  [220/468]  eta: 0:00:10  lr: 0.0003308693936411421  img/s: 3410.0898904951855  loss: 0.5099 (0.5163)  acc1: 100.0000 (99.3460)  acc5: 100.0000 (99.9894)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [230/468]  eta: 0:00:09  lr: 0.0003308693936411421  img/s: 3545.2936763695916  loss: 0.5056 (0.5161)  acc1: 100.0000 (99.3506)  acc5: 100.0000 (99.9899)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [240/468]  eta: 0:00:09  lr: 0.0003308693936411421  img/s: 3464.9798763408244  loss: 0.5121 (0.5160)  acc1: 99.2188 (99.3517)  acc5: 100.0000 (99.9903)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [250/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 3481.901510483887  loss: 0.5145 (0.5162)  acc1: 99.2188 (99.3464)  acc5: 100.0000 (99.9907)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [260/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 3417.1875067628207  loss: 0.5087 (0.5161)  acc1: 99.2188 (99.3534)  acc5: 100.0000 (99.9880)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [270/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 2321.513592984489  loss: 0.5086 (0.5161)  acc1: 99.2188 (99.3571)  acc5: 100.0000 (99.9885)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [280/468]  eta: 0:00:07  lr: 0.0003308693936411421  img/s: 2544.41190521327  loss: 0.5116 (0.5161)  acc1: 99.2188 (99.3550)  acc5: 100.0000 (99.9889)  time: 0.0479  data: 0.0041  max mem: 190</span><br><span class="line">Epoch: [12]  [290/468]  eta: 0:00:07  lr: 0.0003308693936411421  img/s: 3506.5995571608655  loss: 0.5128 (0.5162)  acc1: 99.2188 (99.3449)  acc5: 100.0000 (99.9893)  time: 0.0457  data: 0.0041  max mem: 190</span><br><span class="line">Epoch: [12]  [300/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 3373.660969233863  loss: 0.5145 (0.5163)  acc1: 99.2188 (99.3433)  acc5: 100.0000 (99.9896)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [310/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 3646.9483394583285  loss: 0.5117 (0.5164)  acc1: 99.2188 (99.3343)  acc5: 100.0000 (99.9900)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [320/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 2982.964190266642  loss: 0.5159 (0.5167)  acc1: 99.2188 (99.3137)  acc5: 100.0000 (99.9903)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [330/468]  eta: 0:00:05  lr: 0.0003308693936411421  img/s: 3034.3518772854954  loss: 0.5159 (0.5167)  acc1: 99.2188 (99.3202)  acc5: 100.0000 (99.9906)  time: 0.0444  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [12]  [340/468]  eta: 0:00:05  lr: 0.0003308693936411421  img/s: 3081.7810433504774  loss: 0.5096 (0.5168)  acc1: 99.2188 (99.3173)  acc5: 100.0000 (99.9908)  time: 0.0476  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [12]  [350/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3615.1219269125363  loss: 0.5097 (0.5167)  acc1: 99.2188 (99.3234)  acc5: 100.0000 (99.9911)  time: 0.0413  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [12]  [360/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3503.578895161027  loss: 0.5110 (0.5166)  acc1: 99.2188 (99.3248)  acc5: 100.0000 (99.9913)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [370/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3446.561674263337  loss: 0.5092 (0.5166)  acc1: 99.2188 (99.3240)  acc5: 100.0000 (99.9916)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [380/468]  eta: 0:00:03  lr: 0.0003308693936411421  img/s: 3527.2419271124195  loss: 0.5092 (0.5165)  acc1: 99.2188 (99.3233)  acc5: 100.0000 (99.9918)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [390/468]  eta: 0:00:03  lr: 0.0003308693936411421  img/s: 3609.555938037866  loss: 0.5093 (0.5164)  acc1: 99.2188 (99.3286)  acc5: 100.0000 (99.9920)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [400/468]  eta: 0:00:02  lr: 0.0003308693936411421  img/s: 3477.7288403487632  loss: 0.5089 (0.5163)  acc1: 99.2188 (99.3298)  acc5: 100.0000 (99.9922)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [410/468]  eta: 0:00:02  lr: 0.0003308693936411421  img/s: 3580.810458213833  loss: 0.5094 (0.5162)  acc1: 99.2188 (99.3309)  acc5: 100.0000 (99.9924)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [420/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3572.375715312342  loss: 0.5097 (0.5163)  acc1: 99.2188 (99.3338)  acc5: 100.0000 (99.9926)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [430/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3535.953632962748  loss: 0.5195 (0.5164)  acc1: 99.2188 (99.3293)  acc5: 100.0000 (99.9927)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [440/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3534.1380554275556  loss: 0.5130 (0.5163)  acc1: 99.2188 (99.3339)  acc5: 100.0000 (99.9929)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [450/468]  eta: 0:00:00  lr: 0.0003308693936411421  img/s: 3457.749359164273  loss: 0.5076 (0.5163)  acc1: 99.2188 (99.3383)  acc5: 100.0000 (99.9931)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [460/468]  eta: 0:00:00  lr: 0.0003308693936411421  img/s: 3558.1227681826012  loss: 0.5090 (0.5163)  acc1: 99.2188 (99.3374)  acc5: 100.0000 (99.9932)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5338 (0.5338)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1201  data: 0.1064  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.620 Acc@5 99.970</span><br><span class="line">Epoch: [13]  [  0/468]  eta: 0:01:24  lr: 0.00019098300562505265  img/s: 1682.8015572007998  loss: 0.5060 (0.5060)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1802  data: 0.1041  max mem: 190</span><br><span class="line">Epoch: [13]  [ 10/468]  eta: 0:00:23  lr: 0.00019098300562505265  img/s: 3474.173064478555  loss: 0.5087 (0.5116)  acc1: 100.0000 (99.7159)  acc5: 100.0000 (100.0000)  time: 0.0514  data: 0.0097  max mem: 190</span><br><span class="line">Epoch: [13]  [ 20/468]  eta: 0:00:20  lr: 0.00019098300562505265  img/s: 3444.748299668917  loss: 0.5079 (0.5102)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [ 30/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 3511.117366225001  loss: 0.5056 (0.5099)  acc1: 100.0000 (99.6976)  acc5: 100.0000 (100.0000)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [ 40/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 1363.658307488716  loss: 0.5043 (0.5086)  acc1: 100.0000 (99.7523)  acc5: 100.0000 (100.0000)  time: 0.0407  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [ 50/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 3069.394789350012  loss: 0.5024 (0.5089)  acc1: 100.0000 (99.7243)  acc5: 100.0000 (100.0000)  time: 0.0470  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [13]  [ 60/468]  eta: 0:00:17  lr: 0.00019098300562505265  img/s: 3438.2623442163103  loss: 0.5078 (0.5095)  acc1: 99.2188 (99.6926)  acc5: 100.0000 (99.9872)  time: 0.0461  data: 0.0028  max mem: 190</span><br><span class="line">Epoch: [13]  [ 70/468]  eta: 0:00:17  lr: 0.00019098300562505265  img/s: 3492.479358842586  loss: 0.5070 (0.5098)  acc1: 99.2188 (99.6589)  acc5: 100.0000 (99.9890)  time: 0.0406  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [13]  [ 80/468]  eta: 0:00:16  lr: 0.00019098300562505265  img/s: 3470.579680914333  loss: 0.5058 (0.5097)  acc1: 99.2188 (99.6431)  acc5: 100.0000 (99.9904)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [ 90/468]  eta: 0:00:15  lr: 0.00019098300562505265  img/s: 3411.7802208975713  loss: 0.5038 (0.5095)  acc1: 100.0000 (99.6394)  acc5: 100.0000 (99.9914)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [100/468]  eta: 0:00:15  lr: 0.00019098300562505265  img/s: 3476.692863618702  loss: 0.5039 (0.5095)  acc1: 100.0000 (99.6364)  acc5: 100.0000 (99.9923)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [110/468]  eta: 0:00:14  lr: 0.00019098300562505265  img/s: 3369.5955011046394  loss: 0.5042 (0.5100)  acc1: 99.2188 (99.6199)  acc5: 100.0000 (99.9930)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [120/468]  eta: 0:00:14  lr: 0.00019098300562505265  img/s: 2763.3305470342384  loss: 0.5049 (0.5098)  acc1: 100.0000 (99.6320)  acc5: 100.0000 (99.9935)  time: 0.0390  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [130/468]  eta: 0:00:13  lr: 0.00019098300562505265  img/s: 3448.089042459586  loss: 0.5049 (0.5101)  acc1: 100.0000 (99.6243)  acc5: 100.0000 (99.9940)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [140/468]  eta: 0:00:13  lr: 0.00019098300562505265  img/s: 3369.5320559087686  loss: 0.5095 (0.5103)  acc1: 99.2188 (99.6121)  acc5: 100.0000 (99.9945)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [150/468]  eta: 0:00:12  lr: 0.00019098300562505265  img/s: 3471.432436277109  loss: 0.5060 (0.5102)  acc1: 99.2188 (99.6171)  acc5: 100.0000 (99.9948)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [160/468]  eta: 0:00:12  lr: 0.00019098300562505265  img/s: 3444.505187247775  loss: 0.5056 (0.5102)  acc1: 100.0000 (99.6167)  acc5: 100.0000 (99.9951)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [170/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3307.5865570033575  loss: 0.5027 (0.5099)  acc1: 100.0000 (99.6254)  acc5: 100.0000 (99.9954)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [180/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3425.866161277766  loss: 0.5031 (0.5099)  acc1: 100.0000 (99.6202)  acc5: 100.0000 (99.9957)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [190/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3484.432536978264  loss: 0.5074 (0.5100)  acc1: 99.2188 (99.6073)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [200/468]  eta: 0:00:10  lr: 0.00019098300562505265  img/s: 3348.119189273464  loss: 0.5049 (0.5099)  acc1: 100.0000 (99.6191)  acc5: 100.0000 (99.9961)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [210/468]  eta: 0:00:10  lr: 0.00019098300562505265  img/s: 3368.7074857250423  loss: 0.5055 (0.5099)  acc1: 100.0000 (99.6149)  acc5: 100.0000 (99.9963)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [220/468]  eta: 0:00:09  lr: 0.00019098300562505265  img/s: 3569.2644483595386  loss: 0.5026 (0.5098)  acc1: 100.0000 (99.6182)  acc5: 100.0000 (99.9965)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [230/468]  eta: 0:00:09  lr: 0.00019098300562505265  img/s: 3386.0659337887014  loss: 0.5020 (0.5096)  acc1: 100.0000 (99.6280)  acc5: 100.0000 (99.9966)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [240/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3378.607779588806  loss: 0.5026 (0.5096)  acc1: 100.0000 (99.6304)  acc5: 100.0000 (99.9968)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [250/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3548.785468294521  loss: 0.5026 (0.5094)  acc1: 100.0000 (99.6452)  acc5: 100.0000 (99.9969)  time: 0.0404  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [13]  [260/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3539.357040201469  loss: 0.5032 (0.5096)  acc1: 100.0000 (99.6288)  acc5: 100.0000 (99.9970)  time: 0.0427  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [13]  [270/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3462.633343437796  loss: 0.5123 (0.5099)  acc1: 99.2188 (99.6166)  acc5: 100.0000 (99.9971)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [280/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3555.130432479323  loss: 0.5059 (0.5099)  acc1: 99.2188 (99.6191)  acc5: 100.0000 (99.9972)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [290/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3626.9915214733046  loss: 0.5069 (0.5104)  acc1: 99.2188 (99.6000)  acc5: 100.0000 (99.9973)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [300/468]  eta: 0:00:06  lr: 0.00019098300562505265  img/s: 3500.6319084009283  loss: 0.5151 (0.5105)  acc1: 99.2188 (99.5977)  acc5: 100.0000 (99.9948)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [310/468]  eta: 0:00:06  lr: 0.00019098300562505265  img/s: 3429.871409588061  loss: 0.5056 (0.5106)  acc1: 100.0000 (99.5981)  acc5: 100.0000 (99.9950)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [320/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3511.392938898845  loss: 0.5083 (0.5109)  acc1: 99.2188 (99.5887)  acc5: 100.0000 (99.9951)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [330/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3008.3037492365365  loss: 0.5073 (0.5108)  acc1: 100.0000 (99.5917)  acc5: 100.0000 (99.9953)  time: 0.0458  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [13]  [340/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3446.9378567347017  loss: 0.5035 (0.5106)  acc1: 100.0000 (99.6014)  acc5: 100.0000 (99.9954)  time: 0.0487  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [13]  [350/468]  eta: 0:00:04  lr: 0.00019098300562505265  img/s: 3523.006181507973  loss: 0.5034 (0.5106)  acc1: 100.0000 (99.5971)  acc5: 100.0000 (99.9955)  time: 0.0402  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [13]  [360/468]  eta: 0:00:04  lr: 0.00019098300562505265  img/s: 3495.3898720002085  loss: 0.5038 (0.5106)  acc1: 100.0000 (99.5996)  acc5: 100.0000 (99.9957)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [370/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3456.858794895239  loss: 0.5027 (0.5105)  acc1: 100.0000 (99.6062)  acc5: 100.0000 (99.9958)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [380/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3468.7857752048176  loss: 0.5026 (0.5104)  acc1: 100.0000 (99.6125)  acc5: 100.0000 (99.9959)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [390/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3539.4037077081302  loss: 0.5050 (0.5105)  acc1: 100.0000 (99.6084)  acc5: 100.0000 (99.9960)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [400/468]  eta: 0:00:02  lr: 0.00019098300562505265  img/s: 3395.0592981857044  loss: 0.5056 (0.5105)  acc1: 100.0000 (99.6103)  acc5: 100.0000 (99.9961)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [410/468]  eta: 0:00:02  lr: 0.00019098300562505265  img/s: 3485.880490607936  loss: 0.5043 (0.5105)  acc1: 100.0000 (99.6084)  acc5: 100.0000 (99.9962)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [420/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3440.598000512689  loss: 0.5027 (0.5104)  acc1: 100.0000 (99.6122)  acc5: 100.0000 (99.9963)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [430/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3459.130640962862  loss: 0.5031 (0.5104)  acc1: 100.0000 (99.6103)  acc5: 100.0000 (99.9964)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [440/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3513.5071006923995  loss: 0.5042 (0.5105)  acc1: 100.0000 (99.6120)  acc5: 100.0000 (99.9947)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [450/468]  eta: 0:00:00  lr: 0.00019098300562505265  img/s: 3505.61498178209  loss: 0.5024 (0.5103)  acc1: 100.0000 (99.6189)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [460/468]  eta: 0:00:00  lr: 0.00019098300562505265  img/s: 3506.6453648245274  loss: 0.5038 (0.5103)  acc1: 100.0000 (99.6153)  acc5: 100.0000 (99.9949)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:14  loss: 0.5582 (0.5582)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1795  data: 0.1629  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:02</span><br><span class="line">Test:  Acc@1 98.670 Acc@5 99.910</span><br><span class="line">Epoch: [14]  [  0/468]  eta: 0:01:18  lr: 8.645454235739902e-05  img/s: 2322.0256738521157  loss: 0.5286 (0.5286)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1679  data: 0.1128  max mem: 190</span><br><span class="line">Epoch: [14]  [ 10/468]  eta: 0:00:24  lr: 8.645454235739902e-05  img/s: 3280.4039594280825  loss: 0.5064 (0.5121)  acc1: 100.0000 (99.5739)  acc5: 100.0000 (100.0000)  time: 0.0536  data: 0.0106  max mem: 190</span><br><span class="line">Epoch: [14]  [ 20/468]  eta: 0:00:20  lr: 8.645454235739902e-05  img/s: 3501.63652491521  loss: 0.5060 (0.5105)  acc1: 100.0000 (99.5908)  acc5: 100.0000 (100.0000)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 30/468]  eta: 0:00:19  lr: 8.645454235739902e-05  img/s: 3416.948268839104  loss: 0.5057 (0.5089)  acc1: 100.0000 (99.6472)  acc5: 100.0000 (100.0000)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 40/468]  eta: 0:00:17  lr: 8.645454235739902e-05  img/s: 3601.3718824208113  loss: 0.5030 (0.5086)  acc1: 100.0000 (99.6951)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 50/468]  eta: 0:00:17  lr: 8.645454235739902e-05  img/s: 3491.0940220960706  loss: 0.5035 (0.5095)  acc1: 100.0000 (99.6630)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [ 60/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 3246.169034863894  loss: 0.5039 (0.5095)  acc1: 100.0000 (99.6670)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [14]  [ 70/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 3279.902935516388  loss: 0.5030 (0.5087)  acc1: 100.0000 (99.6919)  acc5: 100.0000 (100.0000)  time: 0.0399  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [14]  [ 80/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 1911.368477266335  loss: 0.5030 (0.5087)  acc1: 100.0000 (99.7010)  acc5: 100.0000 (99.9904)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 90/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 2397.2909546369933  loss: 0.5029 (0.5091)  acc1: 100.0000 (99.6995)  acc5: 100.0000 (99.9914)  time: 0.0512  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [14]  [100/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 3118.043186860415  loss: 0.5028 (0.5085)  acc1: 100.0000 (99.7215)  acc5: 100.0000 (99.9923)  time: 0.0517  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [14]  [110/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 3220.793885619327  loss: 0.5028 (0.5086)  acc1: 100.0000 (99.7185)  acc5: 100.0000 (99.9930)  time: 0.0421  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [14]  [120/468]  eta: 0:00:14  lr: 8.645454235739902e-05  img/s: 3145.684992587992  loss: 0.5033 (0.5085)  acc1: 100.0000 (99.7224)  acc5: 100.0000 (99.9935)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [130/468]  eta: 0:00:14  lr: 8.645454235739902e-05  img/s: 3213.7759393725346  loss: 0.5029 (0.5083)  acc1: 100.0000 (99.7197)  acc5: 100.0000 (99.9940)  time: 0.0420  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [140/468]  eta: 0:00:13  lr: 8.645454235739902e-05  img/s: 2861.4496807410646  loss: 0.5028 (0.5083)  acc1: 100.0000 (99.7174)  acc5: 100.0000 (99.9945)  time: 0.0417  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [150/468]  eta: 0:00:13  lr: 8.645454235739902e-05  img/s: 2756.477801680983  loss: 0.5024 (0.5081)  acc1: 100.0000 (99.7258)  acc5: 100.0000 (99.9948)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [160/468]  eta: 0:00:12  lr: 8.645454235739902e-05  img/s: 3562.7271170806484  loss: 0.5023 (0.5078)  acc1: 100.0000 (99.7428)  acc5: 100.0000 (99.9951)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [170/468]  eta: 0:00:12  lr: 8.645454235739902e-05  img/s: 3452.013271263599  loss: 0.5018 (0.5078)  acc1: 100.0000 (99.7350)  acc5: 100.0000 (99.9954)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [180/468]  eta: 0:00:11  lr: 8.645454235739902e-05  img/s: 3525.967818628417  loss: 0.5033 (0.5079)  acc1: 100.0000 (99.7194)  acc5: 100.0000 (99.9957)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [190/468]  eta: 0:00:11  lr: 8.645454235739902e-05  img/s: 3569.5254913433155  loss: 0.5049 (0.5080)  acc1: 100.0000 (99.7137)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [200/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3462.164418190729  loss: 0.5022 (0.5078)  acc1: 100.0000 (99.7240)  acc5: 100.0000 (99.9961)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [210/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3587.1746846268975  loss: 0.5022 (0.5081)  acc1: 100.0000 (99.7186)  acc5: 100.0000 (99.9963)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [220/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3513.277177185038  loss: 0.5015 (0.5081)  acc1: 100.0000 (99.7207)  acc5: 100.0000 (99.9965)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [230/468]  eta: 0:00:09  lr: 8.645454235739902e-05  img/s: 3025.5794053335135  loss: 0.5026 (0.5081)  acc1: 100.0000 (99.7193)  acc5: 100.0000 (99.9966)  time: 0.0430  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [240/468]  eta: 0:00:09  lr: 8.645454235739902e-05  img/s: 3155.651043319814  loss: 0.5033 (0.5079)  acc1: 100.0000 (99.7309)  acc5: 100.0000 (99.9968)  time: 0.0479  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [14]  [250/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3337.5248633896767  loss: 0.5024 (0.5077)  acc1: 100.0000 (99.7385)  acc5: 100.0000 (99.9969)  time: 0.0429  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [14]  [260/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3527.5432145813897  loss: 0.5022 (0.5075)  acc1: 100.0000 (99.7426)  acc5: 100.0000 (99.9970)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [270/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3607.8579627165573  loss: 0.5031 (0.5078)  acc1: 100.0000 (99.7348)  acc5: 100.0000 (99.9971)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [280/468]  eta: 0:00:07  lr: 8.645454235739902e-05  img/s: 3467.2176282920655  loss: 0.5059 (0.5078)  acc1: 100.0000 (99.7331)  acc5: 100.0000 (99.9972)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [290/468]  eta: 0:00:07  lr: 8.645454235739902e-05  img/s: 3557.085483336646  loss: 0.5026 (0.5077)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9973)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [300/468]  eta: 0:00:06  lr: 8.645454235739902e-05  img/s: 3545.7619739518664  loss: 0.5026 (0.5081)  acc1: 100.0000 (99.7275)  acc5: 100.0000 (99.9974)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [310/468]  eta: 0:00:06  lr: 8.645454235739902e-05  img/s: 3492.433920532903  loss: 0.5024 (0.5079)  acc1: 100.0000 (99.7337)  acc5: 100.0000 (99.9975)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [320/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3540.3840097070733  loss: 0.5016 (0.5078)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [330/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3359.495591557316  loss: 0.5020 (0.5077)  acc1: 100.0000 (99.7404)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [340/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3507.4701074706823  loss: 0.5033 (0.5076)  acc1: 100.0000 (99.7411)  acc5: 100.0000 (99.9977)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [350/468]  eta: 0:00:04  lr: 8.645454235739902e-05  img/s: 3505.5920912587253  loss: 0.5036 (0.5078)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9978)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [360/468]  eta: 0:00:04  lr: 8.645454235739902e-05  img/s: 3241.112941006013  loss: 0.5040 (0.5078)  acc1: 100.0000 (99.7381)  acc5: 100.0000 (99.9978)  time: 0.0428  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [14]  [370/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3499.468187595737  loss: 0.5020 (0.5078)  acc1: 100.0000 (99.7410)  acc5: 100.0000 (99.9979)  time: 0.0486  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [14]  [380/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3273.283777192469  loss: 0.5021 (0.5079)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9979)  time: 0.0429  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [14]  [390/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3569.5254913433155  loss: 0.5042 (0.5079)  acc1: 100.0000 (99.7383)  acc5: 100.0000 (99.9980)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [400/468]  eta: 0:00:02  lr: 8.645454235739902e-05  img/s: 3462.8343502883163  loss: 0.5022 (0.5080)  acc1: 100.0000 (99.7350)  acc5: 100.0000 (99.9981)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [410/468]  eta: 0:00:02  lr: 8.645454235739902e-05  img/s: 3365.64531235307  loss: 0.5032 (0.5081)  acc1: 100.0000 (99.7320)  acc5: 100.0000 (99.9962)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [420/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3430.1343760382324  loss: 0.5045 (0.5080)  acc1: 100.0000 (99.7328)  acc5: 100.0000 (99.9963)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [430/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3436.6776683864855  loss: 0.5019 (0.5079)  acc1: 100.0000 (99.7372)  acc5: 100.0000 (99.9964)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [440/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3557.2740355945457  loss: 0.5018 (0.5077)  acc1: 100.0000 (99.7431)  acc5: 100.0000 (99.9965)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [450/468]  eta: 0:00:00  lr: 8.645454235739902e-05  img/s: 3501.362481412882  loss: 0.5031 (0.5078)  acc1: 100.0000 (99.7384)  acc5: 100.0000 (99.9965)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [460/468]  eta: 0:00:00  lr: 8.645454235739902e-05  img/s: 3422.76470325719  loss: 0.5031 (0.5077)  acc1: 100.0000 (99.7407)  acc5: 100.0000 (99.9966)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5510 (0.5510)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1211  data: 0.1021  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.750 Acc@5 99.920</span><br><span class="line">Epoch: [15]  [  0/468]  eta: 0:01:14  lr: 2.1852399266194312e-05  img/s: 2757.893798127058  loss: 0.5055 (0.5055)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1598  data: 0.1133  max mem: 190</span><br><span class="line">Epoch: [15]  [ 10/468]  eta: 0:00:22  lr: 2.1852399266194312e-05  img/s: 3457.9943447876076  loss: 0.5040 (0.5057)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0488  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [15]  [ 20/468]  eta: 0:00:19  lr: 2.1852399266194312e-05  img/s: 3491.298346924708  loss: 0.5027 (0.5084)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [ 30/468]  eta: 0:00:18  lr: 2.1852399266194312e-05  img/s: 3435.9738368  loss: 0.5024 (0.5065)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 40/468]  eta: 0:00:17  lr: 2.1852399266194312e-05  img/s: 3355.107689231077  loss: 0.5020 (0.5062)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 50/468]  eta: 0:00:16  lr: 2.1852399266194312e-05  img/s: 3455.2569347011804  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 60/468]  eta: 0:00:16  lr: 2.1852399266194312e-05  img/s: 3422.2628827864046  loss: 0.5019 (0.5068)  acc1: 100.0000 (99.8207)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 70/468]  eta: 0:00:15  lr: 2.1852399266194312e-05  img/s: 3367.4185823334233  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8239)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 80/468]  eta: 0:00:15  lr: 2.1852399266194312e-05  img/s: 3468.8081875803605  loss: 0.5019 (0.5069)  acc1: 100.0000 (99.8264)  acc5: 100.0000 (99.9904)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 90/468]  eta: 0:00:14  lr: 2.1852399266194312e-05  img/s: 3418.9501999643376  loss: 0.5028 (0.5071)  acc1: 100.0000 (99.8025)  acc5: 100.0000 (99.9914)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [100/468]  eta: 0:00:14  lr: 2.1852399266194312e-05  img/s: 3447.491215452584  loss: 0.5026 (0.5069)  acc1: 100.0000 (99.8066)  acc5: 100.0000 (99.9923)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [110/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 3479.8252020663595  loss: 0.5016 (0.5068)  acc1: 100.0000 (99.8100)  acc5: 100.0000 (99.9930)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [120/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 3482.601694365521  loss: 0.5021 (0.5070)  acc1: 100.0000 (99.7998)  acc5: 100.0000 (99.9935)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [130/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 1422.856818766084  loss: 0.5022 (0.5067)  acc1: 100.0000 (99.8092)  acc5: 100.0000 (99.9940)  time: 0.0414  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [140/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 2415.8780340734206  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8005)  acc5: 100.0000 (99.9945)  time: 0.0490  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [15]  [150/468]  eta: 0:00:12  lr: 2.1852399266194312e-05  img/s: 3300.9161906752825  loss: 0.5014 (0.5066)  acc1: 100.0000 (99.8086)  acc5: 100.0000 (99.9948)  time: 0.0458  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [15]  [160/468]  eta: 0:00:12  lr: 2.1852399266194312e-05  img/s: 3474.397898033937  loss: 0.5013 (0.5064)  acc1: 100.0000 (99.8156)  acc5: 100.0000 (99.9951)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [170/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3440.487756736839  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8264)  acc5: 100.0000 (99.9954)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [180/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3407.2763921150504  loss: 0.5015 (0.5062)  acc1: 100.0000 (99.8273)  acc5: 100.0000 (99.9957)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [190/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3543.2814055095764  loss: 0.5026 (0.5062)  acc1: 100.0000 (99.8200)  acc5: 100.0000 (99.9959)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [200/468]  eta: 0:00:10  lr: 2.1852399266194312e-05  img/s: 3530.1642677257514  loss: 0.5032 (0.5064)  acc1: 100.0000 (99.8134)  acc5: 100.0000 (99.9961)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [210/468]  eta: 0:00:10  lr: 2.1852399266194312e-05  img/s: 3561.9234499917065  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8223)  acc5: 100.0000 (99.9963)  time: 0.0372  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [15]  [220/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 3406.4332476761524  loss: 0.5014 (0.5061)  acc1: 100.0000 (99.8268)  acc5: 100.0000 (99.9965)  time: 0.0373  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [15]  [230/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 2149.091168193809  loss: 0.5022 (0.5064)  acc1: 100.0000 (99.8174)  acc5: 100.0000 (99.9966)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [240/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 1784.0806850921997  loss: 0.5031 (0.5063)  acc1: 100.0000 (99.8185)  acc5: 100.0000 (99.9968)  time: 0.0456  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [250/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 2532.445798761303  loss: 0.5021 (0.5064)  acc1: 100.0000 (99.8164)  acc5: 100.0000 (99.9969)  time: 0.0536  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [260/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 2783.5606619934465  loss: 0.5015 (0.5062)  acc1: 100.0000 (99.8234)  acc5: 100.0000 (99.9970)  time: 0.0567  data: 0.0063  max mem: 190</span><br><span class="line">Epoch: [15]  [270/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 3299.638071122147  loss: 0.5016 (0.5064)  acc1: 100.0000 (99.8155)  acc5: 100.0000 (99.9971)  time: 0.0464  data: 0.0040  max mem: 190</span><br><span class="line">Epoch: [15]  [280/468]  eta: 0:00:07  lr: 2.1852399266194312e-05  img/s: 3415.40490231629  loss: 0.5020 (0.5065)  acc1: 100.0000 (99.8165)  acc5: 100.0000 (99.9972)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [290/468]  eta: 0:00:07  lr: 2.1852399266194312e-05  img/s: 3205.0463977839863  loss: 0.5019 (0.5065)  acc1: 100.0000 (99.8148)  acc5: 100.0000 (99.9973)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [300/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3537.0485357578154  loss: 0.5014 (0.5064)  acc1: 100.0000 (99.8183)  acc5: 100.0000 (99.9974)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [310/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3132.0132077893286  loss: 0.5016 (0.5064)  acc1: 100.0000 (99.8216)  acc5: 100.0000 (99.9975)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [320/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3513.254186489369  loss: 0.5017 (0.5063)  acc1: 100.0000 (99.8223)  acc5: 100.0000 (99.9976)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [330/468]  eta: 0:00:05  lr: 2.1852399266194312e-05  img/s: 3602.870319168926  loss: 0.5017 (0.5062)  acc1: 100.0000 (99.8253)  acc5: 100.0000 (99.9976)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [340/468]  eta: 0:00:05  lr: 2.1852399266194312e-05  img/s: 3537.328194079316  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8305)  acc5: 100.0000 (99.9977)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [350/468]  eta: 0:00:04  lr: 2.1852399266194312e-05  img/s: 3544.029890550942  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8331)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [360/468]  eta: 0:00:04  lr: 2.1852399266194312e-05  img/s: 3440.9067206747595  loss: 0.5013 (0.5060)  acc1: 100.0000 (99.8312)  acc5: 100.0000 (99.9978)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [370/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3565.566490227202  loss: 0.5013 (0.5059)  acc1: 100.0000 (99.8315)  acc5: 100.0000 (99.9979)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [380/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3579.234859596256  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (99.9979)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [390/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3276.220102642965  loss: 0.5025 (0.5059)  acc1: 100.0000 (99.8322)  acc5: 100.0000 (99.9980)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [400/468]  eta: 0:00:02  lr: 2.1852399266194312e-05  img/s: 2341.0613270076046  loss: 0.5033 (0.5059)  acc1: 100.0000 (99.8344)  acc5: 100.0000 (99.9981)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [410/468]  eta: 0:00:02  lr: 2.1852399266194312e-05  img/s: 2289.818783587819  loss: 0.5022 (0.5059)  acc1: 100.0000 (99.8327)  acc5: 100.0000 (99.9981)  time: 0.0504  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [420/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3458.3730272227162  loss: 0.5017 (0.5060)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9981)  time: 0.0494  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [430/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3442.3407903257867  loss: 0.5016 (0.5060)  acc1: 100.0000 (99.8296)  acc5: 100.0000 (99.9982)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [440/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3324.9780882662603  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8335)  acc5: 100.0000 (99.9982)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [450/468]  eta: 0:00:00  lr: 2.1852399266194312e-05  img/s: 3340.619202289839  loss: 0.5010 (0.5058)  acc1: 100.0000 (99.8354)  acc5: 100.0000 (99.9983)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [460/468]  eta: 0:00:00  lr: 2.1852399266194312e-05  img/s: 3465.6285269796595  loss: 0.5015 (0.5059)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1216  data: 0.1018  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [16]  [  0/468]  eta: 0:01:19  lr: 0.0  img/s: 1982.2950379015851  loss: 0.5016 (0.5016)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1703  data: 0.1057  max mem: 190</span><br><span class="line">Epoch: [16]  [ 10/468]  eta: 0:00:23  lr: 0.0  img/s: 3292.55539201256  loss: 0.5020 (0.5069)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0522  data: 0.0099  max mem: 190</span><br><span class="line">Epoch: [16]  [ 20/468]  eta: 0:00:20  lr: 0.0  img/s: 3561.07290346973  loss: 0.5019 (0.5053)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3545.9024873518883  loss: 0.5019 (0.5074)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3467.934319488405  loss: 0.5022 (0.5069)  acc1: 100.0000 (99.8095)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3523.9311585165738  loss: 0.5020 (0.5070)  acc1: 100.0000 (99.8162)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3601.251095056983  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3460.6022508991996  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3590.8215527850607  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8553)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3534.2311166115887  loss: 0.5014 (0.5057)  acc1: 100.0000 (99.8541)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3035.878988023207  loss: 0.5015 (0.5057)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3506.8744210959494  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8663)  acc5: 100.0000 (100.0000)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3458.4398621444907  loss: 0.5019 (0.5057)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3514.818239549576  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8688)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3442.8043606515325  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8670)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3508.180613458447  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [160/468]  eta: 0:00:11  lr: 0.0  img/s: 3484.9753786034676  loss: 0.5014 (0.5057)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 1982.7416128698683  loss: 0.5026 (0.5058)  acc1: 100.0000 (99.8538)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3118.242398545632  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (100.0000)  time: 0.0478  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [16]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3482.8276202091497  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8568)  acc5: 100.0000 (100.0000)  time: 0.0482  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [16]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3515.3475727138198  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8601)  acc5: 100.0000 (100.0000)  time: 0.0395  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [16]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3514.680178853166  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3472.6674299316296  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8657)  acc5: 100.0000 (100.0000)  time: 0.0402  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [16]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3407.752196211852  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (100.0000)  time: 0.0424  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [16]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3496.163792654337  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8606)  acc5: 100.0000 (100.0000)  time: 0.0393  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [16]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3485.065868652182  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8568)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3557.4861807796547  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3434.3473299045572  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8616)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3463.7950385496306  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3488.0546788204033  loss: 0.5012 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3538.9370880134998  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3528.9344393758133  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3418.710715172664  loss: 0.5019 (0.5056)  acc1: 100.0000 (99.8588)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3593.008425857142  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3582.7460443513137  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8648)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3636.6961917277445  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (100.0000)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3500.449312781994  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3570.3563367449406  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8673)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3518.6422246836064  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3430.5946643662737  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3495.4126294818125  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8656)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3535.2085550229153  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3448.155480481445  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3298.2798867133984  loss: 0.5025 (0.5054)  acc1: 100.0000 (99.8659)  acc5: 100.0000 (99.9982)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3572.4945734267594  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3348.2653561427687  loss: 0.5011 (0.5054)  acc1: 100.0000 (99.8666)  acc5: 100.0000 (99.9983)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 2339.847162961381  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0439  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [16] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:12  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1549  data: 0.1313  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [17]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2452.9998766351555  loss: 0.5031 (0.5031)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1589  data: 0.1066  max mem: 190</span><br><span class="line">Epoch: [17]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3470.1086010871745  loss: 0.5013 (0.5042)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0490  data: 0.0100  max mem: 190</span><br><span class="line">Epoch: [17]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 2451.6332716852753  loss: 0.5011 (0.5029)  acc1: 100.0000 (99.9628)  acc5: 100.0000 (100.0000)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 30/468]  eta: 0:00:21  lr: 0.0  img/s: 1973.3548187899728  loss: 0.5012 (0.5050)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (99.9748)  time: 0.0498  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [17]  [ 40/468]  eta: 0:00:22  lr: 0.0  img/s: 2807.2709171054626  loss: 0.5012 (0.5047)  acc1: 100.0000 (99.9047)  acc5: 100.0000 (99.9809)  time: 0.0589  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [17]  [ 50/468]  eta: 0:00:21  lr: 0.0  img/s: 3277.0000122077763  loss: 0.5015 (0.5047)  acc1: 100.0000 (99.9081)  acc5: 100.0000 (99.9847)  time: 0.0519  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [17]  [ 60/468]  eta: 0:00:19  lr: 0.0  img/s: 3144.763687697327  loss: 0.5018 (0.5050)  acc1: 100.0000 (99.8847)  acc5: 100.0000 (99.9872)  time: 0.0432  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [17]  [ 70/468]  eta: 0:00:18  lr: 0.0  img/s: 3306.7716547072773  loss: 0.5023 (0.5064)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (99.9890)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 80/468]  eta: 0:00:17  lr: 0.0  img/s: 3440.2232004972543  loss: 0.5015 (0.5064)  acc1: 100.0000 (99.8457)  acc5: 100.0000 (99.9904)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 90/468]  eta: 0:00:17  lr: 0.0  img/s: 3507.928465484008  loss: 0.5013 (0.5062)  acc1: 100.0000 (99.8369)  acc5: 100.0000 (99.9914)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [100/468]  eta: 0:00:16  lr: 0.0  img/s: 3577.9944551076987  loss: 0.5023 (0.5063)  acc1: 100.0000 (99.8376)  acc5: 100.0000 (99.9923)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [110/468]  eta: 0:00:15  lr: 0.0  img/s: 3463.035380477201  loss: 0.5026 (0.5063)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9930)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [120/468]  eta: 0:00:15  lr: 0.0  img/s: 3505.0428083645074  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8450)  acc5: 100.0000 (99.9935)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3500.129816280495  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (99.9940)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3441.568450473089  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8559)  acc5: 100.0000 (99.9945)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3556.6141901291817  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8655)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3476.3551785540844  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9951)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3535.7440480502632  loss: 0.5022 (0.5056)  acc1: 100.0000 (99.8584)  acc5: 100.0000 (99.9954)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3560.3172030531923  loss: 0.5022 (0.5056)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (99.9957)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3294.737658639566  loss: 0.5019 (0.5057)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3465.4048269140153  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8523)  acc5: 100.0000 (99.9961)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3518.665285952103  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8556)  acc5: 100.0000 (99.9963)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 1131.3978588889825  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9965)  time: 0.0452  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 2135.9070318871713  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0558  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [17]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3016.535442981076  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9968)  time: 0.0512  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [17]  [250/468]  eta: 0:00:09  lr: 0.0  img/s: 2825.2497658215193  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8693)  acc5: 100.0000 (99.9969)  time: 0.0456  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [17]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 2882.9463170499885  loss: 0.5016 (0.5051)  acc1: 100.0000 (99.8713)  acc5: 100.0000 (99.9970)  time: 0.0464  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [17]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3288.562068923273  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.8616)  acc5: 100.0000 (99.9971)  time: 0.0437  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3306.242183506691  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9972)  time: 0.0411  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3393.085239374309  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9973)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [300/468]  eta: 0:00:07  lr: 0.0  img/s: 3480.4342966795025  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9974)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3473.678533069348  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9975)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3557.3211767824014  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8613)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3493.024710796497  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8607)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3491.5481097533216  loss: 0.5016 (0.5052)  acc1: 100.0000 (99.8648)  acc5: 100.0000 (99.9977)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3523.5148587629947  loss: 0.5014 (0.5052)  acc1: 100.0000 (99.8687)  acc5: 100.0000 (99.9978)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3506.46214135028  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9978)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3623.613226331171  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3479.9379812803027  loss: 0.5022 (0.5053)  acc1: 100.0000 (99.8606)  acc5: 100.0000 (99.9979)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3509.7630961331024  loss: 0.5022 (0.5053)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9980)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3538.9837444463487  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3372.5589366032614  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3518.20411800941  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8682)  acc5: 100.0000 (99.9981)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3518.18106278547  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8659)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3516.79830209814  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3461.16000593116  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8632)  acc5: 100.0000 (99.9983)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3561.0492829758164  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.1093  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [18]  [  0/468]  eta: 0:01:43  lr: 0.0  img/s: 1582.0426281777382  loss: 0.5010 (0.5010)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.2210  data: 0.1400  max mem: 190</span><br><span class="line">Epoch: [18]  [ 10/468]  eta: 0:00:26  lr: 0.0  img/s: 3425.472707667375  loss: 0.5014 (0.5025)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0577  data: 0.0130  max mem: 190</span><br><span class="line">Epoch: [18]  [ 20/468]  eta: 0:00:21  lr: 0.0  img/s: 3583.296036735947  loss: 0.5018 (0.5044)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (100.0000)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [ 30/468]  eta: 0:00:19  lr: 0.0  img/s: 3592.9122436004686  loss: 0.5023 (0.5055)  acc1: 100.0000 (99.8236)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 40/468]  eta: 0:00:18  lr: 0.0  img/s: 3484.839652340986  loss: 0.5023 (0.5059)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3621.437806918137  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8162)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3508.08891909199  loss: 0.5019 (0.5055)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3505.271655316954  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3068.0616960117036  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8457)  acc5: 100.0000 (100.0000)  time: 0.0387  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3119.982054336772  loss: 0.5018 (0.5058)  acc1: 100.0000 (99.8455)  acc5: 100.0000 (100.0000)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3181.7023652192465  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3458.952349045177  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8522)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3476.737893250787  loss: 0.5021 (0.5061)  acc1: 100.0000 (99.8386)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3438.6367170737017  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3564.4065329969458  loss: 0.5016 (0.5057)  acc1: 100.0000 (99.8504)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 2829.6127294671987  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8500)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3410.263180628605  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8496)  acc5: 100.0000 (100.0000)  time: 0.0419  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3449.817263515033  loss: 0.5020 (0.5058)  acc1: 100.0000 (99.8492)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3422.459229793392  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8489)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 3534.1147908973016  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3625.668829984805  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (99.9961)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3558.1463498691055  loss: 0.5016 (0.5059)  acc1: 100.0000 (99.8519)  acc5: 100.0000 (99.9963)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3487.5335325451474  loss: 0.5018 (0.5059)  acc1: 100.0000 (99.8480)  acc5: 100.0000 (99.9965)  time: 0.0404  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [18]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3458.0611650735577  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8546)  acc5: 100.0000 (99.9966)  time: 0.0432  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [18]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3391.713334470494  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8574)  acc5: 100.0000 (99.9968)  time: 0.0404  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [18]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3510.267956035909  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8599)  acc5: 100.0000 (99.9969)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3547.191045979214  loss: 0.5015 (0.5057)  acc1: 100.0000 (99.8533)  acc5: 100.0000 (99.9970)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 2666.648017166018  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8587)  acc5: 100.0000 (99.9971)  time: 0.0450  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [18]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3161.4301815462345  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8610)  acc5: 100.0000 (99.9972)  time: 0.0500  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [18]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 2749.0535349449287  loss: 0.5010 (0.5054)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9973)  time: 0.0479  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [18]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 1436.9591023939017  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8676)  acc5: 100.0000 (99.9974)  time: 0.0534  data: 0.0050  max mem: 190</span><br><span class="line">Epoch: [18]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 1795.242673514974  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9975)  time: 0.0713  data: 0.0073  max mem: 190</span><br><span class="line">Epoch: [18]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3321.3372184381633  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8710)  acc5: 100.0000 (99.9976)  time: 0.0763  data: 0.0082  max mem: 190</span><br><span class="line">Epoch: [18]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3390.8564571240895  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9976)  time: 0.0527  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [18]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3459.866289448415  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9977)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [350/468]  eta: 0:00:05  lr: 0.0  img/s: 3461.9411650911156  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8709)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3527.357800817335  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9978)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3433.3809475084417  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3431.603346777544  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (99.9979)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3442.7602057175104  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9980)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3547.4723105081966  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3446.229816734602  loss: 0.5017 (0.5055)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [420/468]  eta: 0:00:02  lr: 0.0  img/s: 3311.5649642240314  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8627)  acc5: 100.0000 (99.9981)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3400.37059650634  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8622)  acc5: 100.0000 (99.9982)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3415.687386275433  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3421.913877061928  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8683)  acc5: 100.0000 (99.9983)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3543.0241867893274  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18] Total time: 0:00:19</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1238  data: 0.1070  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [19]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2553.5975951408145  loss: 0.5011 (0.5011)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1595  data: 0.1094  max mem: 190</span><br><span class="line">Epoch: [19]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3504.425070823379  loss: 0.5017 (0.5064)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0489  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [19]  [ 20/468]  eta: 0:00:24  lr: 0.0  img/s: 3486.65020554751  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0484  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [19]  [ 30/468]  eta: 0:00:21  lr: 0.0  img/s: 3525.944661539573  loss: 0.5015 (0.5067)  acc1: 100.0000 (99.8488)  acc5: 100.0000 (100.0000)  time: 0.0498  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [19]  [ 40/468]  eta: 0:00:19  lr: 0.0  img/s: 3489.2788519657097  loss: 0.5016 (0.5076)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0394  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [19]  [ 50/468]  eta: 0:00:18  lr: 0.0  img/s: 3354.8351361315763  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 60/468]  eta: 0:00:17  lr: 0.0  img/s: 3312.6683696765517  loss: 0.5014 (0.5068)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 70/468]  eta: 0:00:17  lr: 0.0  img/s: 2751.462736134316  loss: 0.5014 (0.5070)  acc1: 100.0000 (99.8349)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 80/468]  eta: 0:00:16  lr: 0.0  img/s: 3579.8076440935642  loss: 0.5017 (0.5069)  acc1: 100.0000 (99.8360)  acc5: 100.0000 (100.0000)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3475.297523336052  loss: 0.5018 (0.5067)  acc1: 100.0000 (99.8455)  acc5: 100.0000 (99.9914)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [100/468]  eta: 0:00:15  lr: 0.0  img/s: 3478.4273468832407  loss: 0.5018 (0.5063)  acc1: 100.0000 (99.8530)  acc5: 100.0000 (99.9923)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3450.704202901345  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8522)  acc5: 100.0000 (99.9930)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 3475.7925158617118  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (99.9935)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3526.477351550184  loss: 0.5016 (0.5058)  acc1: 100.0000 (99.8569)  acc5: 100.0000 (99.9940)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3456.5917150620016  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8615)  acc5: 100.0000 (99.9945)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3405.741748447382  loss: 0.5017 (0.5060)  acc1: 100.0000 (99.8448)  acc5: 100.0000 (99.9948)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3511.1403289624277  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9951)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3486.763427591671  loss: 0.5012 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9954)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3573.5646524754716  loss: 0.5015 (0.5060)  acc1: 100.0000 (99.8446)  acc5: 100.0000 (99.9957)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 2244.987965309314  loss: 0.5017 (0.5059)  acc1: 100.0000 (99.8487)  acc5: 100.0000 (99.9959)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 2122.91724958777  loss: 0.5017 (0.5059)  acc1: 100.0000 (99.8484)  acc5: 100.0000 (99.9961)  time: 0.0410  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [19]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3555.3894120607674  loss: 0.5016 (0.5058)  acc1: 100.0000 (99.8556)  acc5: 100.0000 (99.9963)  time: 0.0397  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [19]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3395.510220603117  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8480)  acc5: 100.0000 (99.9965)  time: 0.0376  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [19]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3594.1870765605336  loss: 0.5019 (0.5059)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [240/468]  eta: 0:00:08  lr: 0.0  img/s: 3571.2113241936236  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8541)  acc5: 100.0000 (99.9968)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3560.0102913677174  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8537)  acc5: 100.0000 (99.9969)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3507.401364099616  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8533)  acc5: 100.0000 (99.9970)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3432.788209341731  loss: 0.5022 (0.5058)  acc1: 100.0000 (99.8530)  acc5: 100.0000 (99.9971)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3539.5203818590576  loss: 0.5018 (0.5058)  acc1: 100.0000 (99.8526)  acc5: 100.0000 (99.9972)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3547.2613579300682  loss: 0.5017 (0.5056)  acc1: 100.0000 (99.8577)  acc5: 100.0000 (99.9973)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 2699.906018667525  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9974)  time: 0.0444  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [19]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3534.952506995885  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8643)  acc5: 100.0000 (99.9975)  time: 0.0490  data: 0.0022  max mem: 190</span><br><span class="line">Epoch: [19]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3387.0485972228357  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9976)  time: 0.0421  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [19]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3504.2649521882445  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9976)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3549.5832170791214  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9977)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3632.6606130320047  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8731)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3434.5230942449907  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8723)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3486.7860728829082  loss: 0.5017 (0.5051)  acc1: 100.0000 (99.8737)  acc5: 100.0000 (99.9979)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3491.0259191343816  loss: 0.5014 (0.5052)  acc1: 100.0000 (99.8729)  acc5: 100.0000 (99.9979)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3496.4597940695685  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (99.9980)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3354.898029070276  loss: 0.5026 (0.5053)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3429.082752101377  loss: 0.5030 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9981)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3449.2853187019346  loss: 0.5028 (0.5055)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (99.9981)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3384.3156428278753  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8604)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3651.8849617718283  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9982)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3311.217747953891  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3503.921890092677  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1225  data: 0.1034  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [20]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2341.7148440226115  loss: 0.5010 (0.5010)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1601  data: 0.1053  max mem: 190</span><br><span class="line">Epoch: [20]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3526.8248448021022  loss: 0.5020 (0.5086)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (99.9290)  time: 0.0480  data: 0.0099  max mem: 190</span><br><span class="line">Epoch: [20]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3593.922414197064  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (99.9628)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 30/468]  eta: 0:00:17  lr: 0.0  img/s: 3534.9059567939844  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8488)  acc5: 100.0000 (99.9748)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3485.608907644863  loss: 0.5011 (0.5047)  acc1: 100.0000 (99.8857)  acc5: 100.0000 (99.9809)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3497.3936653941864  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9847)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 60/468]  eta: 0:00:15  lr: 0.0  img/s: 3581.527098065377  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8591)  acc5: 100.0000 (99.9872)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3528.6329142211152  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8349)  acc5: 100.0000 (99.9890)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 2350.4599690908058  loss: 0.5021 (0.5064)  acc1: 100.0000 (99.8167)  acc5: 100.0000 (99.9904)  time: 0.0438  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [20]  [ 90/468]  eta: 0:00:16  lr: 0.0  img/s: 2334.2924002034842  loss: 0.5032 (0.5071)  acc1: 100.0000 (99.7940)  acc5: 100.0000 (99.9914)  time: 0.0559  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [20]  [100/468]  eta: 0:00:16  lr: 0.0  img/s: 2650.677699823739  loss: 0.5027 (0.5071)  acc1: 100.0000 (99.7989)  acc5: 100.0000 (99.9923)  time: 0.0599  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [20]  [110/468]  eta: 0:00:16  lr: 0.0  img/s: 3581.6704604587244  loss: 0.5015 (0.5067)  acc1: 100.0000 (99.8170)  acc5: 100.0000 (99.9930)  time: 0.0547  data: 0.0049  max mem: 190</span><br><span class="line">Epoch: [20]  [120/468]  eta: 0:00:15  lr: 0.0  img/s: 3442.914752941931  loss: 0.5014 (0.5064)  acc1: 100.0000 (99.8192)  acc5: 100.0000 (99.9935)  time: 0.0442  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [20]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3491.4345767649966  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8330)  acc5: 100.0000 (99.9940)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [140/468]  eta: 0:00:14  lr: 0.0  img/s: 3387.8607929626614  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8338)  acc5: 100.0000 (99.9945)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3553.7184804697067  loss: 0.5013 (0.5059)  acc1: 100.0000 (99.8344)  acc5: 100.0000 (99.9948)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [160/468]  eta: 0:00:13  lr: 0.0  img/s: 3362.335988776993  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9951)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3438.6587415453987  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8492)  acc5: 100.0000 (99.9954)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [180/468]  eta: 0:00:12  lr: 0.0  img/s: 3528.2155029080272  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (99.9957)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 1953.906248180283  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0404  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [200/468]  eta: 0:00:11  lr: 0.0  img/s: 3480.118443228667  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (99.9961)  time: 0.0413  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3652.009169631378  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9963)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 3518.8959152640136  loss: 0.5023 (0.5051)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9965)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3478.3597372137924  loss: 0.5024 (0.5052)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3593.104613263551  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9968)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3502.0705148694396  loss: 0.5011 (0.5050)  acc1: 100.0000 (99.8693)  acc5: 100.0000 (99.9969)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3548.550904536231  loss: 0.5014 (0.5049)  acc1: 100.0000 (99.8713)  acc5: 100.0000 (99.9970)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 2910.8002667519695  loss: 0.5018 (0.5049)  acc1: 100.0000 (99.8732)  acc5: 100.0000 (99.9971)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3321.871535791408  loss: 0.5018 (0.5048)  acc1: 100.0000 (99.8777)  acc5: 100.0000 (99.9972)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3177.1833563147648  loss: 0.5011 (0.5048)  acc1: 100.0000 (99.8792)  acc5: 100.0000 (99.9973)  time: 0.0402  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3421.7175926220993  loss: 0.5013 (0.5047)  acc1: 100.0000 (99.8832)  acc5: 100.0000 (99.9974)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3331.3326797304508  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8819)  acc5: 100.0000 (99.9975)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3345.3444414673204  loss: 0.5013 (0.5048)  acc1: 100.0000 (99.8807)  acc5: 100.0000 (99.9976)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3488.5759808699495  loss: 0.5024 (0.5050)  acc1: 100.0000 (99.8749)  acc5: 100.0000 (99.9976)  time: 0.0384  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3523.722996344161  loss: 0.5023 (0.5049)  acc1: 100.0000 (99.8740)  acc5: 100.0000 (99.9977)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 2873.1184416140427  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8776)  acc5: 100.0000 (99.9978)  time: 0.0456  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [20]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3543.959706645367  loss: 0.5016 (0.5051)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9978)  time: 0.0500  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [20]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3406.02263614678  loss: 0.5031 (0.5052)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0420  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [20]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3502.6417182076775  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9979)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3555.130432479323  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3579.879255046043  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8656)  acc5: 100.0000 (99.9981)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3541.8321150547567  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9981)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3626.5015232266737  loss: 0.5020 (0.5055)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (99.9981)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3511.5077736135368  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8622)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3442.4290798104607  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9982)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3436.281727642797  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3585.8808693677447  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1171  data: 0.1000  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [21]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2728.0719123961485  loss: 0.5023 (0.5023)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1582  data: 0.1113  max mem: 190</span><br><span class="line">Epoch: [21]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3509.7630961331024  loss: 0.5019 (0.5035)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0104  max mem: 190</span><br><span class="line">Epoch: [21]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3548.363275854092  loss: 0.5011 (0.5025)  acc1: 100.0000 (99.9256)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 30/468]  eta: 0:00:17  lr: 0.0  img/s: 3536.9320245075432  loss: 0.5011 (0.5033)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3387.1127037803462  loss: 0.5015 (0.5049)  acc1: 100.0000 (99.8476)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3461.0038164002062  loss: 0.5020 (0.5049)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3559.3258328637253  loss: 0.5020 (0.5063)  acc1: 100.0000 (99.8335)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3587.1507166004076  loss: 0.5018 (0.5066)  acc1: 100.0000 (99.8239)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3504.219206694211  loss: 0.5022 (0.5073)  acc1: 100.0000 (99.7975)  acc5: 100.0000 (99.9904)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3514.7031882160395  loss: 0.5023 (0.5071)  acc1: 100.0000 (99.8111)  acc5: 100.0000 (99.9914)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3495.3898720002085  loss: 0.5015 (0.5065)  acc1: 100.0000 (99.8298)  acc5: 100.0000 (99.9923)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3589.837127974698  loss: 0.5015 (0.5065)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9930)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3519.795658530509  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8386)  acc5: 100.0000 (99.9935)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3226.63977354  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (99.9940)  time: 0.0443  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [21]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3418.7324851309872  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8504)  acc5: 100.0000 (99.9945)  time: 0.0494  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [21]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3301.728209196632  loss: 0.5011 (0.5056)  acc1: 100.0000 (99.8551)  acc5: 100.0000 (99.9948)  time: 0.0424  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [21]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3516.1994432982938  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9951)  time: 0.0375  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [21]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3412.6910930865265  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (99.9954)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3458.840925929505  loss: 0.5017 (0.5052)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (99.9957)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 3420.583945512351  loss: 0.5025 (0.5056)  acc1: 100.0000 (99.8609)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3542.0657913835194  loss: 0.5025 (0.5056)  acc1: 100.0000 (99.8640)  acc5: 100.0000 (99.9961)  time: 0.0401  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [21]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 2264.1699083992644  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8704)  acc5: 100.0000 (99.9963)  time: 0.0423  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [21]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3493.297450646123  loss: 0.5017 (0.5056)  acc1: 100.0000 (99.8657)  acc5: 100.0000 (99.9965)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3334.0221079563803  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [240/468]  eta: 0:00:08  lr: 0.0  img/s: 3511.278111694648  loss: 0.5009 (0.5056)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9968)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3417.8183855360326  loss: 0.5010 (0.5054)  acc1: 100.0000 (99.8724)  acc5: 100.0000 (99.9969)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3480.8630466495933  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (99.9970)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3405.2881046315442  loss: 0.5010 (0.5053)  acc1: 100.0000 (99.8760)  acc5: 100.0000 (99.9971)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3434.2814229147875  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (99.9972)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3563.2236808920156  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9973)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3592.4794871623294  loss: 0.5019 (0.5055)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9974)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3356.597030229141  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9975)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3536.978628086542  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8686)  acc5: 100.0000 (99.9976)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3451.014739440377  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3516.4527817441085  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9977)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3412.734480084417  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (99.9978)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3465.7851342104245  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9978)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3468.4272165800967  loss: 0.5020 (0.5056)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9979)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3363.4102780962403  loss: 0.5013 (0.5055)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9979)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 2199.209044732099  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8581)  acc5: 100.0000 (99.9980)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 2553.67047351773  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8617)  acc5: 100.0000 (99.9981)  time: 0.0502  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [21]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 1813.054809971835  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8612)  acc5: 100.0000 (99.9981)  time: 0.0729  data: 0.0018  max mem: 190</span><br><span class="line">Epoch: [21]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3504.1505906925136  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0660  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [21]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3498.3052402486546  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9982)  time: 0.0414  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [21]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3455.47932650224  loss: 0.5026 (0.5055)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3469.52553654864  loss: 0.5017 (0.5055)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3408.40123417601  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (99.9983)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1211  data: 0.1003  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [22]  [  0/468]  eta: 0:01:15  lr: 0.0  img/s: 2650.978737692452  loss: 0.5289 (0.5289)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1605  data: 0.1121  max mem: 190</span><br><span class="line">Epoch: [22]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3643.780071807193  loss: 0.5014 (0.5080)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0478  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [22]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3613.248478974856  loss: 0.5017 (0.5065)  acc1: 100.0000 (99.8140)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3002.331487881533  loss: 0.5025 (0.5070)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3119.275085118003  loss: 0.5025 (0.5063)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3240.760777969601  loss: 0.5020 (0.5059)  acc1: 100.0000 (99.8315)  acc5: 100.0000 (100.0000)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3273.243863478399  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8591)  acc5: 100.0000 (100.0000)  time: 0.0399  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 70/468]  eta: 0:00:16  lr: 0.0  img/s: 3546.089855876564  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3495.6857423769866  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8553)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3509.327197615437  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3441.8111485078693  loss: 0.5012 (0.5050)  acc1: 100.0000 (99.8685)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3550.592648439877  loss: 0.5015 (0.5047)  acc1: 100.0000 (99.8733)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3530.7678931965406  loss: 0.5019 (0.5047)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3508.9831436806776  loss: 0.5032 (0.5050)  acc1: 100.0000 (99.8628)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3529.468033212588  loss: 0.5011 (0.5050)  acc1: 100.0000 (99.8615)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 2792.9587614385377  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8603)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 2942.4194586181006  loss: 0.5013 (0.5049)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (100.0000)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 2722.703843637636  loss: 0.5013 (0.5051)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (100.0000)  time: 0.0481  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3555.0362674400894  loss: 0.5030 (0.5052)  acc1: 100.0000 (99.8576)  acc5: 100.0000 (100.0000)  time: 0.0497  data: 0.0020  max mem: 190</span><br><span class="line">Epoch: [22]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3453.900964365442  loss: 0.5035 (0.5053)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (100.0000)  time: 0.0437  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [22]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3532.4173070849565  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8601)  acc5: 100.0000 (100.0000)  time: 0.0420  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3577.0647157981703  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0391  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3581.6704604587244  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (100.0000)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3663.3725597232365  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0360  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3384.7210369697887  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (100.0000)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3542.907281534177  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8599)  acc5: 100.0000 (99.9969)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3555.4835958092162  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8623)  acc5: 100.0000 (99.9970)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3492.0704566150644  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9971)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3521.4348345118  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9972)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3505.0428083645074  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9973)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3506.233138931158  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8676)  acc5: 100.0000 (99.9974)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3525.0417722682564  loss: 0.5024 (0.5054)  acc1: 100.0000 (99.8618)  acc5: 100.0000 (99.9975)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3452.812512862729  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9976)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3561.8052942347244  loss: 0.5023 (0.5053)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3574.278394716519  loss: 0.5026 (0.5055)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (99.9977)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3384.678359328702  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9978)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3586.3839088291684  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3521.6196261069204  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3221.6056214633327  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (99.9979)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3571.116305367275  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3317.9503609215863  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (99.9981)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3300.0842861006613  loss: 0.5026 (0.5054)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (99.9981)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3266.949700001217  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8664)  acc5: 100.0000 (99.9981)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3328.420585372507  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9982)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3564.5721949632502  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (99.9982)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3527.3114504218024  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8718)  acc5: 100.0000 (99.9983)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 2299.469375867327  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8712)  acc5: 100.0000 (99.9983)  time: 0.0474  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [22] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1234  data: 0.1042  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [23]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2733.669966190069  loss: 0.5012 (0.5012)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1601  data: 0.1132  max mem: 190</span><br><span class="line">Epoch: [23]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3446.274060712658  loss: 0.5017 (0.5046)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0492  data: 0.0106  max mem: 190</span><br><span class="line">Epoch: [23]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3491.5708172370287  loss: 0.5015 (0.5045)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3525.875192098039  loss: 0.5010 (0.5035)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3545.949327626747  loss: 0.5010 (0.5039)  acc1: 100.0000 (99.9047)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3406.779102602339  loss: 0.5022 (0.5054)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3442.517373824165  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8719)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3544.2638569806436  loss: 0.5016 (0.5044)  acc1: 100.0000 (99.8900)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3308.952418519797  loss: 0.5017 (0.5041)  acc1: 100.0000 (99.9035)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3444.7925056143727  loss: 0.5013 (0.5038)  acc1: 100.0000 (99.9141)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3346.699946390056  loss: 0.5014 (0.5042)  acc1: 100.0000 (99.8994)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3565.8269925611053  loss: 0.5017 (0.5043)  acc1: 100.0000 (99.9015)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3452.7014849543066  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8902)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3512.19693966335  loss: 0.5024 (0.5048)  acc1: 100.0000 (99.8867)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3590.485347029948  loss: 0.5020 (0.5048)  acc1: 100.0000 (99.8892)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3541.8087491176334  loss: 0.5026 (0.5050)  acc1: 100.0000 (99.8655)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [160/468]  eta: 0:00:11  lr: 0.0  img/s: 3459.130640962862  loss: 0.5020 (0.5050)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3506.5079454237884  loss: 0.5016 (0.5050)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 2821.315311811909  loss: 0.5016 (0.5049)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 2042.9344353371844  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8609)  acc5: 100.0000 (100.0000)  time: 0.0486  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [23]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 2757.029867302083  loss: 0.5019 (0.5053)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (100.0000)  time: 0.0597  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [23]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3406.9088163063275  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0557  data: 0.0046  max mem: 190</span><br><span class="line">Epoch: [23]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 2642.445376331384  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8586)  acc5: 100.0000 (100.0000)  time: 0.0512  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [23]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3511.8523228279496  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0497  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [23]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3404.683434166635  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (100.0000)  time: 0.0423  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [23]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3574.5639714498775  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8537)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3545.7151386265473  loss: 0.5029 (0.5056)  acc1: 100.0000 (99.8473)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3458.840925929505  loss: 0.5019 (0.5058)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3398.4548947618296  loss: 0.5020 (0.5057)  acc1: 100.0000 (99.8471)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3445.367286169011  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8523)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3603.692572057619  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8521)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3618.4356242122785  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8543)  acc5: 100.0000 (99.9975)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3546.9098260473165  loss: 0.5012 (0.5055)  acc1: 100.0000 (99.8588)  acc5: 100.0000 (99.9976)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3492.774736677748  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8607)  acc5: 100.0000 (99.9976)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3468.15834625323  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8602)  acc5: 100.0000 (99.9977)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3601.323566502992  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8642)  acc5: 100.0000 (99.9978)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 2893.6212487064504  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8680)  acc5: 100.0000 (99.9978)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3375.824741879095  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9979)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3237.5166558120463  loss: 0.5017 (0.5052)  acc1: 100.0000 (99.8708)  acc5: 100.0000 (99.9979)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3385.7456233287926  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9980)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3163.553884682243  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8597)  acc5: 100.0000 (99.9981)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3531.023335350293  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9981)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3491.4345767649966  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3502.344669219579  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8677)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3352.5516242241065  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3509.4418972538715  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3522.7056685235857  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1268  data: 0.1103  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [24]  [  0/468]  eta: 0:01:47  lr: 0.0  img/s: 1711.2187061733432  loss: 0.5025 (0.5025)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.2291  data: 0.1543  max mem: 190</span><br><span class="line">Epoch: [24]  [ 10/468]  eta: 0:00:27  lr: 0.0  img/s: 3611.1340611148107  loss: 0.5018 (0.5043)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0607  data: 0.0158  max mem: 190</span><br><span class="line">Epoch: [24]  [ 20/468]  eta: 0:00:22  lr: 0.0  img/s: 3511.41590524092  loss: 0.5014 (0.5039)  acc1: 100.0000 (99.9256)  acc5: 100.0000 (100.0000)  time: 0.0407  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [24]  [ 30/468]  eta: 0:00:19  lr: 0.0  img/s: 3481.3144765424895  loss: 0.5014 (0.5041)  acc1: 100.0000 (99.9244)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 40/468]  eta: 0:00:18  lr: 0.0  img/s: 3515.209470431093  loss: 0.5016 (0.5042)  acc1: 100.0000 (99.9238)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3570.3563367449406  loss: 0.5016 (0.5043)  acc1: 100.0000 (99.9234)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3415.687386275433  loss: 0.5020 (0.5050)  acc1: 100.0000 (99.9103)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 70/468]  eta: 0:00:16  lr: 0.0  img/s: 3545.2234424010303  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.9010)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3526.755340673201  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8939)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3453.1012188454733  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3487.6014993146546  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8840)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 2461.0849347220183  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8874)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 2650.651525848832  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.8838)  acc5: 100.0000 (100.0000)  time: 0.0487  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [24]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3563.412884469873  loss: 0.5020 (0.5059)  acc1: 100.0000 (99.8628)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [24]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3567.5093329080532  loss: 0.5022 (0.5060)  acc1: 100.0000 (99.8559)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [24]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3569.383099527957  loss: 0.5025 (0.5060)  acc1: 100.0000 (99.8500)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3519.657206542761  loss: 0.5023 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3487.624155493192  loss: 0.5023 (0.5062)  acc1: 100.0000 (99.8355)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3531.464640684098  loss: 0.5028 (0.5061)  acc1: 100.0000 (99.8403)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3589.5011065275094  loss: 0.5028 (0.5063)  acc1: 100.0000 (99.8282)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3495.2533333333336  loss: 0.5031 (0.5062)  acc1: 100.0000 (99.8290)  acc5: 100.0000 (100.0000)  time: 0.0397  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3428.9075441330506  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8334)  acc5: 100.0000 (100.0000)  time: 0.0424  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3414.38400386675  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3399.0143147471654  loss: 0.5015 (0.5060)  acc1: 100.0000 (99.8377)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3501.956296557212  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8412)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3344.2399960133553  loss: 0.5023 (0.5060)  acc1: 100.0000 (99.8413)  acc5: 100.0000 (100.0000)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3525.6899536362084  loss: 0.5022 (0.5059)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3487.352300776886  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8414)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 2152.0459854892374  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0529  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [24]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3302.459367522314  loss: 0.5020 (0.5060)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0637  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [24]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3530.41962254225  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8495)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0020  max mem: 190</span><br><span class="line">Epoch: [24]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3457.0146105255026  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (99.9975)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3494.411580543749  loss: 0.5013 (0.5058)  acc1: 100.0000 (99.8515)  acc5: 100.0000 (99.9976)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3519.4956929894715  loss: 0.5011 (0.5058)  acc1: 100.0000 (99.8513)  acc5: 100.0000 (99.9976)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3488.8480264098466  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8534)  acc5: 100.0000 (99.9977)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3487.6014993146546  loss: 0.5016 (0.5057)  acc1: 100.0000 (99.8531)  acc5: 100.0000 (99.9978)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3494.88928236642  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8572)  acc5: 100.0000 (99.9978)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3483.6185915529513  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8610)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3589.2371338030994  loss: 0.5013 (0.5055)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3577.636805874866  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3608.3186837559733  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3593.03247222594  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (99.9981)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3527.8909179321718  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8701)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3469.6824961869556  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9982)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3280.2235731873475  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3400.090640219381  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8683)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3525.0186273415493  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1272  data: 0.1089  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [25]  [  0/468]  eta: 0:01:19  lr: 0.0  img/s: 2520.7810759796785  loss: 0.5107 (0.5107)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1689  data: 0.1181  max mem: 190</span><br><span class="line">Epoch: [25]  [ 10/468]  eta: 0:00:23  lr: 0.0  img/s: 3186.9908166475716  loss: 0.5024 (0.5065)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0521  data: 0.0111  max mem: 190</span><br><span class="line">Epoch: [25]  [ 20/468]  eta: 0:00:20  lr: 0.0  img/s: 3260.640089400675  loss: 0.5013 (0.5041)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 30/468]  eta: 0:00:20  lr: 0.0  img/s: 2113.99792093243  loss: 0.5013 (0.5043)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0456  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [25]  [ 40/468]  eta: 0:00:20  lr: 0.0  img/s: 3353.9967888847937  loss: 0.5018 (0.5048)  acc1: 100.0000 (99.8476)  acc5: 100.0000 (100.0000)  time: 0.0491  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [25]  [ 50/468]  eta: 0:00:19  lr: 0.0  img/s: 3436.0398087643284  loss: 0.5022 (0.5044)  acc1: 100.0000 (99.8775)  acc5: 100.0000 (100.0000)  time: 0.0437  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [25]  [ 60/468]  eta: 0:00:18  lr: 0.0  img/s: 3491.0713208136085  loss: 0.5016 (0.5045)  acc1: 100.0000 (99.8847)  acc5: 100.0000 (100.0000)  time: 0.0389  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [25]  [ 70/468]  eta: 0:00:17  lr: 0.0  img/s: 3572.019188417754  loss: 0.5019 (0.5045)  acc1: 100.0000 (99.8900)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 80/468]  eta: 0:00:16  lr: 0.0  img/s: 3435.6660011262993  loss: 0.5024 (0.5046)  acc1: 100.0000 (99.8843)  acc5: 100.0000 (100.0000)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 90/468]  eta: 0:00:16  lr: 0.0  img/s: 3535.301672593178  loss: 0.5018 (0.5043)  acc1: 100.0000 (99.8970)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [100/468]  eta: 0:00:15  lr: 0.0  img/s: 3394.9304852060527  loss: 0.5015 (0.5049)  acc1: 100.0000 (99.8840)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3215.5082982457193  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8733)  acc5: 100.0000 (100.0000)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 3210.969634985855  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8709)  acc5: 100.0000 (100.0000)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3372.6860574688094  loss: 0.5017 (0.5050)  acc1: 100.0000 (99.8807)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3508.8226082637284  loss: 0.5019 (0.5051)  acc1: 100.0000 (99.8726)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3403.971062459184  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3460.245382007554  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (99.9951)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3412.3874149876056  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8766)  acc5: 100.0000 (99.9954)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3046.127945440206  loss: 0.5021 (0.5051)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (99.9957)  time: 0.0423  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [25]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3169.0065815492317  loss: 0.5019 (0.5049)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (99.9959)  time: 0.0466  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [25]  [200/468]  eta: 0:00:11  lr: 0.0  img/s: 3170.6160978461794  loss: 0.5016 (0.5048)  acc1: 100.0000 (99.8795)  acc5: 100.0000 (99.9961)  time: 0.0438  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [25]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3243.109978132438  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8741)  acc5: 100.0000 (99.9963)  time: 0.0413  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [25]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 3291.707513274228  loss: 0.5015 (0.5048)  acc1: 100.0000 (99.8763)  acc5: 100.0000 (99.9965)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3252.205986224777  loss: 0.5015 (0.5048)  acc1: 100.0000 (99.8782)  acc5: 100.0000 (99.9966)  time: 0.0402  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3223.036818692105  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8833)  acc5: 100.0000 (99.9968)  time: 0.0404  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3207.804019980402  loss: 0.5014 (0.5049)  acc1: 100.0000 (99.8786)  acc5: 100.0000 (99.9969)  time: 0.0403  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3453.0123810932664  loss: 0.5013 (0.5049)  acc1: 100.0000 (99.8803)  acc5: 100.0000 (99.9970)  time: 0.0397  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3455.723990550796  loss: 0.5015 (0.5050)  acc1: 100.0000 (99.8789)  acc5: 100.0000 (99.9971)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3354.436868939318  loss: 0.5015 (0.5050)  acc1: 100.0000 (99.8804)  acc5: 100.0000 (99.9972)  time: 0.0377  data: 0.0003  max mem: 190</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/blog/" title="å¤´åƒ"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" title="å¤´åƒ" alt="å¤´åƒ"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" title="å¤´åƒ" alt="å¤´åƒ"></a><div class="post-copyright__author_name">xiuqhou</div><div class="post-copyright__author_desc">å†²å†²å†²ï¼</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="è¯¥æ–‡ç« ä¸ºåŸåˆ›æ–‡ç« ï¼Œæ³¨æ„ç‰ˆæƒåè®®" href="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/">åŸåˆ›</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/')">swin transformeråˆ†ç±»MNIST</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="ä½¿ç”¨æ‰‹æœºè®¿é—®è¿™ç¯‡æ–‡ç« "><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"></div><div class="reward-dec">ä½¿ç”¨æ‰‹æœºè®¿é—®è¿™ç¯‡æ–‡ç« </div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=swin transformeråˆ†ç±»MNIST&amp;url=http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/&amp;pic=https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformerç»“æ„.webp" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="å¤åˆ¶é“¾æ¥" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="http://xiuqhou.github.io/blog" target="_blank">xiuqhouçš„ä¸ªäººåšå®¢</a>ï¼</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/blog/tags/Classification/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Classification<span class="tagsPageCount">2</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/arxiv-logo.svg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/blog/2022/08/28/20220828%20%E5%86%B3%E7%AD%96%E6%A0%91/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/å†³ç­–æ ‘logo.webp" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">å†³ç­–æ ‘</div></div></a></div><div class="next-post pull-right"><a href="/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>å–œæ¬¢è¿™ç¯‡æ–‡ç« çš„äººä¹Ÿçœ‹äº†</span></div><div class="relatedPosts-list"><div><a href="/blog/2023/01/01/20230101%20T%20COGN%20DEV%20SYST2021.%20Bioinspired%20Visual-Integrated%20Model%20for%20Multilabel%20Classification%20of%20Textile%20Defect%20Images/" title="T COGN DEV SYST2021. Bioinspired Visual-Integrated Model for Multilabel Classification of Textile Defect Images"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/IEEElogo.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-01-01</div><div class="title">T COGN DEV SYST2021. Bioinspired Visual-Integrated Model for Multilabel Classification of Textile Defect Images</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description">Trying to be better!</div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/blog/"><h1 class="author-info__name">xiuqhou</h1><div class="author-info__desc">å†²å†²å†²ï¼</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/xiuqhou" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥çœ‹æˆ‘çš„åšå®¢é¸­~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>æ–‡ç« ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="toc-number">1.</span> <span class="toc-text">å®‰è£…ä¾èµ–</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%AE%98%E6%96%B9%E8%AE%AD%E7%BB%83%E5%8F%82%E8%80%83%E4%B8%AD%E7%BB%99%E5%87%BA%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">pytorchå®˜æ–¹è®­ç»ƒå‚è€ƒä¸­ç»™å‡ºçš„ç›¸å…³ä»£ç </span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>æœ€è¿‘å‘å¸ƒ</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/08/02/20240802%20%E3%80%90Arxiv2023%E3%80%91Detect%20Everything%20with%20Few%20Examples/" title="ã€Arxiv2023ã€‘Detect Everything with Few Examples"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/arxiv-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="ã€Arxiv2023ã€‘Detect Everything with Few Examples"/></a><div class="content"><a class="title" href="/blog/2024/08/02/20240802%20%E3%80%90Arxiv2023%E3%80%91Detect%20Everything%20with%20Few%20Examples/" title="ã€Arxiv2023ã€‘Detect Everything with Few Examples">ã€Arxiv2023ã€‘Detect Everything with Few Examples</a><time datetime="2024-08-02T20:50:00.000Z" title="å‘è¡¨äº 2024-08-02 20:50:00">2024-08-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/24/20240724%20%E3%80%90CVPR2024%E3%80%91Few-Shot%20Object%20Detection%20with%20Foundation%20Models/" title="ã€CVPR2024ã€‘Few-Shot Object Detection with Foundation Models"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/cvpr-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="ã€CVPR2024ã€‘Few-Shot Object Detection with Foundation Models"/></a><div class="content"><a class="title" href="/blog/2024/07/24/20240724%20%E3%80%90CVPR2024%E3%80%91Few-Shot%20Object%20Detection%20with%20Foundation%20Models/" title="ã€CVPR2024ã€‘Few-Shot Object Detection with Foundation Models">ã€CVPR2024ã€‘Few-Shot Object Detection with Foundation Models</a><time datetime="2024-07-24T20:50:00.000Z" title="å‘è¡¨äº 2024-07-24 20:50:00">2024-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/20/20240720%20%E3%80%90CVPR2023%E3%80%91Salience%20DETR%20Enhancing%20Detection%20Transformer%20with%20Hierarchical%20Salience%20Filtering%20Refinement/" title="ã€CVPR2024ã€‘Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/cvpr-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="ã€CVPR2024ã€‘Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement"/></a><div class="content"><a class="title" href="/blog/2024/07/20/20240720%20%E3%80%90CVPR2023%E3%80%91Salience%20DETR%20Enhancing%20Detection%20Transformer%20with%20Hierarchical%20Salience%20Filtering%20Refinement/" title="ã€CVPR2024ã€‘Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement">ã€CVPR2024ã€‘Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement</a><time datetime="2024-07-20T20:26:00.000Z" title="å‘è¡¨äº 2024-07-20 20:26:00">2024-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/19/20240719%20%E3%80%90NIPS2023%E3%80%91Rank-DETR%20for%20High%20Quality%20Object%20Detection/" title="ã€NIPS2023ã€‘Rank-DETR for High Quality Object Detection"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://nips.cc/static/core/img/neurips-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="ã€NIPS2023ã€‘Rank-DETR for High Quality Object Detection"/></a><div class="content"><a class="title" href="/blog/2024/07/19/20240719%20%E3%80%90NIPS2023%E3%80%91Rank-DETR%20for%20High%20Quality%20Object%20Detection/" title="ã€NIPS2023ã€‘Rank-DETR for High Quality Object Detection">ã€NIPS2023ã€‘Rank-DETR for High Quality Object Detection</a><time datetime="2024-07-19T11:13:00.000Z" title="å‘è¡¨äº 2024-07-19 11:13:00">2024-07-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2023/03/22/20230322%20Linux%E5%91%BD%E4%BB%A4/" title="Linuxå‘½ä»¤"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/Linux_logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="Linuxå‘½ä»¤"/></a><div class="content"><a class="title" href="/blog/2023/03/22/20230322%20Linux%E5%91%BD%E4%BB%A4/" title="Linuxå‘½ä»¤">Linuxå‘½ä»¤</a><time datetime="2023-03-22T09:27:00.000Z" title="å‘è¡¨äº 2023-03-22 09:27:00">2023-03-22</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2024 By <a class="footer-bar-link" href="/blog/" title="xiuqhou" target="_blank">xiuqhou</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="ä¸»é¢˜">ä¸»é¢˜</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/blog/archives/" title="archive"><div class="headline">æ–‡ç« </div><div class="length-num">32</div></a><a href="/blog/tags/" title="tag"><div class="headline">æ ‡ç­¾</div><div class="length-num">24</div></a><a href="/blog/categories/" title="category"><div class="headline">åˆ†ç±»</div><div class="length-num">5</div></a></div><span class="sidebar-menu-item-title">åŠŸèƒ½</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="æ˜¾ç¤ºæ¨¡å¼"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>æ˜¾ç¤ºæ¨¡å¼</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">ç½‘é¡µ</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="åšå®¢"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="åšå®¢"/><span class="back-menu-item-text">åšå®¢</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">é¡¹ç›®</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="å®‰çŸ¥é±¼å›¾åºŠ"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="å®‰çŸ¥é±¼å›¾åºŠ"/><span class="back-menu-item-text">å®‰çŸ¥é±¼å›¾åºŠ</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> æ–‡ç« </span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> éš§é“</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> åˆ†ç±»</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> æ ‡ç­¾</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> æˆ‘çš„</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> éŸ³ä¹é¦†</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> è¿½ç•ªé¡µ</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> ç›¸å†Œé›†</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> å°ç©ºè°ƒ</span></a></li></ul></div></div><span class="sidebar-menu-item-title">æ ‡ç­¾</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 0.88rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 0.88rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 0.88rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 0.88rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 0.88rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 0.88rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 0.88rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 0.88rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 0.88rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 0.88rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 0.88rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 0.88rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 0.88rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 0.88rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 0.88rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 0.88rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 0.88rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 0.88rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">ä¼˜åŒ–ç®—æ³•<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 0.88rem;">åšå®¢<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 0.88rem;">æœåŠ¡å™¨<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 0.88rem;">ç”»å›¾<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç¹</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">æ’­æ”¾éŸ³ä¹</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>å¤åˆ¶é€‰ä¸­æ–‡æœ¬</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>ç²˜è´´æ–‡æœ¬</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>å¼•ç”¨åˆ°è¯„è®º</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>æ–°çª—å£æ‰“å¼€</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>å¤åˆ¶é“¾æ¥åœ°å€</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>å¤åˆ¶æ­¤å›¾ç‰‡</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>ä¸‹è½½æ­¤å›¾ç‰‡</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>æ–°çª—å£æ‰“å¼€å›¾ç‰‡</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>ç«™å†…æœç´¢</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>ç™¾åº¦æœç´¢</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>æ’­æ”¾éŸ³ä¹</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>åˆ‡æ¢åˆ°ä¸Šä¸€é¦–</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>åˆ‡æ¢åˆ°ä¸‹ä¸€é¦–</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>æŸ¥çœ‹æ‰€æœ‰æ­Œæ›²</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>å¤åˆ¶æ­Œå</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>éšä¾¿é€›é€›</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>åšå®¢åˆ†ç±»</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>æ–‡ç« æ ‡ç­¾</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>å¤åˆ¶åœ°å€</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">å…³é—­çƒ­è¯„</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">æ·±è‰²æ¨¡å¼</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>è½‰ç‚ºç¹é«”</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><script src="/blog/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// æ¶ˆé™¤æ§åˆ¶å°æ‰“å°
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //åœ¨æ¢å¤å‰è¾“å‡ºæ—¥å¿—
  const grt = new Date("04/01/2021 00:00:00"); //æ­¤å¤„ä¿®æ”¹ä½ çš„å»ºç«™æ—¶é—´æˆ–è€…ç½‘ç«™ä¸Šçº¿æ—¶é—´
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `æ¬¢è¿ä½¿ç”¨å®‰çŸ¥é±¼!`,
    `ç”Ÿæ´»æ˜æœ—, ä¸‡ç‰©å¯çˆ±`,
    `
        
       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
      â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
      â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•   â•šâ•â•    â•šâ•â•â•â•â•â•
        
        `,
    "å·²ä¸Šçº¿",
    dnum,
    "å¤©",
    "Â©2020 By å®‰çŸ¥é±¼ V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `è°ƒç”¨å‰ç½®æ‘„åƒå¤´æ‹ç…§æˆåŠŸï¼Œè¯†åˆ«ä¸ºã€å°ç¬¨è›‹ã€‘.`, `Photo captured: `, `ğŸ¤ª`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c ä½ å¥½ï¼Œå°ç¬¨è›‹.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c âš¡ Powered by å®‰çŸ¥é±¼ %c ä½ æ­£åœ¨è®¿é—® xiuqhou çš„åšå®¢.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c ä½ å·²æ‰“å¼€æ§åˆ¶å°.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c ä½ ç°åœ¨æ­£å¤„äºç›‘æ§ä¸­.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/blog/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    anzhiyu.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/blog/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/blog/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">é€šçŸ¥</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">ä½ å¥½å‘€</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>