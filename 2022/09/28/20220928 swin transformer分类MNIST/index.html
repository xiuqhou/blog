<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>swin transformer分类MNIST | xiuqhou的个人博客</title><meta name="keywords" content="Classification"><meta name="author" content="xiuqhou"><meta name="copyright" content="xiuqhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="swin transformer分类MNIST"><meta name="application-name" content="swin transformer分类MNIST"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="swin transformer分类MNIST"><meta property="og:url" content="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/index.html"><meta property="og:site_name" content="xiuqhou的个人博客"><meta property="og:description" content="安装依赖   timm库中提供了swin transformer使用的DropPath层等结构   torch库是构建神经网络和实现自动反向传播的基础库   sys库提供了一些系统信息和操作的接口   logging库提供了日志记录的功能   1!pip install timm 123456789"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformer结构.webp"><meta property="article:author" content="xiuqhou"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformer结构.webp"><meta name="description" content="安装依赖   timm库中提供了swin transformer使用的DropPath层等结构   torch库是构建神经网络和实现自动反向传播的基础库   sys库提供了一些系统信息和操作的接口   logging库提供了日志记录的功能   1!pip install timm 123456789"><link rel="shortcut icon" href="/blog/favicon.ico"><link rel="canonical" href="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/blog/',
  preloader: {"source":2},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: undefined,
  algolia: undefined,
  localSearch: {"path":"/blog/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: xiuqhou","link":"链接: ","source":"来源: xiuqhou的个人博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'xiuqhou的个人博客',
  title: 'swin transformer分类MNIST',
  postAI: '',
  pageFillDescription: '安装依赖, pytorch官方训练参考中给出的相关代码安装依赖库中提供了使用的层等结构库是构建神经网络和实现自动反向传播的基础库库提供了一些系统信息和操作的接口库提供了日志记录的功能创建日志记录的功能可将输出按照一定格式重定向到文件中在大型网络调试的时候对于掌握网络信息很有用如果名字有层级结构则也会向对应的父级结构传递日志设置记录的日志等级有禁止层级传递格式化输出主进程的控制句柄为向标准输出输出信息文件句柄向文件进行输出日志信息基本结构包括一个用于将图像打成的层对后图像数目进行变换的层以及个用于层级变换数据流向为和层在代码中合并为层用于将图片打成和变换数目输入图像从变换至再变换至代码实现是通过步长为的卷积操作一次性实现两个操作此时形状记为注意该形状为代码实现中的写法和图中表示有所不同层包括多个和一个层网络一共包含个每层具有的数目为每个层中仅第一个不需要实现操作中首先通过操作将图片划分为个形状的小窗口并将其组织成的形状记为其中为划分的小窗口数量然后执行多头注意力操作中先对矩阵通过全连接层生成形状均为的的三个矩阵然后按照多头注意力的数目将其形状变为将其视为个维度为的矩阵通过计算得到形状为的注意力矩阵然后将通过和矩阵相乘得到形状为的矩阵最后将其组织为形状的矩阵进行输出因此输入和输出的维度相同可以任意堆叠多层层之前首先将多层堆叠输出得到的矩阵先按照小窗口的划分方式将小窗口合并为用输入图片的形状表示即为然后将其变换为层最初输入的图片形状层中通过间隔采样得到个形状为的矩阵拼接得到的再送到全连接降维成的矩阵该层输入为输出为从而实现下采样的目的由于最后一层没有下采样层因此经过层得到的输出为经过转化为的形式然后送入全连接进行分类将个窗口合并为一个避免报错信息注册为不可变参数在保存模型时该参数也会被保存截断到之内的正态分布注意力机制用于屏蔽不应的注意力矩阵部分对注意力图进行操作计算复杂度并不是每秒的浮点运算数是小块拉直的数目长度是每个序列的长度计算复杂度为先拆分为多头注意力计算复杂度为计算复杂度为计算复杂度为在小范围内做自注意力一种正则化手段再维度随机设置一定样本不进行主干而直接由分支进行恒等映射给每块区域划分标记序号每一个维度分为三部分生成掩码窗口获得掩码矩阵具体看论文中所说标准化层为窗口数目窗口数目每个窗口的长度通道数目多个窗口可以并行地做注意力将所有小窗口合并为一个大窗口对整个窗口进行左上的平移操作下采样每行每列间隔采样通道数扩增到倍然后再将回到倍切割成个块输入通道数目为嵌入维度为通道数变化层归一化常用于因此可能来源于自带的每一个层的深度因为可以随意堆叠绝对位置编码归一化经过所有层后的通道数量隐藏层维度与嵌入维度的商对图片进行切割的块层数一共为其中每一层的概率从增加到每经过一层图片宽和高减半加倍初始的图片为窗口尺寸是官方训练参考中给出的相关代码图片预处理相关分布式数据采样数据增强措施和数值平滑减轻训练过程中损失函数数值的抖动训练和评估函数本次分类没有用官方给定的主函数训练和评估当不足一个时候使丢弃后面一部分还是随机增加一部分样本库里面自带的的和的此处没用到',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-02 13:27:02',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/blog/" accesskey="h"><div class="title">xiuqhou的个人博客</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 1.05rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 1.05rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 1.05rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 1.05rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 1.05rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 1.05rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 1.05rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 1.05rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 1.05rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 1.05rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 1.05rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 1.05rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 1.05rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 1.05rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 1.05rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 1.05rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 1.05rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 1.05rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">优化算法<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">博客<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">服务器<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 1.05rem;">画图<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/blog/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/03/"><span class="card-archive-list-date">三月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/02/"><span class="card-archive-list-date">二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/01/"><span class="card-archive-list-date">一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/12/"><span class="card-archive-list-date">十二月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/11/"><span class="card-archive-list-date">十一月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/blog/categories/%E4%BB%A3%E7%A0%81/" itemprop="url">代码</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Classification/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Classification</span></a></span></div></div><h1 class="post-title" itemprop="name headline">swin transformer分类MNIST</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-09-28T10:51:50.000Z" title="发表于 2022-09-28 10:51:50">2022-09-28</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02">2024-08-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformer结构.webp"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"><header><a class="post-meta-categories" href="/blog/categories/%E4%BB%A3%E7%A0%81/" itemprop="url">代码</a><a href="/blog/tags/Classification/" tabindex="-1" itemprop="url">Classification</a><h1 id="CrawlerTitle" itemprop="name headline">swin transformer分类MNIST</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">xiuqhou</span><time itemprop="dateCreated datePublished" datetime="2022-09-28T10:51:50.000Z" title="发表于 2022-09-28 10:51:50">2022-09-28</time><time itemprop="dateCreated datePublished" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02">2024-08-02</time></header><h2 id="安装依赖">安装依赖</h2>
<ul>
<li>
<p>timm库中提供了swin transformer使用的DropPath层等结构</p>
</li>
<li>
<p>torch库是构建神经网络和实现自动反向传播的基础库</p>
</li>
<li>
<p>sys库提供了一些系统信息和操作的接口</p>
</li>
<li>
<p>logging库提供了日志记录的功能</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install timm</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Collecting timm</span><br><span class="line">  Downloading timm-0.6.7-py3-none-any.whl (509 kB)</span><br><span class="line">[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m510.0/510.0 kB[0m [31m799.5 kB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m</span><br><span class="line">[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)</span><br><span class="line">Requirement already satisfied: torch&gt;=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)</span><br><span class="line">Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.4-&gt;timm) (4.3.0)</span><br><span class="line">Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (2.28.1)</span><br><span class="line">Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (9.1.1)</span><br><span class="line">Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (1.21.6)</span><br><span class="line">Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2.1.0)</span><br><span class="line">Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (1.26.12)</span><br><span class="line">Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2022.6.15.2)</span><br><span class="line">Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (3.3)</span><br><span class="line">Installing collected packages: timm</span><br><span class="line">Successfully installed timm-0.6.7</span><br><span class="line">[33mWARNING: Running pip as the &#x27;root&#x27; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m</span><br><span class="line">[0m</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">from</span> timm.data <span class="keyword">import</span> Mixup</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint <span class="keyword">as</span> checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> timm.loss <span class="keyword">import</span> LabelSmoothingCrossEntropy, SoftTargetCrossEntropy</span><br><span class="line"><span class="keyword">from</span> timm.utils <span class="keyword">import</span> accuracy, AverageMeter</span><br><span class="line"><span class="keyword">from</span> timm.models.layers <span class="keyword">import</span> DropPath, to_2tuple, trunc_normal_</span><br><span class="line"><span class="keyword">from</span> timm.optim.nadam <span class="keyword">import</span> Nadam</span><br><span class="line"><span class="keyword">from</span> timm.scheduler.cosine_lr <span class="keyword">import</span> CosineLRScheduler</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br></pre></td></tr></table></figure>
<p>创建日志记录的功能，可将输出按照一定格式重定向到文件中，在大型网络调试的时候对于掌握网络信息很有用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@functools.lru_cache()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_logger</span>(<span class="params">output_dir, dist_rank=<span class="number">0</span>, name=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># create logger，如果名字有层级结构，则logger也会向对应的父级结构传递日志</span></span><br><span class="line">    logger = logging.getLogger(name)</span><br><span class="line">    <span class="comment"># 设置记录的日志等级，有CRITICAL、ERROR、WARNING、INFO、DEBUG、NOTSET</span></span><br><span class="line">    logger.setLevel(logging.DEBUG)</span><br><span class="line">    <span class="comment"># 禁止层级传递</span></span><br><span class="line">    logger.propagate = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create formatter</span></span><br><span class="line">    <span class="comment"># 格式化输出</span></span><br><span class="line">    fmt = <span class="string">&#x27;[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s&#x27;</span></span><br><span class="line">    color_fmt = colored(<span class="string">&#x27;[%(asctime)s %(name)s]&#x27;</span>, <span class="string">&#x27;green&#x27;</span>) + colored(<span class="string">&#x27;(%(filename)s %(lineno)d)&#x27;</span>,</span><br><span class="line">                                                                     <span class="string">&#x27;yellow&#x27;</span>) + <span class="string">&#x27;: %(levelname)s %(message)s&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create console handlers for master process</span></span><br><span class="line">    <span class="keyword">if</span> dist_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 主进程的控制句柄为向标准输出输出信息</span></span><br><span class="line">        console_handler = logging.StreamHandler(sys.stdout)</span><br><span class="line">        console_handler.setLevel(logging.DEBUG)</span><br><span class="line">        console_handler.setFormatter(</span><br><span class="line">            logging.Formatter(fmt=color_fmt, datefmt=<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>))</span><br><span class="line">        logger.addHandler(console_handler)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create file handlers</span></span><br><span class="line">    <span class="comment"># 文件句柄，向文件进行输出日志信息</span></span><br><span class="line">    file_handler = logging.FileHandler(os.path.join(output_dir, <span class="string">f&#x27;log_rank<span class="subst">&#123;dist_rank&#125;</span>.txt&#x27;</span>), mode=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    file_handler.setLevel(logging.DEBUG)</span><br><span class="line">    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>))</span><br><span class="line">    logger.addHandler(file_handler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure>
<p>swin transformer基本结构：</p>
<p>包括一个用于将图像打成patch的Patch Partition层、对patch后图像channel数目进行变换的Linear Embedding层，以及4个Basic Block用于层级transformer变换。数据流向为：</p>
<ul>
<li>
<p>Patch Partition和Linear Embedding层在代码中合并为Patch Embed层，用于将图片打成patch和变换channels数目。输入图像从(B,H,W,3)变换至(B,H//num_patch, W//num_patch, num_patch<em>num_patch</em>3)，再变换至(B,H//num_patch, W//num_patch, 96)，代码实现是通过步长为num_patch的卷积操作一次性实现两个操作。此时形状记为(B, H0, W0, C)，注意该形状为代码实现中的写法，和图中表示有所不同。</p>
</li>
<li>
<p>BasicBlock层包括多个Swin Transformer Block和一个Patch Merging层。Swin Transformer网络一共包含4个Basic Block，每层具有的Swin Transformer Block数目为[2,2,6,2]，每个Basic Block层中仅第一个Swin Transformer不需要实现shifted window操作。</p>
<ol>
<li>
<p>Swin Transformer Block中首先通过window_partition操作将图片划分为H0//window_size * W0//window_size个(B, window_size, window_size, C)形状的小窗口，并将其组织成(H0//window_size * W0//window_size * B, window_size * window_size, C)的形状，记为(nW<em>B, N, C)，其中nW=H0//window_size</em>W0//window_size为划分的小窗口数量，然后执行多头注意力WindowsAttention操作。</p>
</li>
<li>
<p>WindowsAttention中先对矩阵通过全连接层生成形状均为的(nW<em>B, N, C)的三个矩阵Q,K,V，然后按照多头注意力的数目将其形状变为(nW</em>B, num_heads, N, C//num_heads)，将其视为nW<em>B</em>num_heads个维度为(N,C//num_heads)的矩阵，通过 attn=Q K^\top 计算得到形状为(nW<em>B, num_heads, N, N)的注意力矩阵，然后将通过attn和V矩阵相乘得到形状为(nW</em>B, num_heads, N, C//num_heads)的矩阵，最后将其组织为(nW*B, N, C)形状的矩阵进行输出。因此WindowsAttention输入和输出的维度相同，可以任意堆叠多层。</p>
</li>
<li>
<p>PatchMerging层之前，首先将多层WindowsAttention堆叠输出得到的(nW<em>B, N, C)矩阵先按照小窗口的划分方式将小窗口合并为(B, N</em>nW, C)，用输入图片的形状表示即为(B, H0<em>W0, C)，然后将其变换为Swin Transformer Block层最初输入的图片形状(B, H0, W0, C)。PatchMerging层中通过间隔采样得到4个形状为(B, H0//2, W0//2, C)的矩阵，拼接得到的(B, H0//2, W0//2, C</em>4)再送到全连接降维成(B, H0//2, W0//2, C<em>2)的矩阵。该层输入为(B, H0, W0, C)，输出为(B, H0//2, W0//2, C</em>2)从而实现下采样的目的。</p>
</li>
</ol>
</li>
<li>
<p>由于最后一层Basic Block没有下采样层，因此经过4层Basic Block得到的输出为(B, H0//2^3, W0//2^3, C<em>2^3)，经过AvgPool转化为(B, 1, C</em>2^3)的形式，然后送入全连接进行分类。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        <span class="variable language_">self</span>.act = act_layer()</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        <span class="variable language_">self</span>.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.act(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size, H, W</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将nW*B个窗口合并为一个</span></span><br><span class="line">    <span class="comment"># B = B*nW/nW = (windows.shape[0]) / (H*W/window_size/window_size)</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        <span class="variable language_">self</span>.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(<span class="variable language_">self</span>.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(<span class="variable language_">self</span>.window_size[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 避免报错信息</span></span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&#x27;ij&#x27;</span>))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += <span class="variable language_">self</span>.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += <span class="variable language_">self</span>.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * <span class="variable language_">self</span>.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        <span class="comment"># 注册为不可变参数，在保存模型时该参数也会被保存</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line">        <span class="comment"># 96 -&gt; 96*3</span></span><br><span class="line">        <span class="variable language_">self</span>.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(dim, dim)</span><br><span class="line">        <span class="variable language_">self</span>.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line">        <span class="comment"># 截断到(\mu-3\sigma,\mu+3\sigma)之内的正态分布</span></span><br><span class="line">        trunc_normal_(<span class="variable language_">self</span>.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        <span class="variable language_">self</span>.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = <span class="variable language_">self</span>.qkv(x).reshape(B_, N, <span class="number">3</span>, <span class="variable language_">self</span>.num_heads, C // <span class="variable language_">self</span>.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># (3, B_, num_heads, N, C//num_heads)</span></span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力机制</span></span><br><span class="line">        q = q * <span class="variable language_">self</span>.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># (B_, num_heads, N, N)</span></span><br><span class="line"></span><br><span class="line">        relative_position_bias = <span class="variable language_">self</span>.relative_position_bias_table[<span class="variable language_">self</span>.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            <span class="variable language_">self</span>.window_size[<span class="number">0</span>] * <span class="variable language_">self</span>.window_size[<span class="number">1</span>], <span class="variable language_">self</span>.window_size[<span class="number">0</span>] * <span class="variable language_">self</span>.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># mask用于屏蔽不应connect的注意力矩阵部分</span></span><br><span class="line">            attn = attn.view(B_ // nW, nW, <span class="variable language_">self</span>.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, N, N)</span><br><span class="line">            attn = <span class="variable language_">self</span>.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = <span class="variable language_">self</span>.softmax(attn)</span><br><span class="line">        <span class="comment"># 对注意力图进行dropout操作</span></span><br><span class="line">        attn = <span class="variable language_">self</span>.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&#x27;dim=<span class="subst">&#123;self.dim&#125;</span>, window_size=<span class="subst">&#123;self.window_size&#125;</span>, num_heads=<span class="subst">&#123;self.num_heads&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">flops</span>(<span class="params">self, N</span>):</span><br><span class="line">        <span class="comment"># 计算复杂度，并不是每秒的浮点运算数</span></span><br><span class="line">        <span class="comment"># calculate flops for 1 window with token length of N</span></span><br><span class="line">        flops = <span class="number">0</span></span><br><span class="line">        <span class="comment"># qkv = self.qkv(x)</span></span><br><span class="line">        <span class="comment"># N是小块拉直的数目(token长度)，dim是每个序列的长度</span></span><br><span class="line">        <span class="comment"># (N,dim)(dim,dim*3)-&gt;(N,3)计算复杂度为(N*dim*dim*3)</span></span><br><span class="line">        flops += N * <span class="variable language_">self</span>.dim * <span class="number">3</span> * <span class="variable language_">self</span>.dim</span><br><span class="line">        <span class="comment"># attn = (q @ k.transpose(-2, -1))</span></span><br><span class="line">        <span class="comment"># 先拆分为多头注意力，计算复杂度为num_heads*N*(dim//N)*N</span></span><br><span class="line">        <span class="comment"># (num_heads, N, dim//num_heads) (num_heads, N, dim//num_heads).T -&gt; (num_heads, N, N)</span></span><br><span class="line">        flops += <span class="variable language_">self</span>.num_heads * N * (<span class="variable language_">self</span>.dim // <span class="variable language_">self</span>.num_heads) * N</span><br><span class="line">        <span class="comment">#  x = (attn @ v)，计算复杂度为num_heads*N^2*dim//num_heads</span></span><br><span class="line">        <span class="comment"># (num_heads, N, N)(num_heads, N, dim//num_heads) -&gt; (num_heads, N, dim//num_heads)</span></span><br><span class="line">        flops += <span class="variable language_">self</span>.num_heads * N * N * (<span class="variable language_">self</span>.dim // <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        <span class="comment"># x = self.proj(x)，计算复杂度为(N, dim, dim)</span></span><br><span class="line">        <span class="comment"># (N, dim) -&gt; (N, dim)</span></span><br><span class="line">        flops += N * <span class="variable language_">self</span>.dim * <span class="variable language_">self</span>.dim</span><br><span class="line">        <span class="keyword">return</span> flops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer Block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Input resulotion.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Window size.</span></span><br><span class="line"><span class="string">        shift_size (int): Shift size for SW-MSA.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, input_resolution, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,</span></span><br><span class="line"><span class="params">                 fused_window_process=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size</span><br><span class="line">        <span class="variable language_">self</span>.shift_size = shift_size</span><br><span class="line">        <span class="variable language_">self</span>.mlp_ratio = mlp_ratio</span><br><span class="line">        <span class="comment"># 在小范围内做自注意力</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">min</span>(<span class="variable language_">self</span>.input_resolution) &lt;= <span class="variable language_">self</span>.window_size:</span><br><span class="line">            <span class="comment"># if window size is larger than input resolution, we don&#x27;t partition windows</span></span><br><span class="line">            <span class="variable language_">self</span>.shift_size = <span class="number">0</span></span><br><span class="line">            <span class="variable language_">self</span>.window_size = <span class="built_in">min</span>(<span class="variable language_">self</span>.input_resolution)</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="variable language_">self</span>.shift_size &lt; <span class="variable language_">self</span>.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = norm_layer(dim)</span><br><span class="line">        <span class="variable language_">self</span>.attn = WindowAttention(</span><br><span class="line">            dim, window_size=to_2tuple(<span class="variable language_">self</span>.window_size), num_heads=num_heads,</span><br><span class="line">            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="comment"># 一种正则化手段，再batch维度随机设置一定样本不进行主干而直接由分支进行恒等映射</span></span><br><span class="line">        <span class="variable language_">self</span>.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">            H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">            img_mask = torch.zeros((<span class="number">1</span>, H, W, <span class="number">1</span>))  <span class="comment"># 1 H W 1</span></span><br><span class="line">            h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -<span class="variable language_">self</span>.window_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.window_size, -<span class="variable language_">self</span>.shift_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.shift_size, <span class="literal">None</span>))</span><br><span class="line">            w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -<span class="variable language_">self</span>.window_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.window_size, -<span class="variable language_">self</span>.shift_size),</span><br><span class="line">                        <span class="built_in">slice</span>(-<span class="variable language_">self</span>.shift_size, <span class="literal">None</span>))</span><br><span class="line">            <span class="comment"># 给每块区域划分标记序号，每一个维度分为0:-7,-7:-2,-2:三部分</span></span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">                    img_mask[:, h, w, :] = cnt</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 生成掩码窗口，64, 7, 7, 1</span></span><br><span class="line">            mask_windows = window_partition(img_mask, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW, window_size, window_size, 1</span></span><br><span class="line">            <span class="comment"># mask_windows (64, 49)</span></span><br><span class="line">            mask_windows = mask_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size * <span class="variable language_">self</span>.window_size)</span><br><span class="line">            attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_mask = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 获得掩码矩阵，具体看论文中所说</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;attn_mask&quot;</span>, attn_mask)</span><br><span class="line">        <span class="variable language_">self</span>.fused_window_process = fused_window_process</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        shortcut = x</span><br><span class="line">        <span class="comment"># 标准化层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            shifted_x = torch.roll(x, shifts=(-<span class="variable language_">self</span>.shift_size, -<span class="variable language_">self</span>.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            <span class="comment"># partition windows</span></span><br><span class="line">            x_windows = window_partition(shifted_x, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = x</span><br><span class="line">            <span class="comment"># partition windows，nW为窗口数目</span></span><br><span class="line">            x_windows = window_partition(shifted_x, <span class="variable language_">self</span>.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">        <span class="comment"># 窗口数目*每个窗口的token长度，通道数目</span></span><br><span class="line">        x_windows = x_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size * <span class="variable language_">self</span>.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># W-MSA/SW-MSA，多个窗口可以并行地做注意力</span></span><br><span class="line">        attn_windows = <span class="variable language_">self</span>.attn(x_windows, mask=<span class="variable language_">self</span>.attn_mask)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge windows</span></span><br><span class="line">        attn_windows = attn_windows.view(-<span class="number">1</span>, <span class="variable language_">self</span>.window_size, <span class="variable language_">self</span>.window_size, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 将所有小窗口合并为一个大窗口</span></span><br><span class="line">            shifted_x = window_reverse(attn_windows, <span class="variable language_">self</span>.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line">            <span class="comment"># 对整个窗口进行左上的平移操作</span></span><br><span class="line">            x = torch.roll(shifted_x, shifts=(<span class="variable language_">self</span>.shift_size, <span class="variable language_">self</span>.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = window_reverse(attn_windows, <span class="variable language_">self</span>.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line">            x = shifted_x</span><br><span class="line">        x = x.view(B, H * W, C)</span><br><span class="line">        x = shortcut + <span class="variable language_">self</span>.drop_path(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.drop_path(<span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.norm2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 下采样，每行、每列间隔采样，通道数扩增到4倍，然后再将回到2倍</span></span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; A basic Swin Transformer layer for one stage.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Input resolution.</span></span><br><span class="line"><span class="string">        depth (int): Number of blocks.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Local window size.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, input_resolution, depth, num_heads, window_size,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 fused_window_process=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.depth = depth</span><br><span class="line">        <span class="variable language_">self</span>.use_checkpoint = use_checkpoint</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build blocks</span></span><br><span class="line">        <span class="variable language_">self</span>.blocks = nn.ModuleList([</span><br><span class="line">            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,</span><br><span class="line">                                 num_heads=num_heads, window_size=window_size,</span><br><span class="line">                                 shift_size=<span class="number">1</span>,</span><br><span class="line">                                 mlp_ratio=mlp_ratio,</span><br><span class="line">                                 qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                                 drop=drop, attn_drop=attn_drop,</span><br><span class="line">                                 drop_path=drop_path[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(drop_path, <span class="built_in">list</span>) <span class="keyword">else</span> drop_path,</span><br><span class="line">                                 norm_layer=norm_layer,</span><br><span class="line">                                 fused_window_process=fused_window_process)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch merging layer</span></span><br><span class="line">        <span class="keyword">if</span> downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.downsample = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> <span class="variable language_">self</span>.blocks:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_checkpoint:</span><br><span class="line">                x = checkpoint.checkpoint(blk, x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = blk(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = <span class="variable language_">self</span>.downsample(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int): Image size.  Default: 224.</span></span><br><span class="line"><span class="string">        patch_size (int): Patch token size. Default: 4.</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3.</span></span><br><span class="line"><span class="string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 224 -&gt; (224, 224)</span></span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        <span class="comment"># 4 -&gt; (4, 4)</span></span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        <span class="comment"># 切割成[56,56]个块</span></span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        <span class="variable language_">self</span>.img_size = img_size</span><br><span class="line">        <span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">        <span class="variable language_">self</span>.patches_resolution = patches_resolution</span><br><span class="line">        <span class="variable language_">self</span>.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 输入通道数目为3</span></span><br><span class="line">        <span class="variable language_">self</span>.in_chans = in_chans</span><br><span class="line">        <span class="comment"># 嵌入维度为96</span></span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">        <span class="comment"># (b,3,224,224) -&gt; (b,96,224//4,224//4)通道数变化</span></span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="comment"># 层归一化，常用于nlp，因此可能来源于transformer自带的</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == <span class="variable language_">self</span>.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == <span class="variable language_">self</span>.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B Ph*Pw C</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer</span></span><br><span class="line"><span class="string">        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -</span></span><br><span class="line"><span class="string">          https://arxiv.org/pdf/2103.14030</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int | tuple(int)): Input image size. Default 224</span></span><br><span class="line"><span class="string">        patch_size (int | tuple(int)): Patch size. Default: 4</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3</span></span><br><span class="line"><span class="string">        num_classes (int): Number of classes for classification head. Default: 1000</span></span><br><span class="line"><span class="string">        embed_dim (int): Patch embedding dimension. Default: 96</span></span><br><span class="line"><span class="string">        depths (tuple(int)): Depth of each Swin Transformer layer.</span></span><br><span class="line"><span class="string">        num_heads (tuple(int)): Number of attention heads in different layers.</span></span><br><span class="line"><span class="string">        window_size (int): Window size. Default: 7</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4</span></span><br><span class="line"><span class="string">        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None</span></span><br><span class="line"><span class="string">        drop_rate (float): Dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        attn_drop_rate (float): Attention dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        drop_path_rate (float): Stochastic depth rate. Default: 0.1</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.</span></span><br><span class="line"><span class="string">        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False</span></span><br><span class="line"><span class="string">        patch_norm (bool): If True, add normalization after patch embedding. Default: True</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False</span></span><br><span class="line"><span class="string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">96</span>, depths=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span>], num_heads=[<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>],</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">7</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm, ape=<span class="literal">False</span>, patch_norm=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 use_checkpoint=<span class="literal">False</span>, fused_window_process=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="built_in">len</span>(depths)  <span class="comment"># depths每一个swin transformer层的深度，因为swin transformer可以随意堆叠</span></span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">        <span class="variable language_">self</span>.ape = ape  <span class="comment"># 绝对位置编码</span></span><br><span class="line">        <span class="variable language_">self</span>.patch_norm = patch_norm  <span class="comment"># patch归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.num_features = <span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** (<span class="variable language_">self</span>.num_layers - <span class="number">1</span>))  <span class="comment"># 经过所有层后的通道数量</span></span><br><span class="line">        <span class="variable language_">self</span>.mlp_ratio = mlp_ratio  <span class="comment"># mlp隐藏层维度与嵌入维度的商</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># split image into non-overlapping patches</span></span><br><span class="line">        <span class="comment"># 对图片进行切割的块</span></span><br><span class="line">        <span class="variable language_">self</span>.patch_embed = PatchEmbed(</span><br><span class="line">            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,</span><br><span class="line">            norm_layer=norm_layer <span class="keyword">if</span> <span class="variable language_">self</span>.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        num_patches = <span class="variable language_">self</span>.patch_embed.num_patches</span><br><span class="line">        patches_resolution = <span class="variable language_">self</span>.patch_embed.patches_resolution</span><br><span class="line">        <span class="variable language_">self</span>.patches_resolution = patches_resolution</span><br><span class="line"></span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.ape:</span><br><span class="line">            <span class="variable language_">self</span>.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            trunc_normal_(<span class="variable language_">self</span>.absolute_pos_embed, std=<span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stochastic depth</span></span><br><span class="line">        <span class="comment"># swin transformer层数一共为12，其中drop_path_rate=0.2，每一层的dropout概率从0增加到0.2</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># 每经过一层BasicLayer，图片宽和高减半，channels加倍，96-&gt;96*2-&gt;96*4</span></span><br><span class="line">        <span class="comment"># 初始的图片为(b, 96, 3136)</span></span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_layers):</span><br><span class="line">            layer = BasicLayer(dim=<span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** i_layer),</span><br><span class="line">                               input_resolution=(patches_resolution[<span class="number">0</span>] // (<span class="number">2</span> ** i_layer),</span><br><span class="line">                                                 patches_resolution[<span class="number">1</span>] // (<span class="number">2</span> ** i_layer)),</span><br><span class="line">                               depth=depths[i_layer],  <span class="comment"># [2, 2, 6, 2]</span></span><br><span class="line">                               num_heads=num_heads[i_layer],</span><br><span class="line">                               window_size=window_size,  <span class="comment"># 窗口尺寸是7</span></span><br><span class="line">                               mlp_ratio=<span class="variable language_">self</span>.mlp_ratio,</span><br><span class="line">                               qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                               drop=drop_rate, attn_drop=attn_drop_rate,</span><br><span class="line">                               drop_path=dpr[<span class="built_in">sum</span>(depths[:i_layer]):<span class="built_in">sum</span>(depths[:i_layer + <span class="number">1</span>])],</span><br><span class="line">                               norm_layer=norm_layer,</span><br><span class="line">                               downsample=PatchMerging <span class="keyword">if</span> (i_layer &lt; <span class="variable language_">self</span>.num_layers - <span class="number">1</span>) <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                               use_checkpoint=use_checkpoint,</span><br><span class="line">                               fused_window_process=fused_window_process)</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="variable language_">self</span>.num_features)</span><br><span class="line">        <span class="variable language_">self</span>.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.head = nn.Linear(<span class="variable language_">self</span>.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.apply(<span class="variable language_">self</span>._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            trunc_normal_(m.weight, std=<span class="number">.02</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.patch_embed(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.ape:</span><br><span class="line">            x = x + <span class="variable language_">self</span>.absolute_pos_embed</span><br><span class="line">        x = <span class="variable language_">self</span>.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = <span class="variable language_">self</span>.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.forward_features(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="pytorch官方训练参考中给出的相关代码">pytorch官方训练参考中给出的相关代码</h2>
<p>图片预处理相关</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> autoaugment, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationPresetTrain</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        crop_size,</span></span><br><span class="line"><span class="params">        mean=(<span class="params"><span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span></span>),</span></span><br><span class="line"><span class="params">        std=(<span class="params"><span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span></span>),</span></span><br><span class="line"><span class="params">        interpolation=InterpolationMode.BILINEAR,</span></span><br><span class="line"><span class="params">        hflip_prob=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        auto_augment_policy=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        ra_magnitude=<span class="number">9</span>,</span></span><br><span class="line"><span class="params">        augmix_severity=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">        random_erase_prob=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        trans = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]</span><br><span class="line">        <span class="keyword">if</span> hflip_prob &gt; <span class="number">0</span>:</span><br><span class="line">            trans.append(transforms.RandomHorizontalFlip(hflip_prob))</span><br><span class="line">        <span class="keyword">if</span> auto_augment_policy <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> auto_augment_policy == <span class="string">&quot;ra&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.RandAugment(interpolation=interpolation, magnitude=ra_magnitude))</span><br><span class="line">            <span class="keyword">elif</span> auto_augment_policy == <span class="string">&quot;ta_wide&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.TrivialAugmentWide(interpolation=interpolation))</span><br><span class="line">            <span class="keyword">elif</span> auto_augment_policy == <span class="string">&quot;augmix&quot;</span>:</span><br><span class="line">                trans.append(autoaugment.AugMix(interpolation=interpolation, severity=augmix_severity))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                aa_policy = autoaugment.AutoAugmentPolicy(auto_augment_policy)</span><br><span class="line">                trans.append(autoaugment.AutoAugment(policy=aa_policy, interpolation=interpolation))</span><br><span class="line">        trans.extend(</span><br><span class="line">            [</span><br><span class="line">                transforms.PILToTensor(),</span><br><span class="line">                transforms.ConvertImageDtype(torch.<span class="built_in">float</span>),</span><br><span class="line">                transforms.Normalize(mean=mean, std=std),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> random_erase_prob &gt; <span class="number">0</span>:</span><br><span class="line">            trans.append(transforms.RandomErasing(p=random_erase_prob))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transforms = transforms.Compose(trans)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transforms(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationPresetEval</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        crop_size,</span></span><br><span class="line"><span class="params">        resize_size=<span class="number">256</span>,</span></span><br><span class="line"><span class="params">        mean=(<span class="params"><span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span></span>),</span></span><br><span class="line"><span class="params">        std=(<span class="params"><span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span></span>),</span></span><br><span class="line"><span class="params">        interpolation=InterpolationMode.BILINEAR,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transforms = transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.Resize(resize_size, interpolation=interpolation),</span><br><span class="line">                transforms.CenterCrop(crop_size),</span><br><span class="line">                transforms.PILToTensor(),</span><br><span class="line">                transforms.ConvertImageDtype(torch.<span class="built_in">float</span>),</span><br><span class="line">                transforms.Normalize(mean=mean, std=std),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transforms(img)</span><br></pre></td></tr></table></figure>
<p>分布式数据采样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RASampler</span>(torch.utils.data.Sampler):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sampler that restricts data loading to a subset of the dataset for distributed,</span></span><br><span class="line"><span class="string">    with repeated augmentation.</span></span><br><span class="line"><span class="string">    It ensures that different each augmented version of a sample will be visible to a</span></span><br><span class="line"><span class="string">    different process (GPU).</span></span><br><span class="line"><span class="string">    Heavily based on &#x27;torch.utils.data.DistributedSampler&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is borrowed from the DeiT Repo:</span></span><br><span class="line"><span class="string">    https://github.com/facebookresearch/deit/blob/main/samplers.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, num_replicas=<span class="literal">None</span>, rank=<span class="literal">None</span>, shuffle=<span class="literal">True</span>, seed=<span class="number">0</span>, repetitions=<span class="number">3</span></span>):</span><br><span class="line">        <span class="keyword">if</span> num_replicas <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Requires distributed package to be available!&quot;</span>)</span><br><span class="line">            num_replicas = dist.get_world_size()</span><br><span class="line">        <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Requires distributed package to be available!&quot;</span>)</span><br><span class="line">            rank = dist.get_rank()</span><br><span class="line">        <span class="variable language_">self</span>.dataset = dataset</span><br><span class="line">        <span class="variable language_">self</span>.num_replicas = num_replicas</span><br><span class="line">        <span class="variable language_">self</span>.rank = rank</span><br><span class="line">        <span class="variable language_">self</span>.epoch = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.num_samples = <span class="built_in">int</span>(math.ceil(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset) * <span class="built_in">float</span>(repetitions) / <span class="variable language_">self</span>.num_replicas))</span><br><span class="line">        <span class="variable language_">self</span>.total_size = <span class="variable language_">self</span>.num_samples * <span class="variable language_">self</span>.num_replicas</span><br><span class="line">        <span class="variable language_">self</span>.num_selected_samples = <span class="built_in">int</span>(math.floor(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset) // <span class="number">256</span> * <span class="number">256</span> / <span class="variable language_">self</span>.num_replicas))</span><br><span class="line">        <span class="variable language_">self</span>.shuffle = shuffle</span><br><span class="line">        <span class="variable language_">self</span>.seed = seed</span><br><span class="line">        <span class="variable language_">self</span>.repetitions = repetitions</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shuffle:</span><br><span class="line">            <span class="comment"># Deterministically shuffle based on epoch</span></span><br><span class="line">            g = torch.Generator()</span><br><span class="line">            g.manual_seed(<span class="variable language_">self</span>.seed + <span class="variable language_">self</span>.epoch)</span><br><span class="line">            indices = torch.randperm(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset), generator=g).tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add extra samples to make it evenly divisible</span></span><br><span class="line">        indices = [ele <span class="keyword">for</span> ele <span class="keyword">in</span> indices <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.repetitions)]</span><br><span class="line">        indices += indices[: (<span class="variable language_">self</span>.total_size - <span class="built_in">len</span>(indices))]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(indices) == <span class="variable language_">self</span>.total_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Subsample</span></span><br><span class="line">        indices = indices[<span class="variable language_">self</span>.rank : <span class="variable language_">self</span>.total_size : <span class="variable language_">self</span>.num_replicas]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(indices) == <span class="variable language_">self</span>.num_samples</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(indices[: <span class="variable language_">self</span>.num_selected_samples])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.num_selected_samples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_epoch</span>(<span class="params">self, epoch</span>):</span><br><span class="line">        <span class="variable language_">self</span>.epoch = epoch</span><br></pre></td></tr></table></figure>
<p>数据增强措施CutMix和MixUp</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomMixup</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly apply Mixup to the provided batch and targets.</span></span><br><span class="line"><span class="string">    The class implements the data augmentations as described in the paper</span></span><br><span class="line"><span class="string">    `&quot;mixup: Beyond Empirical Risk Minimization&quot; &lt;https://arxiv.org/abs/1710.09412&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes used for one-hot encoding.</span></span><br><span class="line"><span class="string">        p (float): probability of the batch being transformed. Default value is 0.5.</span></span><br><span class="line"><span class="string">        alpha (float): hyperparameter of the Beta distribution used for mixup.</span></span><br><span class="line"><span class="string">            Default value is 1.0.</span></span><br><span class="line"><span class="string">        inplace (bool): boolean to make this transform inplace. Default set to False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, p: <span class="built_in">float</span> = <span class="number">0.5</span>, alpha: <span class="built_in">float</span> = <span class="number">1.0</span>, inplace: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_classes &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Please provide a valid positive value for the num_classes. Got num_classes=<span class="subst">&#123;num_classes&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> alpha &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Alpha param can&#x27;t be zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.inplace = inplace</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Tensor, target: Tensor</span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch (Tensor): Float tensor of size (B, C, H, W)</span></span><br><span class="line"><span class="string">            target (Tensor): Integer tensor of size (B, )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Randomly transformed batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> batch.ndim != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Batch ndim should be 4. Got <span class="subst">&#123;batch.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim != <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Target ndim should be 1. Got <span class="subst">&#123;target.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch.is_floating_point():</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Batch dtype should be a float tensor. Got <span class="subst">&#123;batch.dtype&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.dtype != torch.int64:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Target dtype should be torch.int64. Got <span class="subst">&#123;target.dtype&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.inplace:</span><br><span class="line">            batch = batch.clone()</span><br><span class="line">            target = target.clone()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">1</span>:</span><br><span class="line">            target = torch.nn.functional.one_hot(target, num_classes=<span class="variable language_">self</span>.num_classes).to(dtype=batch.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.rand(<span class="number">1</span>).item() &gt;= <span class="variable language_">self</span>.p:</span><br><span class="line">            <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">        <span class="comment"># It&#x27;s faster to roll the batch by one instead of shuffling it to create image pairs</span></span><br><span class="line">        batch_rolled = batch.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        target_rolled = target.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Implemented as on mixup paper, page 3.</span></span><br><span class="line">        lambda_param = <span class="built_in">float</span>(torch._sample_dirichlet(torch.tensor([<span class="variable language_">self</span>.alpha, <span class="variable language_">self</span>.alpha]))[<span class="number">0</span>])</span><br><span class="line">        batch_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        batch.mul_(lambda_param).add_(batch_rolled)</span><br><span class="line"></span><br><span class="line">        target_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        target.mul_(lambda_param).add_(target_rolled)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s = (</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;self.__class__.__name__&#125;</span>(&quot;</span></span><br><span class="line">            <span class="string">f&quot;num_classes=<span class="subst">&#123;self.num_classes&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, p=<span class="subst">&#123;self.p&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, alpha=<span class="subst">&#123;self.alpha&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, inplace=<span class="subst">&#123;self.inplace&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;)&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomCutmix</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly apply Cutmix to the provided batch and targets.</span></span><br><span class="line"><span class="string">    The class implements the data augmentations as described in the paper</span></span><br><span class="line"><span class="string">    `&quot;CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features&quot;</span></span><br><span class="line"><span class="string">    &lt;https://arxiv.org/abs/1905.04899&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_classes (int): number of classes used for one-hot encoding.</span></span><br><span class="line"><span class="string">        p (float): probability of the batch being transformed. Default value is 0.5.</span></span><br><span class="line"><span class="string">        alpha (float): hyperparameter of the Beta distribution used for cutmix.</span></span><br><span class="line"><span class="string">            Default value is 1.0.</span></span><br><span class="line"><span class="string">        inplace (bool): boolean to make this transform inplace. Default set to False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, p: <span class="built_in">float</span> = <span class="number">0.5</span>, alpha: <span class="built_in">float</span> = <span class="number">1.0</span>, inplace: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_classes &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Please provide a valid positive value for the num_classes.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> alpha &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Alpha param can&#x27;t be zero.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.inplace = inplace</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Tensor, target: Tensor</span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch (Tensor): Float tensor of size (B, C, H, W)</span></span><br><span class="line"><span class="string">            target (Tensor): Integer tensor of size (B, )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Randomly transformed batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> batch.ndim != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Batch ndim should be 4. Got <span class="subst">&#123;batch.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim != <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Target ndim should be 1. Got <span class="subst">&#123;target.ndim&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch.is_floating_point():</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Batch dtype should be a float tensor. Got <span class="subst">&#123;batch.dtype&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> target.dtype != torch.int64:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Target dtype should be torch.int64. Got <span class="subst">&#123;target.dtype&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.inplace:</span><br><span class="line">            batch = batch.clone()</span><br><span class="line">            target = target.clone()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">1</span>:</span><br><span class="line">            target = torch.nn.functional.one_hot(target, num_classes=<span class="variable language_">self</span>.num_classes).to(dtype=batch.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.rand(<span class="number">1</span>).item() &gt;= <span class="variable language_">self</span>.p:</span><br><span class="line">            <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">        <span class="comment"># It&#x27;s faster to roll the batch by one instead of shuffling it to create image pairs</span></span><br><span class="line">        batch_rolled = batch.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        target_rolled = target.roll(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Implemented as on cutmix paper, page 12 (with minor corrections on typos).</span></span><br><span class="line">        lambda_param = <span class="built_in">float</span>(torch._sample_dirichlet(torch.tensor([<span class="variable language_">self</span>.alpha, <span class="variable language_">self</span>.alpha]))[<span class="number">0</span>])</span><br><span class="line">        _, H, W = F.get_dimensions(batch)</span><br><span class="line"></span><br><span class="line">        r_x = torch.randint(W, (<span class="number">1</span>,))</span><br><span class="line">        r_y = torch.randint(H, (<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">        r = <span class="number">0.5</span> * math.sqrt(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        r_w_half = <span class="built_in">int</span>(r * W)</span><br><span class="line">        r_h_half = <span class="built_in">int</span>(r * H)</span><br><span class="line"></span><br><span class="line">        x1 = <span class="built_in">int</span>(torch.clamp(r_x - r_w_half, <span class="built_in">min</span>=<span class="number">0</span>))</span><br><span class="line">        y1 = <span class="built_in">int</span>(torch.clamp(r_y - r_h_half, <span class="built_in">min</span>=<span class="number">0</span>))</span><br><span class="line">        x2 = <span class="built_in">int</span>(torch.clamp(r_x + r_w_half, <span class="built_in">max</span>=W))</span><br><span class="line">        y2 = <span class="built_in">int</span>(torch.clamp(r_y + r_h_half, <span class="built_in">max</span>=H))</span><br><span class="line"></span><br><span class="line">        batch[:, :, y1:y2, x1:x2] = batch_rolled[:, :, y1:y2, x1:x2]</span><br><span class="line">        lambda_param = <span class="built_in">float</span>(<span class="number">1.0</span> - (x2 - x1) * (y2 - y1) / (W * H))</span><br><span class="line"></span><br><span class="line">        target_rolled.mul_(<span class="number">1.0</span> - lambda_param)</span><br><span class="line">        target.mul_(lambda_param).add_(target_rolled)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s = (</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;self.__class__.__name__&#125;</span>(&quot;</span></span><br><span class="line">            <span class="string">f&quot;num_classes=<span class="subst">&#123;self.num_classes&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, p=<span class="subst">&#123;self.p&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, alpha=<span class="subst">&#123;self.alpha&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;, inplace=<span class="subst">&#123;self.inplace&#125;</span>&quot;</span></span><br><span class="line">            <span class="string">f&quot;)&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>数值平滑，减轻训练过程中损失函数数值的抖动</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque, OrderedDict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmoothedValue</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track a series of values and provide access to smoothed values over a</span></span><br><span class="line"><span class="string">    window or the global series average.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, window_size=<span class="number">20</span>, fmt=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> fmt <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            fmt = <span class="string">&quot;&#123;median:.4f&#125; (&#123;global_avg:.4f&#125;)&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.deque = deque(maxlen=window_size)</span><br><span class="line">        <span class="variable language_">self</span>.total = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.count = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.fmt = fmt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, value, n=<span class="number">1</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.deque.append(value)</span><br><span class="line">        <span class="variable language_">self</span>.count += n</span><br><span class="line">        <span class="variable language_">self</span>.total += value * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">synchronize_between_processes</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Warning: does not synchronize the deque!</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        t = reduce_across_processes([<span class="variable language_">self</span>.count, <span class="variable language_">self</span>.total])</span><br><span class="line">        t = t.tolist()</span><br><span class="line">        <span class="variable language_">self</span>.count = <span class="built_in">int</span>(t[<span class="number">0</span>])</span><br><span class="line">        <span class="variable language_">self</span>.total = t[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">median</span>(<span class="params">self</span>):</span><br><span class="line">        d = torch.tensor(<span class="built_in">list</span>(<span class="variable language_">self</span>.deque))</span><br><span class="line">        <span class="keyword">return</span> d.median().item()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">        d = torch.tensor(<span class="built_in">list</span>(<span class="variable language_">self</span>.deque), dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> d.mean().item()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">global_avg</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.total / <span class="variable language_">self</span>.count</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">max</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(<span class="variable language_">self</span>.deque)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">value</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.deque[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fmt.<span class="built_in">format</span>(</span><br><span class="line">            median=<span class="variable language_">self</span>.median, avg=<span class="variable language_">self</span>.avg, global_avg=<span class="variable language_">self</span>.global_avg, <span class="built_in">max</span>=<span class="variable language_">self</span>.<span class="built_in">max</span>, value=<span class="variable language_">self</span>.value</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MetricLogger</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, delimiter=<span class="string">&quot;\t&quot;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.meters = defaultdict(SmoothedValue)</span><br><span class="line">        <span class="variable language_">self</span>.delimiter = delimiter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(v, torch.Tensor):</span><br><span class="line">                v = v.item()</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">isinstance</span>(v, (<span class="built_in">float</span>, <span class="built_in">int</span>))</span><br><span class="line">            <span class="variable language_">self</span>.meters[k].update(v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getattr__</span>(<span class="params">self, attr</span>):</span><br><span class="line">        <span class="keyword">if</span> attr <span class="keyword">in</span> <span class="variable language_">self</span>.meters:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.meters[attr]</span><br><span class="line">        <span class="keyword">if</span> attr <span class="keyword">in</span> <span class="variable language_">self</span>.__dict__:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.__dict__[attr]</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(<span class="string">f&quot;&#x27;<span class="subst">&#123;<span class="built_in">type</span>(self).__name__&#125;</span>&#x27; object has no attribute &#x27;<span class="subst">&#123;attr&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        loss_str = []</span><br><span class="line">        <span class="keyword">for</span> name, meter <span class="keyword">in</span> <span class="variable language_">self</span>.meters.items():</span><br><span class="line">            loss_str.append(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(meter)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.delimiter.join(loss_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">synchronize_between_processes</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> meter <span class="keyword">in</span> <span class="variable language_">self</span>.meters.values():</span><br><span class="line">            meter.synchronize_between_processes()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_meter</span>(<span class="params">self, name, meter</span>):</span><br><span class="line">        <span class="variable language_">self</span>.meters[name] = meter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log_every</span>(<span class="params">self, iterable, print_freq, header=<span class="literal">None</span></span>):</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> header:</span><br><span class="line">            header = <span class="string">&quot;&quot;</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        end = time.time()</span><br><span class="line">        iter_time = SmoothedValue(fmt=<span class="string">&quot;&#123;avg:.4f&#125;&quot;</span>)</span><br><span class="line">        data_time = SmoothedValue(fmt=<span class="string">&quot;&#123;avg:.4f&#125;&quot;</span>)</span><br><span class="line">        space_fmt = <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="built_in">str</span>(<span class="built_in">len</span>(iterable)))) + <span class="string">&quot;d&quot;</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            log_msg = <span class="variable language_">self</span>.delimiter.join(</span><br><span class="line">                [</span><br><span class="line">                    header,</span><br><span class="line">                    <span class="string">&quot;[&#123;0&quot;</span> + space_fmt + <span class="string">&quot;&#125;/&#123;1&#125;]&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;eta: &#123;eta&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;&#123;meters&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;time: &#123;time&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;data: &#123;data&#125;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;max mem: &#123;memory:.0f&#125;&quot;</span>,</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            log_msg = <span class="variable language_">self</span>.delimiter.join(</span><br><span class="line">                [header, <span class="string">&quot;[&#123;0&quot;</span> + space_fmt + <span class="string">&quot;&#125;/&#123;1&#125;]&quot;</span>, <span class="string">&quot;eta: &#123;eta&#125;&quot;</span>, <span class="string">&quot;&#123;meters&#125;&quot;</span>, <span class="string">&quot;time: &#123;time&#125;&quot;</span>, <span class="string">&quot;data: &#123;data&#125;&quot;</span>]</span><br><span class="line">            )</span><br><span class="line">        MB = <span class="number">1024.0</span> * <span class="number">1024.0</span></span><br><span class="line">        <span class="keyword">for</span> obj <span class="keyword">in</span> iterable:</span><br><span class="line">            data_time.update(time.time() - end)</span><br><span class="line">            <span class="keyword">yield</span> obj</span><br><span class="line">            iter_time.update(time.time() - end)</span><br><span class="line">            <span class="keyword">if</span> i % print_freq == <span class="number">0</span>:</span><br><span class="line">                eta_seconds = iter_time.global_avg * (<span class="built_in">len</span>(iterable) - i)</span><br><span class="line">                eta_string = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(eta_seconds)))</span><br><span class="line">                <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        log_msg.<span class="built_in">format</span>(</span><br><span class="line">                            i,</span><br><span class="line">                            <span class="built_in">len</span>(iterable),</span><br><span class="line">                            eta=eta_string,</span><br><span class="line">                            meters=<span class="built_in">str</span>(<span class="variable language_">self</span>),</span><br><span class="line">                            time=<span class="built_in">str</span>(iter_time),</span><br><span class="line">                            data=<span class="built_in">str</span>(data_time),</span><br><span class="line">                            memory=torch.cuda.max_memory_allocated() / MB,</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        log_msg.<span class="built_in">format</span>(</span><br><span class="line">                            i, <span class="built_in">len</span>(iterable), eta=eta_string, meters=<span class="built_in">str</span>(<span class="variable language_">self</span>), time=<span class="built_in">str</span>(iter_time), data=<span class="built_in">str</span>(data_time)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            end = time.time()</span><br><span class="line">        total_time = time.time() - start_time</span><br><span class="line">        total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;header&#125;</span> Total time: <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExponentialMovingAverage</span>(torch.optim.swa_utils.AveragedModel):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Maintains moving averages of model parameters using an exponential decay.</span></span><br><span class="line"><span class="string">    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``</span></span><br><span class="line"><span class="string">    `torch.optim.swa_utils.AveragedModel &lt;https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies&gt;`_</span></span><br><span class="line"><span class="string">    is used to compute the EMA.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, decay, device=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">ema_avg</span>(<span class="params">avg_model_param, model_param, num_averaged</span>):</span><br><span class="line">            <span class="keyword">return</span> decay * avg_model_param + (<span class="number">1</span> - decay) * model_param</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__(model, device, ema_avg, use_buffers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, target, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the accuracy over the k top predictions for the specified values of k&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        maxk = <span class="built_in">max</span>(topk)</span><br><span class="line">        batch_size = target.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> target.ndim == <span class="number">2</span>:</span><br><span class="line">            target = target.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        _, pred = output.topk(maxk, <span class="number">1</span>, <span class="literal">True</span>, <span class="literal">True</span>)</span><br><span class="line">        pred = pred.t()</span><br><span class="line">        correct = pred.eq(target[<span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">            correct_k = correct[:k].flatten().<span class="built_in">sum</span>(dtype=torch.float32)</span><br><span class="line">            res.append(correct_k * (<span class="number">100.0</span> / batch_size))</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mkdir</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">    <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> e.errno != errno.EEXIST:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_for_distributed</span>(<span class="params">is_master</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function disables printing when not in master process</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">import</span> builtins <span class="keyword">as</span> __builtin__</span><br><span class="line"></span><br><span class="line">    builtin_print = __builtin__.<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">print</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        force = kwargs.pop(<span class="string">&quot;force&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> is_master <span class="keyword">or</span> force:</span><br><span class="line">            builtin_print(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    __builtin__.<span class="built_in">print</span> = <span class="built_in">print</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_dist_avail_and_initialized</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_available():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dist.is_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_world_size</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> dist.get_world_size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_rank</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dist.get_rank()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_main_process</span>():</span><br><span class="line">    <span class="keyword">return</span> get_rank() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_on_master</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    <span class="keyword">if</span> is_main_process():</span><br><span class="line">        torch.save(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_distributed_mode</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;RANK&quot;</span> <span class="keyword">in</span> os.environ <span class="keyword">and</span> <span class="string">&quot;WORLD_SIZE&quot;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])</span><br><span class="line">        args.world_size = <span class="built_in">int</span>(os.environ[<span class="string">&quot;WORLD_SIZE&quot;</span>])</span><br><span class="line">        args.gpu = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;SLURM_PROCID&quot;</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">        args.rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;SLURM_PROCID&quot;</span>])</span><br><span class="line">        args.gpu = args.rank % torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">hasattr</span>(args, <span class="string">&quot;rank&quot;</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Not using distributed mode&quot;</span>)</span><br><span class="line">        args.distributed = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    args.distributed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    torch.cuda.set_device(args.gpu)</span><br><span class="line">    args.dist_backend = <span class="string">&quot;nccl&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;| distributed init (rank <span class="subst">&#123;args.rank&#125;</span>): <span class="subst">&#123;args.dist_url&#125;</span>&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.distributed.init_process_group(</span><br><span class="line">        backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank</span><br><span class="line">    )</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    setup_for_distributed(args.rank == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">average_checkpoints</span>(<span class="params">inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads checkpoints from inputs and returns a model with averaged weights. Original implementation taken from:</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/fairseq/blob/a48f235636557b8d3bc4922a6fa90f3a0fa57955/scripts/average_checkpoints.py#L16</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      inputs (List[str]): An iterable of string paths of checkpoints to load from.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A dict of string keys mapping to various values. The &#x27;model&#x27; key</span></span><br><span class="line"><span class="string">      from the returned dict should correspond to an OrderedDict mapping</span></span><br><span class="line"><span class="string">      string parameter names to torch Tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    params_dict = OrderedDict()</span><br><span class="line">    params_keys = <span class="literal">None</span></span><br><span class="line">    new_state = <span class="literal">None</span></span><br><span class="line">    num_models = <span class="built_in">len</span>(inputs)</span><br><span class="line">    <span class="keyword">for</span> fpath <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            state = torch.load(</span><br><span class="line">                f,</span><br><span class="line">                map_location=(<span class="keyword">lambda</span> s, _: torch.serialization.default_restore_location(s, <span class="string">&quot;cpu&quot;</span>)),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># Copies over the settings from the first checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> new_state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            new_state = state</span><br><span class="line">        model_params = state[<span class="string">&quot;model&quot;</span>]</span><br><span class="line">        model_params_keys = <span class="built_in">list</span>(model_params.keys())</span><br><span class="line">        <span class="keyword">if</span> params_keys <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            params_keys = model_params_keys</span><br><span class="line">        <span class="keyword">elif</span> params_keys != model_params_keys:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">f&quot;For checkpoint <span class="subst">&#123;f&#125;</span>, expected list of params: <span class="subst">&#123;params_keys&#125;</span>, but found: <span class="subst">&#123;model_params_keys&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> params_keys:</span><br><span class="line">            p = model_params[k]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(p, torch.HalfTensor):</span><br><span class="line">                p = p.<span class="built_in">float</span>()</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> params_dict:</span><br><span class="line">                params_dict[k] = p.clone()</span><br><span class="line">                <span class="comment"># <span class="doctag">NOTE:</span> clone() is needed in case of p is a shared parameter</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                params_dict[k] += p</span><br><span class="line">    averaged_params = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> params_dict.items():</span><br><span class="line">        averaged_params[k] = v</span><br><span class="line">        <span class="keyword">if</span> averaged_params[k].is_floating_point():</span><br><span class="line">            averaged_params[k].div_(num_models)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            averaged_params[k] //= num_models</span><br><span class="line">    new_state[<span class="string">&quot;model&quot;</span>] = averaged_params</span><br><span class="line">    <span class="keyword">return</span> new_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">store_model_weights</span>(<span class="params">model, checkpoint_path, checkpoint_key=<span class="string">&quot;model&quot;</span>, strict=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This method can be used to prepare weights files for new models. It receives as</span></span><br><span class="line"><span class="string">    input a model architecture and a checkpoint from the training script and produces</span></span><br><span class="line"><span class="string">    a file with the weights ready for release.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples:</span></span><br><span class="line"><span class="string">        from torchvision import models as M</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Classification</span></span><br><span class="line"><span class="string">        model = M.mobilenet_v3_large(weights=None)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./class.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Quantized Classification</span></span><br><span class="line"><span class="string">        model = M.quantization.mobilenet_v3_large(weights=None, quantize=False)</span></span><br><span class="line"><span class="string">        model.fuse_model(is_qat=True)</span></span><br><span class="line"><span class="string">        model.qconfig = torch.ao.quantization.get_default_qat_qconfig(&#x27;qnnpack&#x27;)</span></span><br><span class="line"><span class="string">        _ = torch.ao.quantization.prepare_qat(model, inplace=True)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./qat.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Object Detection</span></span><br><span class="line"><span class="string">        model = M.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=None, weights_backbone=None)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./obj.pth&#x27;))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # Segmentation</span></span><br><span class="line"><span class="string">        model = M.segmentation.deeplabv3_mobilenet_v3_large(weights=None, weights_backbone=None, aux_loss=True)</span></span><br><span class="line"><span class="string">        print(store_model_weights(model, &#x27;./segm.pth&#x27;, strict=False))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model (pytorch.nn.Module): The model on which the weights will be loaded for validation purposes.</span></span><br><span class="line"><span class="string">        checkpoint_path (str): The path of the checkpoint we will load.</span></span><br><span class="line"><span class="string">        checkpoint_key (str, optional): The key of the checkpoint where the model weights are stored.</span></span><br><span class="line"><span class="string">            Default: &quot;model&quot;.</span></span><br><span class="line"><span class="string">        strict (bool): whether to strictly enforce that the keys</span></span><br><span class="line"><span class="string">            in :attr:`state_dict` match the keys returned by this module&#x27;s</span></span><br><span class="line"><span class="string">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        output_path (str): The location where the weights are saved.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Store the new model next to the checkpoint_path</span></span><br><span class="line">    checkpoint_path = os.path.abspath(checkpoint_path)</span><br><span class="line">    output_dir = os.path.dirname(checkpoint_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Deep copy to avoid side-effects on the model object.</span></span><br><span class="line">    model = copy.deepcopy(model)</span><br><span class="line">    checkpoint = torch.load(checkpoint_path, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the weights to the model to validate that everything works</span></span><br><span class="line">    <span class="comment"># and remove unnecessary weights (such as auxiliaries, etc)</span></span><br><span class="line">    <span class="keyword">if</span> checkpoint_key == <span class="string">&quot;model_ema&quot;</span>:</span><br><span class="line">        <span class="keyword">del</span> checkpoint[checkpoint_key][<span class="string">&quot;n_averaged&quot;</span>]</span><br><span class="line">        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(checkpoint[checkpoint_key], <span class="string">&quot;module.&quot;</span>)</span><br><span class="line">    model.load_state_dict(checkpoint[checkpoint_key], strict=strict)</span><br><span class="line"></span><br><span class="line">    tmp_path = os.path.join(output_dir, <span class="built_in">str</span>(model.__hash__()))</span><br><span class="line">    torch.save(model.state_dict(), tmp_path)</span><br><span class="line"></span><br><span class="line">    sha256_hash = hashlib.sha256()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(tmp_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># Read and update hash string value in blocks of 4K</span></span><br><span class="line">        <span class="keyword">for</span> byte_block <span class="keyword">in</span> <span class="built_in">iter</span>(<span class="keyword">lambda</span>: f.read(<span class="number">4096</span>), <span class="string">b&quot;&quot;</span>):</span><br><span class="line">            sha256_hash.update(byte_block)</span><br><span class="line">        hh = sha256_hash.hexdigest()</span><br><span class="line"></span><br><span class="line">    output_path = os.path.join(output_dir, <span class="string">&quot;weights-&quot;</span> + <span class="built_in">str</span>(hh[:<span class="number">8</span>]) + <span class="string">&quot;.pth&quot;</span>)</span><br><span class="line">    os.replace(tmp_path, output_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_across_processes</span>(<span class="params">val</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_dist_avail_and_initialized():</span><br><span class="line">        <span class="comment"># nothing to sync, but we still convert to tensor for consistency with the distributed case.</span></span><br><span class="line">        <span class="keyword">return</span> torch.tensor(val)</span><br><span class="line"></span><br><span class="line">    t = torch.tensor(val, device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    dist.barrier()</span><br><span class="line">    dist.all_reduce(t)</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_weight_decay</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model: torch.nn.Module,</span></span><br><span class="line"><span class="params">    weight_decay: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    norm_weight_decay: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    norm_classes: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">type</span>]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    custom_keys_weight_decay: <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">str</span>, <span class="built_in">float</span>]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> norm_classes:</span><br><span class="line">        norm_classes = [</span><br><span class="line">            torch.nn.modules.batchnorm._BatchNorm,</span><br><span class="line">            torch.nn.LayerNorm,</span><br><span class="line">            torch.nn.GroupNorm,</span><br><span class="line">            torch.nn.modules.instancenorm._InstanceNorm,</span><br><span class="line">            torch.nn.LocalResponseNorm,</span><br><span class="line">        ]</span><br><span class="line">    norm_classes = <span class="built_in">tuple</span>(norm_classes)</span><br><span class="line"></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;norm&quot;</span>: [],</span><br><span class="line">    &#125;</span><br><span class="line">    params_weight_decay = &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: weight_decay,</span><br><span class="line">        <span class="string">&quot;norm&quot;</span>: norm_weight_decay,</span><br><span class="line">    &#125;</span><br><span class="line">    custom_keys = []</span><br><span class="line">    <span class="keyword">if</span> custom_keys_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> key, weight_decay <span class="keyword">in</span> custom_keys_weight_decay:</span><br><span class="line">            params[key] = []</span><br><span class="line">            params_weight_decay[key] = weight_decay</span><br><span class="line">            custom_keys.append(key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_params</span>(<span class="params">module, prefix=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, p <span class="keyword">in</span> module.named_parameters(recurse=<span class="literal">False</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> p.requires_grad:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            is_custom_key = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> custom_keys:</span><br><span class="line">                target_name = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;name&#125;</span>&quot;</span> <span class="keyword">if</span> prefix != <span class="string">&quot;&quot;</span> <span class="keyword">and</span> <span class="string">&quot;.&quot;</span> <span class="keyword">in</span> key <span class="keyword">else</span> name</span><br><span class="line">                <span class="keyword">if</span> key == target_name:</span><br><span class="line">                    params[key].append(p)</span><br><span class="line">                    is_custom_key = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_custom_key:</span><br><span class="line">                <span class="keyword">if</span> norm_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(module, norm_classes):</span><br><span class="line">                    params[<span class="string">&quot;norm&quot;</span>].append(p)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    params[<span class="string">&quot;other&quot;</span>].append(p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> child_name, child_module <span class="keyword">in</span> module.named_children():</span><br><span class="line">            child_prefix = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;child_name&#125;</span>&quot;</span> <span class="keyword">if</span> prefix != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> child_name</span><br><span class="line">            _add_params(child_module, prefix=child_prefix)</span><br><span class="line"></span><br><span class="line">    _add_params(model)</span><br><span class="line"></span><br><span class="line">    param_groups = []</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> params:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(params[key]) &gt; <span class="number">0</span>:</span><br><span class="line">            param_groups.append(&#123;<span class="string">&quot;params&quot;</span>: params[key], <span class="string">&quot;weight_decay&quot;</span>: params_weight_decay[key]&#125;)</span><br><span class="line">    <span class="keyword">return</span> param_groups</span><br></pre></td></tr></table></figure>
<p>训练和评估函数，本次分类没有用官方给定的主函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> default_collate</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> InterpolationMode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model, criterion, optimizer, data_loader, device, epoch, print_freq=<span class="number">10</span>, model_ema=<span class="literal">None</span>, scaler=<span class="literal">None</span>, clip_grad_norm=<span class="literal">None</span>, lr_warmup_epochs=<span class="number">10</span>, model_ema_steps=<span class="number">5</span></span>):</span><br><span class="line">    model.train()</span><br><span class="line">    metric_logger = MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    metric_logger.add_meter(<span class="string">&quot;lr&quot;</span>, SmoothedValue(window_size=<span class="number">1</span>, fmt=<span class="string">&quot;&#123;value&#125;&quot;</span>))</span><br><span class="line">    metric_logger.add_meter(<span class="string">&quot;img/s&quot;</span>, SmoothedValue(window_size=<span class="number">10</span>, fmt=<span class="string">&quot;&#123;value&#125;&quot;</span>))</span><br><span class="line"></span><br><span class="line">    header = <span class="string">f&quot;Epoch: [<span class="subst">&#123;epoch&#125;</span>]&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i, (image, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(metric_logger.log_every(data_loader, print_freq, header)):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        image, target = image.to(device), target.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.cuda.amp.autocast(enabled=scaler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">            output = model(image)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">if</span> scaler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scaler.scale(loss).backward()</span><br><span class="line">            <span class="keyword">if</span> clip_grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># we should unscale the gradients of optimizer&#x27;s assigned params if do gradient clipping</span></span><br><span class="line">                scaler.unscale_(optimizer)</span><br><span class="line">                nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)</span><br><span class="line">            scaler.step(optimizer)</span><br><span class="line">            scaler.update()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">if</span> clip_grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> model_ema <span class="keyword">and</span> i % model_ema_steps == <span class="number">0</span>:</span><br><span class="line">            model_ema.update_parameters(model)</span><br><span class="line">            <span class="keyword">if</span> epoch &lt; lr_warmup_epochs:</span><br><span class="line">                <span class="comment"># Reset ema buffer to keep copying weights during warmup period</span></span><br><span class="line">                model_ema.n_averaged.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">        batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>])</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;acc1&quot;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;acc5&quot;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">        metric_logger.meters[<span class="string">&quot;img/s&quot;</span>].update(batch_size / (time.time() - start_time))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, criterion, data_loader, device, print_freq=<span class="number">100</span>, log_suffix=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    metric_logger = MetricLogger(delimiter=<span class="string">&quot;  &quot;</span>)</span><br><span class="line">    header = <span class="string">f&quot;Test: <span class="subst">&#123;log_suffix&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    num_processed_samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        <span class="keyword">for</span> image, target <span class="keyword">in</span> metric_logger.log_every(data_loader, print_freq, header):</span><br><span class="line">            image = image.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            target = target.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line">            output = model(image)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">            acc1, acc5 = accuracy(output, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">            <span class="comment"># FIXME need to take into account that the datasets</span></span><br><span class="line">            <span class="comment"># could have been padded in distributed setup</span></span><br><span class="line">            batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">            metric_logger.update(loss=loss.item())</span><br><span class="line">            metric_logger.meters[<span class="string">&quot;acc1&quot;</span>].update(acc1.item(), n=batch_size)</span><br><span class="line">            metric_logger.meters[<span class="string">&quot;acc5&quot;</span>].update(acc5.item(), n=batch_size)</span><br><span class="line">            num_processed_samples += batch_size</span><br><span class="line">    <span class="comment"># gather the stats from all processes</span></span><br><span class="line"></span><br><span class="line">    num_processed_samples = reduce_across_processes(num_processed_samples)</span><br><span class="line">    <span class="keyword">if</span> (</span><br><span class="line">        <span class="built_in">hasattr</span>(data_loader.dataset, <span class="string">&quot;__len__&quot;</span>)</span><br><span class="line">        <span class="keyword">and</span> <span class="built_in">len</span>(data_loader.dataset) != num_processed_samples</span><br><span class="line">        <span class="keyword">and</span> torch.distributed.get_rank() == <span class="number">0</span></span><br><span class="line">    ):</span><br><span class="line">        <span class="comment"># See FIXME above</span></span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">f&quot;It looks like the dataset has <span class="subst">&#123;<span class="built_in">len</span>(data_loader.dataset)&#125;</span> samples, but <span class="subst">&#123;num_processed_samples&#125;</span> &quot;</span></span><br><span class="line">            <span class="string">&quot;samples were used for the validation, which might bias the results. &quot;</span></span><br><span class="line">            <span class="string">&quot;Try adjusting the batch size and / or the world size. &quot;</span></span><br><span class="line">            <span class="string">&quot;Setting the world size to 1 is always a safe bet.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    metric_logger.synchronize_between_processes()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;header&#125;</span> Acc@1 <span class="subst">&#123;metric_logger.acc1.global_avg:<span class="number">.3</span>f&#125;</span> Acc@5 <span class="subst">&#123;metric_logger.acc5.global_avg:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> metric_logger.acc1.global_avg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_cache_path</span>(<span class="params">filepath</span>):</span><br><span class="line">    <span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line">    h = hashlib.sha1(filepath.encode()).hexdigest()</span><br><span class="line">    cache_path = os.path.join(<span class="string">&quot;~&quot;</span>, <span class="string">&quot;.torch&quot;</span>, <span class="string">&quot;vision&quot;</span>, <span class="string">&quot;datasets&quot;</span>, <span class="string">&quot;imagefolder&quot;</span>, h[:<span class="number">10</span>] + <span class="string">&quot;.pt&quot;</span>)</span><br><span class="line">    cache_path = os.path.expanduser(cache_path)</span><br><span class="line">    <span class="keyword">return</span> cache_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">traindir, valdir, args</span>):</span><br><span class="line">    <span class="comment"># Data loading code</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading data&quot;</span>)</span><br><span class="line">    val_resize_size, val_crop_size, train_crop_size = (</span><br><span class="line">        args.val_resize_size,</span><br><span class="line">        args.val_crop_size,</span><br><span class="line">        args.train_crop_size,</span><br><span class="line">    )</span><br><span class="line">    interpolation = InterpolationMode(args.interpolation)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading training data&quot;</span>)</span><br><span class="line">    st = time.time()</span><br><span class="line">    cache_path = _get_cache_path(traindir)</span><br><span class="line">    <span class="keyword">if</span> args.cache_dataset <span class="keyword">and</span> os.path.exists(cache_path):</span><br><span class="line">        <span class="comment"># Attention, as the transforms are also cached!</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loading dataset_train from <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">        dataset, _ = torch.load(cache_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        auto_augment_policy = <span class="built_in">getattr</span>(args, <span class="string">&quot;auto_augment&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        random_erase_prob = <span class="built_in">getattr</span>(args, <span class="string">&quot;random_erase&quot;</span>, <span class="number">0.0</span>)</span><br><span class="line">        ra_magnitude = args.ra_magnitude</span><br><span class="line">        augmix_severity = args.augmix_severity</span><br><span class="line">        dataset = torchvision.datasets.ImageFolder(</span><br><span class="line">            traindir,</span><br><span class="line">            ClassificationPresetTrain(</span><br><span class="line">                crop_size=train_crop_size,</span><br><span class="line">                interpolation=interpolation,</span><br><span class="line">                auto_augment_policy=auto_augment_policy,</span><br><span class="line">                random_erase_prob=random_erase_prob,</span><br><span class="line">                ra_magnitude=ra_magnitude,</span><br><span class="line">                augmix_severity=augmix_severity,</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> args.cache_dataset:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Saving dataset_train to <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">            mkdir(os.path.dirname(cache_path))</span><br><span class="line">            save_on_master((dataset, traindir), cache_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Took&quot;</span>, time.time() - st)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading validation data&quot;</span>)</span><br><span class="line">    cache_path = _get_cache_path(valdir)</span><br><span class="line">    <span class="keyword">if</span> args.cache_dataset <span class="keyword">and</span> os.path.exists(cache_path):</span><br><span class="line">        <span class="comment"># Attention, as the transforms are also cached!</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Loading dataset_test from <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">        dataset_test, _ = torch.load(cache_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> args.weights <span class="keyword">and</span> args.test_only:</span><br><span class="line">            weights = torchvision.models.get_weight(args.weights)</span><br><span class="line">            preprocessing = weights.transforms()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            preprocessing = ClassificationPresetEval(</span><br><span class="line">                crop_size=val_crop_size, resize_size=val_resize_size, interpolation=interpolation</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        dataset_test = torchvision.datasets.ImageFolder(</span><br><span class="line">            valdir,</span><br><span class="line">            preprocessing,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> args.cache_dataset:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Saving dataset_test to <span class="subst">&#123;cache_path&#125;</span>&quot;</span>)</span><br><span class="line">            mkdir(os.path.dirname(cache_path))</span><br><span class="line">            save_on_master((dataset_test, valdir), cache_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Creating data loaders&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(args, <span class="string">&quot;ra_sampler&quot;</span>) <span class="keyword">and</span> args.ra_sampler:</span><br><span class="line">            train_sampler = RASampler(dataset, shuffle=<span class="literal">True</span>, repetitions=args.ra_reps)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)</span><br><span class="line">        test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_sampler = torch.utils.data.RandomSampler(dataset)</span><br><span class="line">        test_sampler = torch.utils.data.SequentialSampler(dataset_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset, dataset_test, train_sampler, test_sampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">if</span> args.output_dir:</span><br><span class="line">        mkdir(args.output_dir)</span><br><span class="line"></span><br><span class="line">    init_distributed_mode(args)</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">    device = torch.device(args.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_deterministic_algorithms:</span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">        torch.use_deterministic_algorithms(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    train_dir = os.path.join(args.data_path, <span class="string">&quot;train&quot;</span>)</span><br><span class="line">    val_dir = os.path.join(args.data_path, <span class="string">&quot;val&quot;</span>)</span><br><span class="line">    dataset, dataset_test, train_sampler, test_sampler = load_data(train_dir, val_dir, args)</span><br><span class="line"></span><br><span class="line">    collate_fn = <span class="literal">None</span></span><br><span class="line">    num_classes = <span class="built_in">len</span>(dataset.classes)</span><br><span class="line">    mixup_transforms = []</span><br><span class="line">    <span class="keyword">if</span> args.mixup_alpha &gt; <span class="number">0.0</span>:</span><br><span class="line">        mixup_transforms.append(RandomMixup(num_classes, p=<span class="number">1.0</span>, alpha=args.mixup_alpha))</span><br><span class="line">    <span class="keyword">if</span> args.cutmix_alpha &gt; <span class="number">0.0</span>:</span><br><span class="line">        mixup_transforms.append(RandomCutmix(num_classes, p=<span class="number">1.0</span>, alpha=args.cutmix_alpha))</span><br><span class="line">    <span class="keyword">if</span> mixup_transforms:</span><br><span class="line">        mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">            <span class="keyword">return</span> mixupcutmix(*default_collate(batch))</span><br><span class="line"></span><br><span class="line">    data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        num_workers=args.workers,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">        dataset_test, batch_size=args.batch_size, sampler=test_sampler, num_workers=args.workers, pin_memory=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Creating model&quot;</span>)</span><br><span class="line">    model = torchvision.models.get_model(args.model, weights=args.weights, num_classes=num_classes)</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.distributed <span class="keyword">and</span> args.sync_bn:</span><br><span class="line">        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)</span><br><span class="line"></span><br><span class="line">    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)</span><br><span class="line"></span><br><span class="line">    custom_keys_weight_decay = []</span><br><span class="line">    <span class="keyword">if</span> args.bias_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        custom_keys_weight_decay.append((<span class="string">&quot;bias&quot;</span>, args.bias_weight_decay))</span><br><span class="line">    <span class="keyword">if</span> args.transformer_embedding_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">&quot;class_token&quot;</span>, <span class="string">&quot;position_embedding&quot;</span>, <span class="string">&quot;relative_position_bias_table&quot;</span>]:</span><br><span class="line">            custom_keys_weight_decay.append((key, args.transformer_embedding_decay))</span><br><span class="line">    parameters = set_weight_decay(</span><br><span class="line">        model,</span><br><span class="line">        args.weight_decay,</span><br><span class="line">        norm_weight_decay=args.norm_weight_decay,</span><br><span class="line">        custom_keys_weight_decay=custom_keys_weight_decay <span class="keyword">if</span> <span class="built_in">len</span>(custom_keys_weight_decay) &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    opt_name = args.opt.lower()</span><br><span class="line">    <span class="keyword">if</span> opt_name.startswith(<span class="string">&quot;sgd&quot;</span>):</span><br><span class="line">        optimizer = torch.optim.SGD(</span><br><span class="line">            parameters,</span><br><span class="line">            lr=args.lr,</span><br><span class="line">            momentum=args.momentum,</span><br><span class="line">            weight_decay=args.weight_decay,</span><br><span class="line">            nesterov=<span class="string">&quot;nesterov&quot;</span> <span class="keyword">in</span> opt_name,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> opt_name == <span class="string">&quot;rmsprop&quot;</span>:</span><br><span class="line">        optimizer = torch.optim.RMSprop(</span><br><span class="line">            parameters, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, eps=<span class="number">0.0316</span>, alpha=<span class="number">0.9</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> opt_name == <span class="string">&quot;adamw&quot;</span>:</span><br><span class="line">        optimizer = torch.optim.AdamW(parameters, lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Invalid optimizer <span class="subst">&#123;args.opt&#125;</span>. Only SGD, RMSprop and AdamW are supported.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    scaler = torch.cuda.amp.GradScaler() <span class="keyword">if</span> args.amp <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.lr_scheduler = args.lr_scheduler.lower()</span><br><span class="line">    <span class="keyword">if</span> args.lr_scheduler == <span class="string">&quot;steplr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)</span><br><span class="line">    <span class="keyword">elif</span> args.lr_scheduler == <span class="string">&quot;cosineannealinglr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(</span><br><span class="line">            optimizer, T_max=args.epochs - args.lr_warmup_epochs, eta_min=args.lr_min</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> args.lr_scheduler == <span class="string">&quot;exponentiallr&quot;</span>:</span><br><span class="line">        main_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_gamma)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">            <span class="string">f&quot;Invalid lr scheduler &#x27;<span class="subst">&#123;args.lr_scheduler&#125;</span>&#x27;. Only StepLR, CosineAnnealingLR and ExponentialLR &quot;</span></span><br><span class="line">            <span class="string">&quot;are supported.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.lr_warmup_epochs &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> args.lr_warmup_method == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">            warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(</span><br><span class="line">                optimizer, start_factor=args.lr_warmup_decay, total_iters=args.lr_warmup_epochs</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> args.lr_warmup_method == <span class="string">&quot;constant&quot;</span>:</span><br><span class="line">            warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(</span><br><span class="line">                optimizer, factor=args.lr_warmup_decay, total_iters=args.lr_warmup_epochs</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">                <span class="string">f&quot;Invalid warmup lr method &#x27;<span class="subst">&#123;args.lr_warmup_method&#125;</span>&#x27;. Only linear and constant are supported.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(</span><br><span class="line">            optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[args.lr_warmup_epochs]</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        lr_scheduler = main_lr_scheduler</span><br><span class="line"></span><br><span class="line">    model_without_ddp = model</span><br><span class="line">    <span class="keyword">if</span> args.distributed:</span><br><span class="line">        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</span><br><span class="line">        model_without_ddp = model.module</span><br><span class="line"></span><br><span class="line">    model_ema = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> args.model_ema:</span><br><span class="line">        <span class="comment"># Decay adjustment that aims to keep the decay independent from other hyper-parameters originally proposed at:</span></span><br><span class="line">        <span class="comment"># https://github.com/facebookresearch/pycls/blob/f8cd9627/pycls/core/net.py#L123</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># total_ema_updates = (Dataset_size / n_GPUs) * epochs / (batch_size_per_gpu * EMA_steps)</span></span><br><span class="line">        <span class="comment"># We consider constant = Dataset_size for a given dataset/setup and ommit it. Thus:</span></span><br><span class="line">        <span class="comment"># adjust = 1 / total_ema_updates ~= n_GPUs * batch_size_per_gpu * EMA_steps / epochs</span></span><br><span class="line">        adjust = args.world_size * args.batch_size * args.model_ema_steps / args.epochs</span><br><span class="line">        alpha = <span class="number">1.0</span> - args.model_ema_decay</span><br><span class="line">        alpha = <span class="built_in">min</span>(<span class="number">1.0</span>, alpha * adjust)</span><br><span class="line">        model_ema = ExponentialMovingAverage(model_without_ddp, device=device, decay=<span class="number">1.0</span> - alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.resume:</span><br><span class="line">        checkpoint = torch.load(args.resume, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        model_without_ddp.load_state_dict(checkpoint[<span class="string">&quot;model&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args.test_only:</span><br><span class="line">            optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">            lr_scheduler.load_state_dict(checkpoint[<span class="string">&quot;lr_scheduler&quot;</span>])</span><br><span class="line">        args.start_epoch = checkpoint[<span class="string">&quot;epoch&quot;</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            model_ema.load_state_dict(checkpoint[<span class="string">&quot;model_ema&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> scaler:</span><br><span class="line">            scaler.load_state_dict(checkpoint[<span class="string">&quot;scaler&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.test_only:</span><br><span class="line">        <span class="comment"># We disable the cudnn benchmarking because it can noticeably affect the accuracy</span></span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            evaluate(model_ema, criterion, data_loader_test, device=device, log_suffix=<span class="string">&quot;EMA&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            evaluate(model, criterion, data_loader_test, device=device)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start training&quot;</span>)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.epochs):</span><br><span class="line">        <span class="keyword">if</span> args.distributed:</span><br><span class="line">            train_sampler.set_epoch(epoch)</span><br><span class="line">        train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema, scaler)</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        evaluate(model, criterion, data_loader_test, device=device)</span><br><span class="line">        <span class="keyword">if</span> model_ema:</span><br><span class="line">            evaluate(model_ema, criterion, data_loader_test, device=device, log_suffix=<span class="string">&quot;EMA&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> args.output_dir:</span><br><span class="line">            checkpoint = &#123;</span><br><span class="line">                <span class="string">&quot;model&quot;</span>: model_without_ddp.state_dict(),</span><br><span class="line">                <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict(),</span><br><span class="line">                <span class="string">&quot;lr_scheduler&quot;</span>: lr_scheduler.state_dict(),</span><br><span class="line">                <span class="string">&quot;epoch&quot;</span>: epoch,</span><br><span class="line">                <span class="string">&quot;args&quot;</span>: args,</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> model_ema:</span><br><span class="line">                checkpoint[<span class="string">&quot;model_ema&quot;</span>] = model_ema.state_dict()</span><br><span class="line">            <span class="keyword">if</span> scaler:</span><br><span class="line">                checkpoint[<span class="string">&quot;scaler&quot;</span>] = scaler.state_dict()</span><br><span class="line">            save_on_master(checkpoint, os.path.join(args.output_dir, <span class="string">f&quot;model_<span class="subst">&#123;epoch&#125;</span>.pth&quot;</span>))</span><br><span class="line">            save_on_master(checkpoint, os.path.join(args.output_dir, <span class="string">&quot;checkpoint.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line">    total_time = time.time() - start_time</span><br><span class="line">    total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training time <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练和评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> Compose, ToTensor, PILToTensor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data_train = MNIST(<span class="string">&#x27;.data/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=ToTensor())</span><br><span class="line">data_val = MNIST(<span class="string">&#x27;./data/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=ToTensor())</span><br><span class="line">num_classes = <span class="built_in">len</span>(data_train.classes)</span><br><span class="line">sampler_train = torch.utils.data.RandomSampler(data_train, replacement=<span class="literal">False</span>)</span><br><span class="line">data_loader_train = torch.utils.data.DataLoader(</span><br><span class="line">    data_train, sampler=sampler_train,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>,</span><br><span class="line">    pin_memory=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">True</span>,  <span class="comment"># 当不足一个batch时候，使丢弃后面一部分还是随机增加一部分样本</span></span><br><span class="line">)</span><br><span class="line">sampler_val = torch.utils.data.SequentialSampler(data_val)</span><br><span class="line">data_loader_val = torch.utils.data.DataLoader(</span><br><span class="line">    data_val, sampler=sampler_val,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>,</span><br><span class="line">    pin_memory=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># timm库里面自带的mixup，0.8的MIXUP和0.8的CUTMIX，此处没用到</span></span><br><span class="line">mixup_fn = Mixup(</span><br><span class="line">    <span class="comment"># mixup=0.8,cutmix=0.8,cutmix_minmax is None,mixup_prob=1, mixup_switch_prob=0.5, mixup_mode=&#x27;batch&#x27;, label_smoothing=0.1</span></span><br><span class="line">    mixup_alpha=<span class="number">0.8</span>, cutmix_alpha=<span class="number">0.8</span>, prob=<span class="number">0.8</span>, switch_prob=<span class="number">0.5</span>, mode=<span class="string">&#x27;batch&#x27;</span>,</span><br><span class="line">    label_smoothing=<span class="number">0.1</span>, num_classes=num_classes)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model = SwinTransformer(img_size=<span class="number">28</span>, patch_size=<span class="number">7</span>, in_chans=<span class="number">1</span>, num_classes=<span class="number">10</span>, embed_dim=<span class="number">96</span>, depths=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>], num_heads=[<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>], window_size=<span class="number">2</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">True</span>, drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.</span>, ape=<span class="literal">False</span>, patch_norm=<span class="literal">True</span>, use_checkpoint=<span class="literal">False</span>, fused_window_process=<span class="literal">False</span>).to(device)</span><br><span class="line">optimizer = Nadam(model.parameters())</span><br><span class="line">loss_fn = LabelSmoothingCrossEntropy()</span><br><span class="line">lr_scheduler = CosineLRScheduler(optimizer, t_initial=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start training&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train_one_epoch(model, loss_fn, optimizer, data_loader_train, device, epoch)</span><br><span class="line">    lr_scheduler.step(epoch)</span><br><span class="line">    evaluate(model, loss_fn, data_loader_val, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">total_time = time.time() - start_time</span><br><span class="line">total_time_str = <span class="built_in">str</span>(datetime.timedelta(seconds=<span class="built_in">int</span>(total_time)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training time <span class="subst">&#123;total_time_str&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br></pre></td><td class="code"><pre><span class="line">Start training</span><br><span class="line">Epoch: [0]  [  0/468]  eta: 0:53:11  lr: 0.002  img/s: 19.040995153557684  loss: 2.3678 (2.3678)  acc1: 8.5938 (8.5938)  acc5: 49.2188 (49.2188)  time: 6.8184  data: 0.0960  max mem: 138</span><br><span class="line">Epoch: [0]  [ 10/468]  eta: 0:05:01  lr: 0.002  img/s: 3501.2711431105545  loss: 2.6789 (2.8085)  acc1: 11.7188 (12.9972)  acc5: 52.3438 (52.9830)  time: 0.6580  data: 0.0090  max mem: 190</span><br><span class="line">Epoch: [0]  [ 20/468]  eta: 0:02:43  lr: 0.002  img/s: 3585.1146043405674  loss: 2.4055 (2.5948)  acc1: 11.7188 (12.7232)  acc5: 53.9062 (56.5848)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 30/468]  eta: 0:01:53  lr: 0.002  img/s: 3649.6010441592343  loss: 2.2687 (2.4918)  acc1: 12.5000 (13.7853)  acc5: 64.0625 (59.5010)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 40/468]  eta: 0:01:27  lr: 0.002  img/s: 3546.464652336473  loss: 2.2370 (2.4254)  acc1: 17.1875 (14.8438)  acc5: 67.1875 (61.1852)  time: 0.0361  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 50/468]  eta: 0:01:11  lr: 0.002  img/s: 3591.8786094682473  loss: 2.2039 (2.3840)  acc1: 17.1875 (15.4105)  acc5: 67.1875 (62.3775)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 60/468]  eta: 0:01:00  lr: 0.002  img/s: 3516.176414340542  loss: 2.0975 (2.3266)  acc1: 21.0938 (17.5717)  acc5: 74.2188 (65.4201)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 70/468]  eta: 0:00:53  lr: 0.002  img/s: 3595.655457400995  loss: 1.9138 (2.2534)  acc1: 32.8125 (20.4005)  acc5: 88.2812 (69.0361)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [ 80/468]  eta: 0:00:47  lr: 0.002  img/s: 3523.9311585165738  loss: 1.6661 (2.1731)  acc1: 46.0938 (24.2091)  acc5: 92.9688 (72.1065)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [ 90/468]  eta: 0:00:42  lr: 0.002  img/s: 3560.978423374125  loss: 1.5143 (2.0945)  acc1: 54.6875 (27.6700)  acc5: 94.5312 (74.6738)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [100/468]  eta: 0:00:38  lr: 0.002  img/s: 3397.5516052070348  loss: 1.3297 (2.0148)  acc1: 61.7188 (31.3738)  acc5: 96.8750 (76.8487)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [110/468]  eta: 0:00:35  lr: 0.002  img/s: 3430.11246062728  loss: 1.2297 (1.9384)  acc1: 65.6250 (34.9733)  acc5: 97.6562 (78.7796)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [120/468]  eta: 0:00:32  lr: 0.002  img/s: 3561.07290346973  loss: 1.1007 (1.8659)  acc1: 75.0000 (38.4362)  acc5: 98.4375 (80.3525)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [130/468]  eta: 0:00:30  lr: 0.002  img/s: 3507.3555366825635  loss: 0.9622 (1.7933)  acc1: 80.4688 (41.7820)  acc5: 99.2188 (81.7987)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [140/468]  eta: 0:00:27  lr: 0.002  img/s: 3631.1864186675684  loss: 0.9119 (1.7340)  acc1: 82.0312 (44.5867)  acc5: 99.2188 (83.0286)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [150/468]  eta: 0:00:26  lr: 0.002  img/s: 3552.660252253206  loss: 0.8685 (1.6751)  acc1: 83.5938 (47.2941)  acc5: 99.2188 (84.1060)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [160/468]  eta: 0:00:24  lr: 0.002  img/s: 3542.01905365107  loss: 0.8460 (1.6237)  acc1: 86.7188 (49.7234)  acc5: 99.2188 (85.0398)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [170/468]  eta: 0:00:22  lr: 0.002  img/s: 3549.3250826391645  loss: 0.8027 (1.5745)  acc1: 89.0625 (52.0514)  acc5: 99.2188 (85.8872)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [180/468]  eta: 0:00:21  lr: 0.002  img/s: 3582.865593550626  loss: 0.7574 (1.5300)  acc1: 89.0625 (54.1523)  acc5: 99.2188 (86.6238)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [190/468]  eta: 0:00:20  lr: 0.002  img/s: 3565.306025952637  loss: 0.7399 (1.4878)  acc1: 89.8438 (56.1109)  acc5: 99.2188 (87.3037)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [200/468]  eta: 0:00:18  lr: 0.002  img/s: 3605.6771975069846  loss: 0.7423 (1.4527)  acc1: 89.8438 (57.7231)  acc5: 99.2188 (87.9120)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [210/468]  eta: 0:00:17  lr: 0.002  img/s: 3401.2969342954707  loss: 0.7311 (1.4173)  acc1: 90.6250 (59.3676)  acc5: 99.2188 (88.4590)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [220/468]  eta: 0:00:16  lr: 0.002  img/s: 3480.7953422632554  loss: 0.7156 (1.3896)  acc1: 91.4062 (60.6582)  acc5: 99.2188 (88.9352)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [230/468]  eta: 0:00:15  lr: 0.002  img/s: 3562.7034746370077  loss: 0.7577 (1.3609)  acc1: 89.8438 (61.9690)  acc5: 99.2188 (89.3939)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [240/468]  eta: 0:00:14  lr: 0.002  img/s: 3469.951602895553  loss: 0.6970 (1.3335)  acc1: 92.1875 (63.1905)  acc5: 100.0000 (89.8178)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [250/468]  eta: 0:00:14  lr: 0.002  img/s: 2312.135436719682  loss: 0.6869 (1.3075)  acc1: 92.9688 (64.3800)  acc5: 100.0000 (90.2141)  time: 0.0424  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [0]  [260/468]  eta: 0:00:13  lr: 0.002  img/s: 3446.49529764465  loss: 0.6936 (1.2856)  acc1: 92.9688 (65.3766)  acc5: 100.0000 (90.5741)  time: 0.0492  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [0]  [270/468]  eta: 0:00:12  lr: 0.002  img/s: 3455.8797038944317  loss: 0.6986 (1.2633)  acc1: 91.4062 (66.3947)  acc5: 100.0000 (90.9162)  time: 0.0451  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [0]  [280/468]  eta: 0:00:11  lr: 0.002  img/s: 3372.855566863935  loss: 0.6628 (1.2424)  acc1: 92.9688 (67.3349)  acc5: 100.0000 (91.2339)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [290/468]  eta: 0:00:10  lr: 0.002  img/s: 3639.0379784587644  loss: 0.6992 (1.2249)  acc1: 92.1875 (68.1352)  acc5: 100.0000 (91.5190)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [300/468]  eta: 0:00:10  lr: 0.002  img/s: 3276.9800098882383  loss: 0.7055 (1.2073)  acc1: 91.4062 (68.9369)  acc5: 100.0000 (91.7956)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [310/468]  eta: 0:00:09  lr: 0.002  img/s: 3596.450327576736  loss: 0.6860 (1.1903)  acc1: 92.1875 (69.7171)  acc5: 100.0000 (92.0418)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [320/468]  eta: 0:00:08  lr: 0.002  img/s: 3440.2452452965604  loss: 0.6749 (1.1755)  acc1: 92.9688 (70.3831)  acc5: 100.0000 (92.2873)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [330/468]  eta: 0:00:08  lr: 0.002  img/s: 3599.681596309607  loss: 0.6807 (1.1608)  acc1: 92.9688 (71.0583)  acc5: 100.0000 (92.5038)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [340/468]  eta: 0:00:07  lr: 0.002  img/s: 3264.029960907339  loss: 0.6687 (1.1463)  acc1: 93.7500 (71.7238)  acc5: 100.0000 (92.7144)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [350/468]  eta: 0:00:06  lr: 0.002  img/s: 2509.434432857657  loss: 0.6476 (1.1321)  acc1: 93.7500 (72.3669)  acc5: 100.0000 (92.9153)  time: 0.0386  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [0]  [360/468]  eta: 0:00:06  lr: 0.002  img/s: 2476.787392566006  loss: 0.6356 (1.1186)  acc1: 93.7500 (72.9787)  acc5: 100.0000 (93.1073)  time: 0.0489  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [0]  [370/468]  eta: 0:00:05  lr: 0.002  img/s: 2908.151346900747  loss: 0.6492 (1.1062)  acc1: 93.7500 (73.5428)  acc5: 100.0000 (93.2867)  time: 0.0485  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [0]  [380/468]  eta: 0:00:04  lr: 0.002  img/s: 3440.5098049268154  loss: 0.6544 (1.0938)  acc1: 93.7500 (74.0957)  acc5: 100.0000 (93.4568)  time: 0.0406  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [0]  [390/468]  eta: 0:00:04  lr: 0.002  img/s: 3392.3991482209317  loss: 0.6606 (1.0837)  acc1: 92.9688 (74.5524)  acc5: 100.0000 (93.6141)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [400/468]  eta: 0:00:03  lr: 0.002  img/s: 3594.0427104392884  loss: 0.6609 (1.0728)  acc1: 93.7500 (75.0487)  acc5: 100.0000 (93.7695)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [410/468]  eta: 0:00:03  lr: 0.002  img/s: 3482.601694365521  loss: 0.6539 (1.0631)  acc1: 92.9688 (75.4733)  acc5: 100.0000 (93.9154)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [420/468]  eta: 0:00:02  lr: 0.002  img/s: 3460.6245576490455  loss: 0.6279 (1.0527)  acc1: 95.3125 (75.9538)  acc5: 100.0000 (94.0525)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [430/468]  eta: 0:00:02  lr: 0.002  img/s: 3554.1184196589343  loss: 0.6099 (1.0425)  acc1: 95.3125 (76.4084)  acc5: 100.0000 (94.1905)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [0]  [440/468]  eta: 0:00:01  lr: 0.002  img/s: 3696.159833667238  loss: 0.6209 (1.0333)  acc1: 94.5312 (76.8282)  acc5: 100.0000 (94.3187)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [450/468]  eta: 0:00:00  lr: 0.002  img/s: 3577.42225065302  loss: 0.6355 (1.0245)  acc1: 94.5312 (77.2208)  acc5: 100.0000 (94.4429)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0]  [460/468]  eta: 0:00:00  lr: 0.002  img/s: 3550.005699889573  loss: 0.6098 (1.0152)  acc1: 95.3125 (77.6318)  acc5: 100.0000 (94.5618)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [0] Total time: 0:00:24</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5895 (0.5895)  acc1: 96.0938 (96.0938)  acc5: 100.0000 (100.0000)  time: 0.1171  data: 0.0978  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.010 Acc@5 99.930</span><br><span class="line">Epoch: [1]  [  0/468]  eta: 0:01:13  lr: 0.002  img/s: 2390.055122491953  loss: 0.6364 (0.6364)  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.1565  data: 0.1029  max mem: 190</span><br><span class="line">Epoch: [1]  [ 10/468]  eta: 0:00:21  lr: 0.002  img/s: 3516.7061566980865  loss: 0.5902 (0.6113)  acc1: 96.0938 (95.3125)  acc5: 100.0000 (99.9290)  time: 0.0476  data: 0.0096  max mem: 190</span><br><span class="line">Epoch: [1]  [ 20/468]  eta: 0:00:18  lr: 0.002  img/s: 3492.6156808660126  loss: 0.6155 (0.6309)  acc1: 94.5312 (94.4568)  acc5: 100.0000 (99.8140)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [ 30/468]  eta: 0:00:21  lr: 0.002  img/s: 1832.5428361749698  loss: 0.6361 (0.6283)  acc1: 94.5312 (94.7329)  acc5: 100.0000 (99.8236)  time: 0.0500  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [1]  [ 40/468]  eta: 0:00:20  lr: 0.002  img/s: 3413.4288221156903  loss: 0.6217 (0.6312)  acc1: 94.5312 (94.6837)  acc5: 100.0000 (99.8285)  time: 0.0549  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [1]  [ 50/468]  eta: 0:00:19  lr: 0.002  img/s: 3562.2306916503  loss: 0.6316 (0.6319)  acc1: 94.5312 (94.5925)  acc5: 100.0000 (99.7702)  time: 0.0420  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [1]  [ 60/468]  eta: 0:00:18  lr: 0.002  img/s: 3474.66773671607  loss: 0.6233 (0.6315)  acc1: 94.5312 (94.6337)  acc5: 100.0000 (99.8079)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [ 70/468]  eta: 0:00:17  lr: 0.002  img/s: 3406.649398775342  loss: 0.6054 (0.6279)  acc1: 95.3125 (94.8393)  acc5: 100.0000 (99.8349)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [ 80/468]  eta: 0:00:16  lr: 0.002  img/s: 3441.9214771124502  loss: 0.6031 (0.6286)  acc1: 95.3125 (94.7531)  acc5: 100.0000 (99.8553)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [ 90/468]  eta: 0:00:16  lr: 0.002  img/s: 3423.2448431751377  loss: 0.6496 (0.6324)  acc1: 94.5312 (94.5999)  acc5: 100.0000 (99.8369)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [100/468]  eta: 0:00:15  lr: 0.002  img/s: 3509.3960164987807  loss: 0.6414 (0.6306)  acc1: 93.7500 (94.6473)  acc5: 100.0000 (99.8453)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [110/468]  eta: 0:00:14  lr: 0.002  img/s: 3536.1166606290135  loss: 0.6056 (0.6282)  acc1: 95.3125 (94.7494)  acc5: 100.0000 (99.8592)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [120/468]  eta: 0:00:14  lr: 0.002  img/s: 3551.7201338996283  loss: 0.6056 (0.6265)  acc1: 96.0938 (94.8670)  acc5: 100.0000 (99.8580)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [130/468]  eta: 0:00:13  lr: 0.002  img/s: 3513.990038028289  loss: 0.6123 (0.6287)  acc1: 95.3125 (94.7758)  acc5: 100.0000 (99.8628)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [140/468]  eta: 0:00:13  lr: 0.002  img/s: 2365.0075857025804  loss: 0.6116 (0.6272)  acc1: 95.3125 (94.8637)  acc5: 100.0000 (99.8670)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [150/468]  eta: 0:00:13  lr: 0.002  img/s: 2116.48142804204  loss: 0.6052 (0.6264)  acc1: 96.0938 (94.9141)  acc5: 100.0000 (99.8707)  time: 0.0510  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [1]  [160/468]  eta: 0:00:13  lr: 0.002  img/s: 2346.6791619860214  loss: 0.6072 (0.6246)  acc1: 95.3125 (94.9874)  acc5: 100.0000 (99.8690)  time: 0.0622  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [1]  [170/468]  eta: 0:00:12  lr: 0.002  img/s: 3253.033634881875  loss: 0.5855 (0.6233)  acc1: 95.3125 (95.0612)  acc5: 100.0000 (99.8629)  time: 0.0514  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [1]  [180/468]  eta: 0:00:12  lr: 0.002  img/s: 3258.898336773097  loss: 0.6087 (0.6239)  acc1: 95.3125 (95.0233)  acc5: 100.0000 (99.8576)  time: 0.0408  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [1]  [190/468]  eta: 0:00:11  lr: 0.002  img/s: 3504.1734623945067  loss: 0.6345 (0.6238)  acc1: 94.5312 (95.0344)  acc5: 100.0000 (99.8527)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [200/468]  eta: 0:00:11  lr: 0.002  img/s: 3558.052024998509  loss: 0.6168 (0.6241)  acc1: 95.3125 (95.0249)  acc5: 100.0000 (99.8445)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [210/468]  eta: 0:00:10  lr: 0.002  img/s: 3303.73967410033  loss: 0.6099 (0.6240)  acc1: 95.3125 (95.0163)  acc5: 100.0000 (99.8408)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [220/468]  eta: 0:00:10  lr: 0.002  img/s: 3516.1533856844394  loss: 0.6165 (0.6246)  acc1: 94.5312 (94.9625)  acc5: 100.0000 (99.8374)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [230/468]  eta: 0:00:09  lr: 0.002  img/s: 3532.231380598979  loss: 0.6224 (0.6240)  acc1: 94.5312 (95.0081)  acc5: 100.0000 (99.8444)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [240/468]  eta: 0:00:09  lr: 0.002  img/s: 3485.9936626668746  loss: 0.6139 (0.6245)  acc1: 95.3125 (94.9721)  acc5: 100.0000 (99.8412)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [250/468]  eta: 0:00:09  lr: 0.002  img/s: 3558.971905866755  loss: 0.6313 (0.6250)  acc1: 95.3125 (94.9608)  acc5: 100.0000 (99.8381)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [260/468]  eta: 0:00:08  lr: 0.002  img/s: 3356.2403070729297  loss: 0.6108 (0.6250)  acc1: 95.3125 (94.9683)  acc5: 100.0000 (99.8414)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [270/468]  eta: 0:00:08  lr: 0.002  img/s: 3548.9027618027735  loss: 0.6029 (0.6244)  acc1: 96.0938 (94.9983)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [280/468]  eta: 0:00:07  lr: 0.002  img/s: 3527.7286478388287  loss: 0.6134 (0.6240)  acc1: 96.0938 (95.0317)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [290/468]  eta: 0:00:07  lr: 0.002  img/s: 3131.92185230342  loss: 0.6139 (0.6236)  acc1: 95.3125 (95.0279)  acc5: 100.0000 (99.8443)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [300/468]  eta: 0:00:06  lr: 0.002  img/s: 2843.4905061782665  loss: 0.6021 (0.6229)  acc1: 95.3125 (95.0607)  acc5: 100.0000 (99.8469)  time: 0.0470  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [1]  [310/468]  eta: 0:00:06  lr: 0.002  img/s: 3476.4452214905036  loss: 0.6042 (0.6231)  acc1: 95.3125 (95.0437)  acc5: 100.0000 (99.8493)  time: 0.0499  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [1]  [320/468]  eta: 0:00:06  lr: 0.002  img/s: 3499.6506808685394  loss: 0.6194 (0.6227)  acc1: 95.3125 (95.0594)  acc5: 100.0000 (99.8491)  time: 0.0403  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [1]  [330/468]  eta: 0:00:05  lr: 0.002  img/s: 3577.5414448213132  loss: 0.6167 (0.6226)  acc1: 95.3125 (95.0600)  acc5: 100.0000 (99.8513)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [340/468]  eta: 0:00:05  lr: 0.002  img/s: 3553.3421493292035  loss: 0.6114 (0.6226)  acc1: 94.5312 (95.0536)  acc5: 100.0000 (99.8534)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [350/468]  eta: 0:00:04  lr: 0.002  img/s: 3620.0947519605133  loss: 0.6088 (0.6230)  acc1: 94.5312 (95.0521)  acc5: 100.0000 (99.8442)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [360/468]  eta: 0:00:04  lr: 0.002  img/s: 3531.1394576391585  loss: 0.6052 (0.6223)  acc1: 96.0938 (95.0918)  acc5: 100.0000 (99.8442)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [370/468]  eta: 0:00:03  lr: 0.002  img/s: 3390.620891751926  loss: 0.5937 (0.6220)  acc1: 96.0938 (95.0998)  acc5: 100.0000 (99.8484)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [380/468]  eta: 0:00:03  lr: 0.002  img/s: 3559.184253618048  loss: 0.5937 (0.6215)  acc1: 95.3125 (95.1013)  acc5: 100.0000 (99.8483)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [390/468]  eta: 0:00:03  lr: 0.002  img/s: 3451.502838370396  loss: 0.6005 (0.6211)  acc1: 96.0938 (95.1247)  acc5: 100.0000 (99.8461)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [400/468]  eta: 0:00:02  lr: 0.002  img/s: 3515.7389214498544  loss: 0.6005 (0.6209)  acc1: 95.3125 (95.1391)  acc5: 100.0000 (99.8402)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [410/468]  eta: 0:00:02  lr: 0.002  img/s: 3433.4687779795863  loss: 0.6419 (0.6247)  acc1: 94.5312 (94.9856)  acc5: 100.0000 (99.8270)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [420/468]  eta: 0:00:01  lr: 0.002  img/s: 3529.1896162973385  loss: 0.6978 (0.6257)  acc1: 90.6250 (94.9358)  acc5: 100.0000 (99.8256)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [1]  [430/468]  eta: 0:00:01  lr: 0.002  img/s: 3377.545010160236  loss: 0.6389 (0.6257)  acc1: 94.5312 (94.9355)  acc5: 100.0000 (99.8242)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [440/468]  eta: 0:00:01  lr: 0.002  img/s: 3556.0723573089226  loss: 0.6291 (0.6259)  acc1: 94.5312 (94.9157)  acc5: 100.0000 (99.8228)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [450/468]  eta: 0:00:00  lr: 0.002  img/s: 3517.881372369145  loss: 0.6064 (0.6255)  acc1: 95.3125 (94.9349)  acc5: 100.0000 (99.8268)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1]  [460/468]  eta: 0:00:00  lr: 0.002  img/s: 3552.3546592029434  loss: 0.6011 (0.6259)  acc1: 94.5312 (94.9109)  acc5: 100.0000 (99.8288)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [1] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5358 (0.5358)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1236  data: 0.0995  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 96.310 Acc@5 99.900</span><br><span class="line">Epoch: [2]  [  0/468]  eta: 0:01:13  lr: 0.0019781476007338056  img/s: 2821.315311811909  loss: 0.5854 (0.5854)  acc1: 96.0938 (96.0938)  acc5: 100.0000 (100.0000)  time: 0.1570  data: 0.1116  max mem: 190</span><br><span class="line">Epoch: [2]  [ 10/468]  eta: 0:00:22  lr: 0.0019781476007338056  img/s: 3476.2426314426316  loss: 0.6098 (0.6028)  acc1: 96.0938 (95.8097)  acc5: 100.0000 (99.9290)  time: 0.0488  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [2]  [ 20/468]  eta: 0:00:20  lr: 0.0019781476007338056  img/s: 3316.966389877422  loss: 0.6098 (0.6142)  acc1: 95.3125 (95.2381)  acc5: 100.0000 (99.8140)  time: 0.0404  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [2]  [ 30/468]  eta: 0:00:19  lr: 0.0019781476007338056  img/s: 3366.1095596671953  loss: 0.6261 (0.6167)  acc1: 94.5312 (95.1613)  acc5: 100.0000 (99.8236)  time: 0.0425  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [ 40/468]  eta: 0:00:18  lr: 0.0019781476007338056  img/s: 3201.510569910491  loss: 0.6153 (0.6128)  acc1: 95.3125 (95.4649)  acc5: 100.0000 (99.8095)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 50/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 3468.8081875803605  loss: 0.5954 (0.6109)  acc1: 96.0938 (95.6342)  acc5: 100.0000 (99.8315)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 60/468]  eta: 0:00:16  lr: 0.0019781476007338056  img/s: 3455.4570860338936  loss: 0.5947 (0.6105)  acc1: 96.0938 (95.6327)  acc5: 100.0000 (99.8335)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [ 70/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 2209.2453098830915  loss: 0.6161 (0.6150)  acc1: 95.3125 (95.4445)  acc5: 100.0000 (99.8129)  time: 0.0469  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [2]  [ 80/468]  eta: 0:00:17  lr: 0.0019781476007338056  img/s: 3254.7888546693503  loss: 0.6118 (0.6127)  acc1: 95.3125 (95.4572)  acc5: 100.0000 (99.8264)  time: 0.0529  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [2]  [ 90/468]  eta: 0:00:16  lr: 0.0019781476007338056  img/s: 3281.928012519562  loss: 0.6002 (0.6121)  acc1: 96.0938 (95.5100)  acc5: 100.0000 (99.7940)  time: 0.0450  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [2]  [100/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 3343.969205663069  loss: 0.6085 (0.6118)  acc1: 96.0938 (95.5523)  acc5: 100.0000 (99.7912)  time: 0.0400  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [2]  [110/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 2319.247087284273  loss: 0.5869 (0.6091)  acc1: 96.0938 (95.6503)  acc5: 100.0000 (99.8029)  time: 0.0431  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [120/468]  eta: 0:00:15  lr: 0.0019781476007338056  img/s: 2519.728123078656  loss: 0.5794 (0.6075)  acc1: 96.0938 (95.6999)  acc5: 100.0000 (99.8063)  time: 0.0471  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [130/468]  eta: 0:00:14  lr: 0.0019781476007338056  img/s: 2833.570376000169  loss: 0.5886 (0.6080)  acc1: 96.0938 (95.6763)  acc5: 100.0000 (99.8151)  time: 0.0471  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [2]  [140/468]  eta: 0:00:14  lr: 0.0019781476007338056  img/s: 3309.3399577140954  loss: 0.5939 (0.6070)  acc1: 96.0938 (95.7170)  acc5: 100.0000 (99.8227)  time: 0.0441  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [150/468]  eta: 0:00:13  lr: 0.0019781476007338056  img/s: 3178.707086017431  loss: 0.6062 (0.6089)  acc1: 96.0938 (95.6178)  acc5: 100.0000 (99.8241)  time: 0.0411  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [160/468]  eta: 0:00:13  lr: 0.0019781476007338056  img/s: 3495.025792591628  loss: 0.6187 (0.6087)  acc1: 95.3125 (95.6619)  acc5: 100.0000 (99.8205)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [170/468]  eta: 0:00:12  lr: 0.0019781476007338056  img/s: 3553.836100299203  loss: 0.6058 (0.6102)  acc1: 96.0938 (95.6232)  acc5: 100.0000 (99.8173)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [180/468]  eta: 0:00:12  lr: 0.0019781476007338056  img/s: 3498.419220518568  loss: 0.6069 (0.6115)  acc1: 95.3125 (95.5413)  acc5: 100.0000 (99.8187)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [190/468]  eta: 0:00:11  lr: 0.0019781476007338056  img/s: 3494.0022257655137  loss: 0.6069 (0.6111)  acc1: 95.3125 (95.5334)  acc5: 100.0000 (99.8282)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [200/468]  eta: 0:00:11  lr: 0.0019781476007338056  img/s: 3423.7469516861383  loss: 0.5874 (0.6098)  acc1: 96.0938 (95.5729)  acc5: 100.0000 (99.8368)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [210/468]  eta: 0:00:10  lr: 0.0019781476007338056  img/s: 3504.1963343950706  loss: 0.5874 (0.6095)  acc1: 96.0938 (95.5791)  acc5: 100.0000 (99.8408)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [220/468]  eta: 0:00:10  lr: 0.0019781476007338056  img/s: 3503.6932193434704  loss: 0.5977 (0.6110)  acc1: 95.3125 (95.5246)  acc5: 100.0000 (99.8409)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [230/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3421.2596831546884  loss: 0.6120 (0.6113)  acc1: 94.5312 (95.4917)  acc5: 100.0000 (99.8410)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [240/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3483.8672567520216  loss: 0.6175 (0.6122)  acc1: 94.5312 (95.4487)  acc5: 100.0000 (99.8476)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [250/468]  eta: 0:00:09  lr: 0.0019781476007338056  img/s: 3258.5818543785963  loss: 0.6107 (0.6115)  acc1: 95.3125 (95.4681)  acc5: 100.0000 (99.8444)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [260/468]  eta: 0:00:08  lr: 0.0019781476007338056  img/s: 1981.9730431155099  loss: 0.6107 (0.6116)  acc1: 95.3125 (95.4622)  acc5: 100.0000 (99.8443)  time: 0.0385  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [270/468]  eta: 0:00:08  lr: 0.0019781476007338056  img/s: 3555.4600494043007  loss: 0.6134 (0.6117)  acc1: 95.3125 (95.4595)  acc5: 100.0000 (99.8443)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [280/468]  eta: 0:00:07  lr: 0.0019781476007338056  img/s: 3548.410181164449  loss: 0.5971 (0.6111)  acc1: 96.0938 (95.4932)  acc5: 100.0000 (99.8443)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [290/468]  eta: 0:00:07  lr: 0.0019781476007338056  img/s: 3422.0447458664253  loss: 0.5899 (0.6107)  acc1: 96.0938 (95.4924)  acc5: 100.0000 (99.8416)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [300/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3448.1997739184053  loss: 0.6042 (0.6106)  acc1: 96.0938 (95.5124)  acc5: 100.0000 (99.8443)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [310/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3380.6714607760414  loss: 0.6186 (0.6109)  acc1: 95.3125 (95.5110)  acc5: 100.0000 (99.8468)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [2]  [320/468]  eta: 0:00:06  lr: 0.0019781476007338056  img/s: 3389.9786070594178  loss: 0.6108 (0.6104)  acc1: 95.3125 (95.5242)  acc5: 100.0000 (99.8515)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [330/468]  eta: 0:00:05  lr: 0.0019781476007338056  img/s: 3506.7369837423335  loss: 0.5978 (0.6102)  acc1: 96.0938 (95.5249)  acc5: 100.0000 (99.8513)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [340/468]  eta: 0:00:05  lr: 0.0019781476007338056  img/s: 1529.856215929718  loss: 0.6067 (0.6103)  acc1: 95.3125 (95.5187)  acc5: 100.0000 (99.8534)  time: 0.0402  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [2]  [350/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 2602.5688343796205  loss: 0.6111 (0.6111)  acc1: 94.5312 (95.4950)  acc5: 100.0000 (99.8397)  time: 0.0504  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [2]  [360/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 3450.659845100749  loss: 0.6074 (0.6105)  acc1: 95.3125 (95.5181)  acc5: 100.0000 (99.8377)  time: 0.0477  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [2]  [370/468]  eta: 0:00:04  lr: 0.0019781476007338056  img/s: 3490.935119318551  loss: 0.6074 (0.6108)  acc1: 95.3125 (95.5041)  acc5: 100.0000 (99.8357)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [380/468]  eta: 0:00:03  lr: 0.0019781476007338056  img/s: 3499.1032581420964  loss: 0.5872 (0.6100)  acc1: 96.0938 (95.5504)  acc5: 100.0000 (99.8380)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [390/468]  eta: 0:00:03  lr: 0.0019781476007338056  img/s: 3348.140069473461  loss: 0.5739 (0.6090)  acc1: 96.8750 (95.5942)  acc5: 100.0000 (99.8362)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [400/468]  eta: 0:00:02  lr: 0.0019781476007338056  img/s: 3369.2571543327645  loss: 0.5725 (0.6083)  acc1: 96.8750 (95.6242)  acc5: 100.0000 (99.8363)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [410/468]  eta: 0:00:02  lr: 0.0019781476007338056  img/s: 3479.306511820821  loss: 0.5725 (0.6077)  acc1: 96.8750 (95.6490)  acc5: 100.0000 (99.8384)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [420/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 3559.3966267105125  loss: 0.5968 (0.6074)  acc1: 96.0938 (95.6632)  acc5: 100.0000 (99.8404)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [2]  [430/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 2184.8355973726834  loss: 0.5968 (0.6067)  acc1: 96.0938 (95.6805)  acc5: 100.0000 (99.8423)  time: 0.0443  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [2]  [440/468]  eta: 0:00:01  lr: 0.0019781476007338056  img/s: 2506.7746442045495  loss: 0.5753 (0.6061)  acc1: 96.0938 (95.7022)  acc5: 100.0000 (99.8441)  time: 0.0546  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [2]  [450/468]  eta: 0:00:00  lr: 0.0019781476007338056  img/s: 3444.5714872321314  loss: 0.5954 (0.6062)  acc1: 96.0938 (95.7023)  acc5: 100.0000 (99.8424)  time: 0.0545  data: 0.0039  max mem: 190</span><br><span class="line">Epoch: [2]  [460/468]  eta: 0:00:00  lr: 0.0019781476007338056  img/s: 3485.7446938364747  loss: 0.5979 (0.6060)  acc1: 95.3125 (95.7074)  acc5: 100.0000 (99.8424)  time: 0.0444  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [2] Total time: 0:00:19</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5610 (0.5610)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 0.1196  data: 0.0968  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 96.960 Acc@5 99.950</span><br><span class="line">Epoch: [3]  [  0/468]  eta: 0:01:21  lr: 0.001913545457642601  img/s: 2668.437331318684  loss: 0.6060 (0.6060)  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.1742  data: 0.1262  max mem: 190</span><br><span class="line">Epoch: [3]  [ 10/468]  eta: 0:00:23  lr: 0.001913545457642601  img/s: 3215.315721703509  loss: 0.6060 (0.5948)  acc1: 95.3125 (96.0227)  acc5: 100.0000 (99.9290)  time: 0.0504  data: 0.0117  max mem: 190</span><br><span class="line">Epoch: [3]  [ 20/468]  eta: 0:00:19  lr: 0.001913545457642601  img/s: 3411.194916923468  loss: 0.5929 (0.5942)  acc1: 96.0938 (96.0193)  acc5: 100.0000 (99.9256)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 30/468]  eta: 0:00:18  lr: 0.001913545457642601  img/s: 3474.3529289948488  loss: 0.5953 (0.5982)  acc1: 96.0938 (96.1190)  acc5: 100.0000 (99.8236)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 40/468]  eta: 0:00:17  lr: 0.001913545457642601  img/s: 3487.4429143065013  loss: 0.6071 (0.5976)  acc1: 95.3125 (96.0938)  acc5: 100.0000 (99.8666)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 50/468]  eta: 0:00:16  lr: 0.001913545457642601  img/s: 3424.161848088833  loss: 0.5653 (0.5907)  acc1: 97.6562 (96.3848)  acc5: 100.0000 (99.8928)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 60/468]  eta: 0:00:16  lr: 0.001913545457642601  img/s: 3444.3946929453127  loss: 0.5667 (0.5880)  acc1: 96.8750 (96.4267)  acc5: 100.0000 (99.9103)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 70/468]  eta: 0:00:15  lr: 0.001913545457642601  img/s: 3553.5538257876624  loss: 0.5666 (0.5839)  acc1: 96.8750 (96.6329)  acc5: 100.0000 (99.9120)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 80/468]  eta: 0:00:15  lr: 0.001913545457642601  img/s: 3473.8358687001364  loss: 0.5538 (0.5815)  acc1: 97.6562 (96.7207)  acc5: 100.0000 (99.9228)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [ 90/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3484.319466258226  loss: 0.5678 (0.5810)  acc1: 96.8750 (96.7119)  acc5: 100.0000 (99.9227)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [100/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3542.579987858632  loss: 0.5647 (0.5788)  acc1: 96.8750 (96.7899)  acc5: 100.0000 (99.9149)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [110/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 2912.3950960182274  loss: 0.5647 (0.5789)  acc1: 96.8750 (96.7483)  acc5: 100.0000 (99.9226)  time: 0.0464  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [3]  [120/468]  eta: 0:00:14  lr: 0.001913545457642601  img/s: 3437.117709573746  loss: 0.5790 (0.5800)  acc1: 96.0938 (96.7265)  acc5: 100.0000 (99.9290)  time: 0.0512  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [3]  [130/468]  eta: 0:00:13  lr: 0.001913545457642601  img/s: 3497.894972765891  loss: 0.6027 (0.5813)  acc1: 96.0938 (96.6961)  acc5: 100.0000 (99.9165)  time: 0.0423  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [3]  [140/468]  eta: 0:00:13  lr: 0.001913545457642601  img/s: 3335.4720610345557  loss: 0.5973 (0.5822)  acc1: 96.0938 (96.6201)  acc5: 100.0000 (99.9113)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [150/468]  eta: 0:00:12  lr: 0.001913545457642601  img/s: 3523.792381002389  loss: 0.5937 (0.5826)  acc1: 96.0938 (96.6215)  acc5: 100.0000 (99.9120)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [160/468]  eta: 0:00:12  lr: 0.001913545457642601  img/s: 3514.9793240712856  loss: 0.6063 (0.5881)  acc1: 96.0938 (96.4431)  acc5: 100.0000 (99.8932)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [170/468]  eta: 0:00:11  lr: 0.001913545457642601  img/s: 3534.068262755656  loss: 0.6085 (0.5878)  acc1: 96.0938 (96.4638)  acc5: 100.0000 (99.8995)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [180/468]  eta: 0:00:11  lr: 0.001913545457642601  img/s: 3485.1111154387945  loss: 0.5896 (0.5887)  acc1: 96.8750 (96.4132)  acc5: 100.0000 (99.9050)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [190/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3553.4832641660546  loss: 0.5913 (0.5890)  acc1: 95.3125 (96.3719)  acc5: 100.0000 (99.9059)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [200/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3421.3905019245967  loss: 0.5847 (0.5893)  acc1: 95.3125 (96.3386)  acc5: 100.0000 (99.9067)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [210/468]  eta: 0:00:10  lr: 0.001913545457642601  img/s: 3595.2701922626184  loss: 0.5847 (0.5894)  acc1: 96.0938 (96.3344)  acc5: 100.0000 (99.9111)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [220/468]  eta: 0:00:09  lr: 0.001913545457642601  img/s: 3537.258275353152  loss: 0.5805 (0.5894)  acc1: 96.8750 (96.3518)  acc5: 100.0000 (99.9152)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [230/468]  eta: 0:00:09  lr: 0.001913545457642601  img/s: 3386.7281008314303  loss: 0.5732 (0.5886)  acc1: 96.8750 (96.3846)  acc5: 100.0000 (99.9154)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [240/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3616.8023821393444  loss: 0.5657 (0.5874)  acc1: 96.8750 (96.4341)  acc5: 100.0000 (99.9157)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [250/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3618.533177863002  loss: 0.5598 (0.5867)  acc1: 96.8750 (96.4704)  acc5: 100.0000 (99.9128)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [260/468]  eta: 0:00:08  lr: 0.001913545457642601  img/s: 3545.551224731048  loss: 0.5767 (0.5865)  acc1: 96.8750 (96.4559)  acc5: 100.0000 (99.9162)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [270/468]  eta: 0:00:07  lr: 0.001913545457642601  img/s: 3522.613213303851  loss: 0.5849 (0.5870)  acc1: 96.0938 (96.4310)  acc5: 100.0000 (99.9193)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [280/468]  eta: 0:00:07  lr: 0.001913545457642601  img/s: 3410.609813737199  loss: 0.5820 (0.5864)  acc1: 96.8750 (96.4552)  acc5: 100.0000 (99.9166)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [290/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3503.327408219464  loss: 0.5733 (0.5860)  acc1: 96.8750 (96.4696)  acc5: 100.0000 (99.9168)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [300/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3625.130232212671  loss: 0.5620 (0.5857)  acc1: 96.8750 (96.4805)  acc5: 100.0000 (99.9195)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [310/468]  eta: 0:00:06  lr: 0.001913545457642601  img/s: 3454.3232016471497  loss: 0.5660 (0.5855)  acc1: 96.8750 (96.4957)  acc5: 100.0000 (99.9196)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [320/468]  eta: 0:00:05  lr: 0.001913545457642601  img/s: 3379.0330746524173  loss: 0.6028 (0.5871)  acc1: 96.0938 (96.4393)  acc5: 100.0000 (99.9124)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [330/468]  eta: 0:00:05  lr: 0.001913545457642601  img/s: 3490.503884688152  loss: 0.6150 (0.5877)  acc1: 95.3125 (96.4171)  acc5: 100.0000 (99.9103)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [340/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3462.0081379977432  loss: 0.5826 (0.5875)  acc1: 96.0938 (96.4214)  acc5: 100.0000 (99.9084)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [350/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3005.1716606306222  loss: 0.5865 (0.5878)  acc1: 96.0938 (96.4009)  acc5: 100.0000 (99.9110)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [360/468]  eta: 0:00:04  lr: 0.001913545457642601  img/s: 3514.818239549576  loss: 0.5899 (0.5879)  acc1: 96.0938 (96.4140)  acc5: 100.0000 (99.9134)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [370/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 3496.049959300622  loss: 0.5930 (0.5880)  acc1: 96.0938 (96.3907)  acc5: 100.0000 (99.9137)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [380/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 3580.0702315935478  loss: 0.6009 (0.5906)  acc1: 94.5312 (96.2803)  acc5: 100.0000 (99.9118)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [390/468]  eta: 0:00:03  lr: 0.001913545457642601  img/s: 2281.0239118981663  loss: 0.6759 (0.5998)  acc1: 92.1875 (95.9359)  acc5: 100.0000 (99.8441)  time: 0.0419  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [3]  [400/468]  eta: 0:00:02  lr: 0.001913545457642601  img/s: 3228.949118290952  loss: 0.8094 (0.6080)  acc1: 86.7188 (95.6223)  acc5: 99.2188 (99.8208)  time: 0.0525  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [3]  [410/468]  eta: 0:00:02  lr: 0.001913545457642601  img/s: 3199.450015196572  loss: 0.7277 (0.6102)  acc1: 89.0625 (95.5254)  acc5: 100.0000 (99.8213)  time: 0.0491  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [3]  [420/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3448.9086237023334  loss: 0.6970 (0.6125)  acc1: 91.4062 (95.4313)  acc5: 100.0000 (99.8126)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [430/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3289.6703533722634  loss: 0.6822 (0.6141)  acc1: 92.1875 (95.3705)  acc5: 100.0000 (99.8133)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [440/468]  eta: 0:00:01  lr: 0.001913545457642601  img/s: 3540.1505552185267  loss: 0.6592 (0.6149)  acc1: 93.7500 (95.3426)  acc5: 100.0000 (99.8104)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3]  [450/468]  eta: 0:00:00  lr: 0.001913545457642601  img/s: 3581.431529512221  loss: 0.6415 (0.6156)  acc1: 93.7500 (95.3090)  acc5: 100.0000 (99.8095)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [3]  [460/468]  eta: 0:00:00  lr: 0.001913545457642601  img/s: 3241.6413290906125  loss: 0.6232 (0.6155)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (99.8119)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [3] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5786 (0.5786)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1213  data: 0.0950  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.390 Acc@5 99.820</span><br><span class="line">Epoch: [4]  [  0/468]  eta: 0:01:13  lr: 0.0018090169943749475  img/s: 2566.57445811701  loss: 0.6653 (0.6653)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.1563  data: 0.1064  max mem: 190</span><br><span class="line">Epoch: [4]  [ 10/468]  eta: 0:00:24  lr: 0.0018090169943749475  img/s: 3494.7982814737666  loss: 0.6109 (0.6174)  acc1: 96.0938 (95.3125)  acc5: 100.0000 (99.8580)  time: 0.0527  data: 0.0101  max mem: 190</span><br><span class="line">Epoch: [4]  [ 20/468]  eta: 0:00:21  lr: 0.0018090169943749475  img/s: 3517.005646904684  loss: 0.6020 (0.6088)  acc1: 96.0938 (95.7217)  acc5: 100.0000 (99.9256)  time: 0.0418  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [4]  [ 30/468]  eta: 0:00:19  lr: 0.0018090169943749475  img/s: 3463.392825117893  loss: 0.5943 (0.6025)  acc1: 96.0938 (95.9173)  acc5: 100.0000 (99.9244)  time: 0.0390  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [4]  [ 40/468]  eta: 0:00:18  lr: 0.0018090169943749475  img/s: 3438.5706453513694  loss: 0.5790 (0.5999)  acc1: 96.0938 (95.9794)  acc5: 100.0000 (99.9428)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [ 50/468]  eta: 0:00:17  lr: 0.0018090169943749475  img/s: 3453.900964365442  loss: 0.5844 (0.5983)  acc1: 96.0938 (96.0478)  acc5: 100.0000 (99.9081)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [ 60/468]  eta: 0:00:16  lr: 0.0018090169943749475  img/s: 3443.908602219514  loss: 0.6074 (0.6035)  acc1: 95.3125 (95.7992)  acc5: 100.0000 (99.8975)  time: 0.0384  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [4]  [ 70/468]  eta: 0:00:16  lr: 0.0018090169943749475  img/s: 3476.0850777289297  loss: 0.6247 (0.6063)  acc1: 94.5312 (95.7416)  acc5: 100.0000 (99.9010)  time: 0.0386  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [4]  [ 80/468]  eta: 0:00:15  lr: 0.0018090169943749475  img/s: 3564.5721949632502  loss: 0.6247 (0.6091)  acc1: 94.5312 (95.6019)  acc5: 100.0000 (99.8939)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [ 90/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3499.81037809648  loss: 0.6081 (0.6080)  acc1: 95.3125 (95.6731)  acc5: 100.0000 (99.8884)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [100/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3433.578572387902  loss: 0.6030 (0.6081)  acc1: 95.3125 (95.6142)  acc5: 100.0000 (99.8917)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [110/468]  eta: 0:00:14  lr: 0.0018090169943749475  img/s: 3602.9912151778103  loss: 0.6003 (0.6076)  acc1: 94.5312 (95.6011)  acc5: 100.0000 (99.8944)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [120/468]  eta: 0:00:13  lr: 0.0018090169943749475  img/s: 3575.6114766763462  loss: 0.5878 (0.6064)  acc1: 96.0938 (95.6353)  acc5: 100.0000 (99.9032)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [130/468]  eta: 0:00:13  lr: 0.0018090169943749475  img/s: 3571.3301048374224  loss: 0.5812 (0.6045)  acc1: 96.8750 (95.7180)  acc5: 100.0000 (99.9046)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [140/468]  eta: 0:00:12  lr: 0.0018090169943749475  img/s: 3436.017817828068  loss: 0.5775 (0.6025)  acc1: 96.8750 (95.7890)  acc5: 100.0000 (99.9058)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [150/468]  eta: 0:00:12  lr: 0.0018090169943749475  img/s: 3567.675283422602  loss: 0.5696 (0.6018)  acc1: 96.8750 (95.8506)  acc5: 100.0000 (99.9069)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [160/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3576.2307456601966  loss: 0.5856 (0.6003)  acc1: 96.8750 (95.9045)  acc5: 100.0000 (99.9078)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [170/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 2341.4799443492216  loss: 0.5834 (0.5991)  acc1: 96.8750 (95.9430)  acc5: 100.0000 (99.9086)  time: 0.0456  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [180/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3425.844300372658  loss: 0.5871 (0.5986)  acc1: 96.0938 (95.9384)  acc5: 100.0000 (99.9007)  time: 0.0495  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [4]  [190/468]  eta: 0:00:11  lr: 0.0018090169943749475  img/s: 3642.025045790652  loss: 0.5852 (0.5970)  acc1: 96.0938 (96.0079)  acc5: 100.0000 (99.9059)  time: 0.0403  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [4]  [200/468]  eta: 0:00:10  lr: 0.0018090169943749475  img/s: 3513.277177185038  loss: 0.5691 (0.5967)  acc1: 96.8750 (96.0199)  acc5: 100.0000 (99.9028)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [210/468]  eta: 0:00:10  lr: 0.0018090169943749475  img/s: 3409.310302783987  loss: 0.5779 (0.5966)  acc1: 96.0938 (96.0086)  acc5: 100.0000 (99.9000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [220/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 2335.2163617541387  loss: 0.5842 (0.5966)  acc1: 96.0938 (96.0230)  acc5: 100.0000 (99.9010)  time: 0.0430  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [4]  [230/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 2321.1723311989554  loss: 0.5957 (0.5973)  acc1: 96.0938 (96.0058)  acc5: 100.0000 (99.8985)  time: 0.0547  data: 0.0040  max mem: 190</span><br><span class="line">Epoch: [4]  [240/468]  eta: 0:00:09  lr: 0.0018090169943749475  img/s: 3229.8426923030647  loss: 0.5963 (0.5970)  acc1: 96.0938 (96.0095)  acc5: 100.0000 (99.8930)  time: 0.0570  data: 0.0054  max mem: 190</span><br><span class="line">Epoch: [4]  [250/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3209.798589023078  loss: 0.5701 (0.5961)  acc1: 96.8750 (96.0471)  acc5: 100.0000 (99.8911)  time: 0.0471  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [4]  [260/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3546.6755101636354  loss: 0.5701 (0.6003)  acc1: 96.8750 (95.8752)  acc5: 100.0000 (99.8743)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [270/468]  eta: 0:00:08  lr: 0.0018090169943749475  img/s: 3492.1840309623703  loss: 0.6600 (0.6041)  acc1: 92.9688 (95.7074)  acc5: 100.0000 (99.8645)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [280/468]  eta: 0:00:07  lr: 0.0018090169943749475  img/s: 3504.7453519949863  loss: 0.6688 (0.6056)  acc1: 92.1875 (95.6350)  acc5: 100.0000 (99.8638)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [290/468]  eta: 0:00:07  lr: 0.0018090169943749475  img/s: 3551.9786168432056  loss: 0.6537 (0.6070)  acc1: 93.7500 (95.5944)  acc5: 100.0000 (99.8523)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [300/468]  eta: 0:00:06  lr: 0.0018090169943749475  img/s: 3557.674775521023  loss: 0.6653 (0.6094)  acc1: 93.7500 (95.5046)  acc5: 99.2188 (99.8391)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [310/468]  eta: 0:00:06  lr: 0.0018090169943749475  img/s: 3551.7906255168537  loss: 0.6758 (0.6117)  acc1: 92.9688 (95.4130)  acc5: 100.0000 (99.8342)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [320/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3595.39057874928  loss: 0.6588 (0.6142)  acc1: 92.9688 (95.3149)  acc5: 100.0000 (99.8248)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [330/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3503.7846840614516  loss: 0.6509 (0.6150)  acc1: 93.7500 (95.2960)  acc5: 100.0000 (99.8230)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [340/468]  eta: 0:00:05  lr: 0.0018090169943749475  img/s: 3458.952349045177  loss: 0.6186 (0.6149)  acc1: 95.3125 (95.3033)  acc5: 100.0000 (99.8236)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [350/468]  eta: 0:00:04  lr: 0.0018090169943749475  img/s: 3440.5318533993836  loss: 0.5989 (0.6143)  acc1: 96.0938 (95.3281)  acc5: 100.0000 (99.8264)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [360/468]  eta: 0:00:04  lr: 0.0018090169943749475  img/s: 3385.6815685087436  loss: 0.5974 (0.6144)  acc1: 96.0938 (95.3276)  acc5: 100.0000 (99.8247)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [370/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3553.3421493292035  loss: 0.5979 (0.6139)  acc1: 96.0938 (95.3651)  acc5: 100.0000 (99.8273)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [380/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3550.9214244139903  loss: 0.5998 (0.6138)  acc1: 96.0938 (95.3494)  acc5: 100.0000 (99.8319)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [390/468]  eta: 0:00:03  lr: 0.0018090169943749475  img/s: 3523.838638959266  loss: 0.6247 (0.6144)  acc1: 94.5312 (95.3165)  acc5: 100.0000 (99.8342)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [4]  [400/468]  eta: 0:00:02  lr: 0.0018090169943749475  img/s: 3421.870256351422  loss: 0.6127 (0.6145)  acc1: 94.5312 (95.3281)  acc5: 100.0000 (99.8266)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [410/468]  eta: 0:00:02  lr: 0.0018090169943749475  img/s: 3300.7132484491526  loss: 0.6100 (0.6147)  acc1: 94.5312 (95.3125)  acc5: 100.0000 (99.8251)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [420/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3615.414067813731  loss: 0.6168 (0.6146)  acc1: 95.3125 (95.3106)  acc5: 100.0000 (99.8200)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [430/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3585.21036955912  loss: 0.5991 (0.6142)  acc1: 95.3125 (95.3252)  acc5: 100.0000 (99.8187)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [4]  [440/468]  eta: 0:00:01  lr: 0.0018090169943749475  img/s: 3062.4962893243205  loss: 0.5911 (0.6138)  acc1: 95.3125 (95.3444)  acc5: 100.0000 (99.8211)  time: 0.0415  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [4]  [450/468]  eta: 0:00:00  lr: 0.0018090169943749475  img/s: 2291.2944530513128  loss: 0.5909 (0.6136)  acc1: 95.3125 (95.3419)  acc5: 100.0000 (99.8233)  time: 0.0487  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [4]  [460/468]  eta: 0:00:00  lr: 0.0018090169943749475  img/s: 3341.2845069020027  loss: 0.5903 (0.6131)  acc1: 95.3125 (95.3498)  acc5: 100.0000 (99.8271)  time: 0.0447  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [4] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.6161 (0.6161)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 0.1243  data: 0.0979  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 95.730 Acc@5 99.930</span><br><span class="line">Epoch: [5]  [  0/468]  eta: 0:01:14  lr: 0.0016691306063588583  img/s: 2564.9313560617643  loss: 0.6228 (0.6228)  acc1: 96.0938 (96.0938)  acc5: 99.2188 (99.2188)  time: 0.1600  data: 0.1101  max mem: 190</span><br><span class="line">Epoch: [5]  [ 10/468]  eta: 0:00:24  lr: 0.0016691306063588583  img/s: 3533.300725257657  loss: 0.6159 (0.6111)  acc1: 96.0938 (95.5966)  acc5: 100.0000 (99.7869)  time: 0.0539  data: 0.0111  max mem: 190</span><br><span class="line">Epoch: [5]  [ 20/468]  eta: 0:00:21  lr: 0.0016691306063588583  img/s: 3536.978628086542  loss: 0.5970 (0.5997)  acc1: 96.0938 (95.9077)  acc5: 100.0000 (99.8140)  time: 0.0420  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [5]  [ 30/468]  eta: 0:00:19  lr: 0.0016691306063588583  img/s: 3382.5245370749562  loss: 0.5810 (0.6025)  acc1: 96.0938 (95.8417)  acc5: 100.0000 (99.7984)  time: 0.0390  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [5]  [ 40/468]  eta: 0:00:18  lr: 0.0016691306063588583  img/s: 3408.271406805485  loss: 0.6031 (0.6021)  acc1: 95.3125 (95.8460)  acc5: 100.0000 (99.8285)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 50/468]  eta: 0:00:17  lr: 0.0016691306063588583  img/s: 3385.361330760597  loss: 0.5901 (0.6014)  acc1: 96.0938 (95.8946)  acc5: 100.0000 (99.8468)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 60/468]  eta: 0:00:16  lr: 0.0016691306063588583  img/s: 3415.5135444632474  loss: 0.5825 (0.5997)  acc1: 96.8750 (96.0041)  acc5: 100.0000 (99.8719)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 70/468]  eta: 0:00:16  lr: 0.0016691306063588583  img/s: 3336.840315242523  loss: 0.5662 (0.5967)  acc1: 97.6562 (96.0827)  acc5: 100.0000 (99.8790)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 80/468]  eta: 0:00:15  lr: 0.0016691306063588583  img/s: 3558.1935141798613  loss: 0.5666 (0.5931)  acc1: 97.6562 (96.2288)  acc5: 100.0000 (99.8939)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [ 90/468]  eta: 0:00:15  lr: 0.0016691306063588583  img/s: 3518.388570679599  loss: 0.5748 (0.5911)  acc1: 96.8750 (96.2826)  acc5: 100.0000 (99.9056)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [100/468]  eta: 0:00:14  lr: 0.0016691306063588583  img/s: 3507.9972295186944  loss: 0.5749 (0.5911)  acc1: 96.0938 (96.2871)  acc5: 100.0000 (99.9149)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [110/468]  eta: 0:00:14  lr: 0.0016691306063588583  img/s: 3445.898023106547  loss: 0.5652 (0.5887)  acc1: 96.8750 (96.3612)  acc5: 100.0000 (99.9155)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [120/468]  eta: 0:00:13  lr: 0.0016691306063588583  img/s: 3503.6703539101095  loss: 0.5570 (0.5868)  acc1: 98.4375 (96.4941)  acc5: 100.0000 (99.9161)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [130/468]  eta: 0:00:13  lr: 0.0016691306063588583  img/s: 3515.8770653376905  loss: 0.5673 (0.5860)  acc1: 97.6562 (96.5649)  acc5: 100.0000 (99.9105)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [140/468]  eta: 0:00:12  lr: 0.0016691306063588583  img/s: 3546.5115074646583  loss: 0.5682 (0.5845)  acc1: 97.6562 (96.6312)  acc5: 100.0000 (99.9169)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [150/468]  eta: 0:00:12  lr: 0.0016691306063588583  img/s: 3463.392825117893  loss: 0.5682 (0.5837)  acc1: 96.8750 (96.6474)  acc5: 100.0000 (99.9224)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [160/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3087.843005532997  loss: 0.5680 (0.5827)  acc1: 96.8750 (96.7052)  acc5: 100.0000 (99.9175)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [170/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3348.808373410181  loss: 0.5629 (0.5817)  acc1: 97.6562 (96.7608)  acc5: 100.0000 (99.9132)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [180/468]  eta: 0:00:11  lr: 0.0016691306063588583  img/s: 3404.726617792547  loss: 0.5687 (0.5814)  acc1: 97.6562 (96.7800)  acc5: 100.0000 (99.9050)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [190/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3387.732525634958  loss: 0.5621 (0.5802)  acc1: 97.6562 (96.8136)  acc5: 100.0000 (99.9018)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [200/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3349.100840283713  loss: 0.5525 (0.5793)  acc1: 96.8750 (96.8595)  acc5: 100.0000 (99.8989)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [210/468]  eta: 0:00:10  lr: 0.0016691306063588583  img/s: 3505.157228105454  loss: 0.5715 (0.5793)  acc1: 96.8750 (96.8269)  acc5: 100.0000 (99.9037)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [220/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 2838.7243924620884  loss: 0.5872 (0.5800)  acc1: 96.8750 (96.8149)  acc5: 100.0000 (99.9081)  time: 0.0469  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [5]  [230/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 3548.808926375907  loss: 0.5927 (0.5808)  acc1: 96.0938 (96.7803)  acc5: 100.0000 (99.9087)  time: 0.0489  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [5]  [240/468]  eta: 0:00:09  lr: 0.0016691306063588583  img/s: 3558.052024998509  loss: 0.5956 (0.5810)  acc1: 96.0938 (96.7680)  acc5: 100.0000 (99.9060)  time: 0.0394  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [5]  [250/468]  eta: 0:00:08  lr: 0.0016691306063588583  img/s: 3512.0361100571745  loss: 0.5814 (0.5812)  acc1: 96.8750 (96.7661)  acc5: 100.0000 (99.9035)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [260/468]  eta: 0:00:08  lr: 0.0016691306063588583  img/s: 3292.7977233139522  loss: 0.5690 (0.5806)  acc1: 96.8750 (96.7762)  acc5: 100.0000 (99.9072)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [270/468]  eta: 0:00:07  lr: 0.0016691306063588583  img/s: 3578.6384039567793  loss: 0.5666 (0.5808)  acc1: 96.8750 (96.7626)  acc5: 100.0000 (99.9106)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [280/468]  eta: 0:00:07  lr: 0.0016691306063588583  img/s: 3536.978628086542  loss: 0.5828 (0.5810)  acc1: 96.0938 (96.7471)  acc5: 100.0000 (99.9083)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [290/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3547.8473992717563  loss: 0.5722 (0.5804)  acc1: 96.8750 (96.7730)  acc5: 100.0000 (99.9114)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [300/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3528.3778177946606  loss: 0.5726 (0.5804)  acc1: 97.6562 (96.7712)  acc5: 100.0000 (99.9143)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [310/468]  eta: 0:00:06  lr: 0.0016691306063588583  img/s: 3517.6047803753013  loss: 0.5726 (0.5806)  acc1: 96.8750 (96.7620)  acc5: 100.0000 (99.9146)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [320/468]  eta: 0:00:05  lr: 0.0016691306063588583  img/s: 3548.433634284657  loss: 0.5642 (0.5801)  acc1: 96.8750 (96.7825)  acc5: 100.0000 (99.9173)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [330/468]  eta: 0:00:05  lr: 0.0016691306063588583  img/s: 3526.8480134538577  loss: 0.5545 (0.5798)  acc1: 96.8750 (96.7782)  acc5: 100.0000 (99.9198)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [340/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3395.2740082087994  loss: 0.5802 (0.5803)  acc1: 96.0938 (96.7536)  acc5: 100.0000 (99.9198)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [350/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3578.161382555435  loss: 0.5988 (0.5803)  acc1: 96.0938 (96.7548)  acc5: 100.0000 (99.9176)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [360/468]  eta: 0:00:04  lr: 0.0016691306063588583  img/s: 3519.4264774328885  loss: 0.5805 (0.5803)  acc1: 96.8750 (96.7430)  acc5: 100.0000 (99.9178)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [370/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3484.5908483157004  loss: 0.5814 (0.5806)  acc1: 96.8750 (96.7297)  acc5: 100.0000 (99.9179)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [380/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3446.738691079981  loss: 0.5775 (0.5806)  acc1: 96.8750 (96.7315)  acc5: 100.0000 (99.9180)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [390/468]  eta: 0:00:03  lr: 0.0016691306063588583  img/s: 3408.812419441887  loss: 0.5775 (0.5808)  acc1: 96.8750 (96.7291)  acc5: 100.0000 (99.9161)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [400/468]  eta: 0:00:02  lr: 0.0016691306063588583  img/s: 3489.619052571369  loss: 0.5905 (0.5811)  acc1: 96.8750 (96.7191)  acc5: 100.0000 (99.9104)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [5]  [410/468]  eta: 0:00:02  lr: 0.0016691306063588583  img/s: 3493.979486645494  loss: 0.5783 (0.5811)  acc1: 96.8750 (96.7115)  acc5: 100.0000 (99.9088)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [420/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3499.240097767639  loss: 0.5662 (0.5808)  acc1: 96.8750 (96.7191)  acc5: 100.0000 (99.9109)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [430/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3484.4777673211097  loss: 0.5600 (0.5802)  acc1: 96.8750 (96.7391)  acc5: 100.0000 (99.9112)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [440/468]  eta: 0:00:01  lr: 0.0016691306063588583  img/s: 3520.742038717801  loss: 0.5597 (0.5799)  acc1: 97.6562 (96.7528)  acc5: 100.0000 (99.9114)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [450/468]  eta: 0:00:00  lr: 0.0016691306063588583  img/s: 3472.4877398824115  loss: 0.5598 (0.5798)  acc1: 96.8750 (96.7503)  acc5: 100.0000 (99.9134)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5]  [460/468]  eta: 0:00:00  lr: 0.0016691306063588583  img/s: 3504.310698876654  loss: 0.5630 (0.5794)  acc1: 97.6562 (96.7615)  acc5: 100.0000 (99.9153)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [5] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5471 (0.5471)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1204  data: 0.1010  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.440 Acc@5 99.980</span><br><span class="line">Epoch: [6]  [  0/468]  eta: 0:01:42  lr: 0.0015  img/s: 1579.9847319707471  loss: 0.6669 (0.6669)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.2198  data: 0.1388  max mem: 190</span><br><span class="line">Epoch: [6]  [ 10/468]  eta: 0:00:37  lr: 0.0015  img/s: 3150.1517482558515  loss: 0.5509 (0.5569)  acc1: 98.4375 (97.9403)  acc5: 100.0000 (100.0000)  time: 0.0820  data: 0.0176  max mem: 190</span><br><span class="line">Epoch: [6]  [ 20/468]  eta: 0:00:32  lr: 0.0015  img/s: 2385.509817600142  loss: 0.5436 (0.5496)  acc1: 98.4375 (98.1027)  acc5: 100.0000 (99.9256)  time: 0.0648  data: 0.0048  max mem: 190</span><br><span class="line">Epoch: [6]  [ 30/468]  eta: 0:00:28  lr: 0.0015  img/s: 3463.6386111146953  loss: 0.5402 (0.5503)  acc1: 97.6562 (98.0091)  acc5: 100.0000 (99.9496)  time: 0.0559  data: 0.0043  max mem: 190</span><br><span class="line">Epoch: [6]  [ 40/468]  eta: 0:00:24  lr: 0.0015  img/s: 3532.8589609449546  loss: 0.5531 (0.5517)  acc1: 97.6562 (97.8849)  acc5: 100.0000 (99.9428)  time: 0.0438  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [6]  [ 50/468]  eta: 0:00:22  lr: 0.0015  img/s: 3459.219793814433  loss: 0.5620 (0.5530)  acc1: 97.6562 (97.8248)  acc5: 100.0000 (99.9540)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [ 60/468]  eta: 0:00:20  lr: 0.0015  img/s: 3523.537983946655  loss: 0.5487 (0.5516)  acc1: 97.6562 (97.8996)  acc5: 100.0000 (99.9616)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 70/468]  eta: 0:00:19  lr: 0.0015  img/s: 3478.6978118459674  loss: 0.5407 (0.5519)  acc1: 98.4375 (97.9093)  acc5: 100.0000 (99.9670)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 80/468]  eta: 0:00:18  lr: 0.0015  img/s: 3461.6509791025915  loss: 0.5368 (0.5515)  acc1: 97.6562 (97.9649)  acc5: 100.0000 (99.9614)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [ 90/468]  eta: 0:00:17  lr: 0.0015  img/s: 3463.593920156899  loss: 0.5368 (0.5517)  acc1: 98.4375 (97.9653)  acc5: 100.0000 (99.9657)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [100/468]  eta: 0:00:16  lr: 0.0015  img/s: 3570.3563367449406  loss: 0.5376 (0.5519)  acc1: 97.6562 (97.9502)  acc5: 100.0000 (99.9691)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [110/468]  eta: 0:00:16  lr: 0.0015  img/s: 3560.765861488055  loss: 0.5520 (0.5527)  acc1: 97.6562 (97.8885)  acc5: 100.0000 (99.9578)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [120/468]  eta: 0:00:15  lr: 0.0015  img/s: 2785.958465226822  loss: 0.5322 (0.5515)  acc1: 97.6562 (97.9468)  acc5: 100.0000 (99.9548)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [130/468]  eta: 0:00:15  lr: 0.0015  img/s: 2802.054875025444  loss: 0.5300 (0.5520)  acc1: 97.6562 (97.8948)  acc5: 100.0000 (99.9523)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [140/468]  eta: 0:00:14  lr: 0.0015  img/s: 3535.5810547389497  loss: 0.5530 (0.5526)  acc1: 97.6562 (97.8723)  acc5: 100.0000 (99.9501)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [150/468]  eta: 0:00:13  lr: 0.0015  img/s: 3535.7207623714125  loss: 0.5523 (0.5529)  acc1: 97.6562 (97.8477)  acc5: 100.0000 (99.9483)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [160/468]  eta: 0:00:13  lr: 0.0015  img/s: 3605.6287660008866  loss: 0.5583 (0.5537)  acc1: 97.6562 (97.8164)  acc5: 100.0000 (99.9515)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [170/468]  eta: 0:00:12  lr: 0.0015  img/s: 3558.688814943458  loss: 0.5609 (0.5539)  acc1: 97.6562 (97.8207)  acc5: 100.0000 (99.9543)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [180/468]  eta: 0:00:12  lr: 0.0015  img/s: 3531.743416681468  loss: 0.5429 (0.5535)  acc1: 97.6562 (97.8289)  acc5: 100.0000 (99.9568)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [190/468]  eta: 0:00:11  lr: 0.0015  img/s: 3577.5176053522405  loss: 0.5429 (0.5534)  acc1: 97.6562 (97.8321)  acc5: 100.0000 (99.9550)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [200/468]  eta: 0:00:11  lr: 0.0015  img/s: 3349.2470928781754  loss: 0.5524 (0.5535)  acc1: 97.6562 (97.8234)  acc5: 100.0000 (99.9572)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [210/468]  eta: 0:00:10  lr: 0.0015  img/s: 3514.5421290022714  loss: 0.5593 (0.5540)  acc1: 97.6562 (97.8044)  acc5: 100.0000 (99.9556)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [220/468]  eta: 0:00:10  lr: 0.0015  img/s: 3602.0967767907464  loss: 0.5576 (0.5543)  acc1: 97.6562 (97.7835)  acc5: 100.0000 (99.9540)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [230/468]  eta: 0:00:09  lr: 0.0015  img/s: 3557.816234700031  loss: 0.5456 (0.5544)  acc1: 97.6562 (97.7746)  acc5: 100.0000 (99.9527)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [240/468]  eta: 0:00:09  lr: 0.0015  img/s: 3482.5791033932496  loss: 0.5582 (0.5549)  acc1: 96.8750 (97.7470)  acc5: 100.0000 (99.9546)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [250/468]  eta: 0:00:08  lr: 0.0015  img/s: 3449.9502753555203  loss: 0.5683 (0.5558)  acc1: 96.8750 (97.7185)  acc5: 100.0000 (99.9533)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [260/468]  eta: 0:00:08  lr: 0.0015  img/s: 3443.1134769057117  loss: 0.5683 (0.5562)  acc1: 96.8750 (97.7011)  acc5: 100.0000 (99.9551)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [270/468]  eta: 0:00:08  lr: 0.0015  img/s: 3272.545531017415  loss: 0.5686 (0.5569)  acc1: 96.8750 (97.6764)  acc5: 100.0000 (99.9539)  time: 0.0466  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [6]  [280/468]  eta: 0:00:07  lr: 0.0015  img/s: 3548.8793024808465  loss: 0.5686 (0.5571)  acc1: 96.8750 (97.6646)  acc5: 100.0000 (99.9555)  time: 0.0494  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [6]  [290/468]  eta: 0:00:07  lr: 0.0015  img/s: 3544.848908227744  loss: 0.5674 (0.5572)  acc1: 97.6562 (97.6616)  acc5: 100.0000 (99.9570)  time: 0.0401  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [6]  [300/468]  eta: 0:00:06  lr: 0.0015  img/s: 3525.4352825294677  loss: 0.5674 (0.5573)  acc1: 97.6562 (97.6640)  acc5: 100.0000 (99.9533)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [310/468]  eta: 0:00:06  lr: 0.0015  img/s: 3477.2558178697495  loss: 0.5506 (0.5572)  acc1: 97.6562 (97.6613)  acc5: 100.0000 (99.9548)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [320/468]  eta: 0:00:06  lr: 0.0015  img/s: 3415.2310892562928  loss: 0.5397 (0.5569)  acc1: 97.6562 (97.6830)  acc5: 100.0000 (99.9538)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [330/468]  eta: 0:00:05  lr: 0.0015  img/s: 3478.066792347709  loss: 0.5433 (0.5572)  acc1: 97.6562 (97.6657)  acc5: 100.0000 (99.9528)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [340/468]  eta: 0:00:05  lr: 0.0015  img/s: 2952.0947976751477  loss: 0.5600 (0.5575)  acc1: 96.8750 (97.6631)  acc5: 100.0000 (99.9496)  time: 0.0409  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [6]  [350/468]  eta: 0:00:04  lr: 0.0015  img/s: 2809.577425871074  loss: 0.5623 (0.5582)  acc1: 96.8750 (97.6295)  acc5: 100.0000 (99.9510)  time: 0.0456  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [6]  [360/468]  eta: 0:00:04  lr: 0.0015  img/s: 3442.8264385432767  loss: 0.5646 (0.5587)  acc1: 97.6562 (97.6151)  acc5: 100.0000 (99.9459)  time: 0.0457  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [6]  [370/468]  eta: 0:00:04  lr: 0.0015  img/s: 3640.8888888888887  loss: 0.5475 (0.5584)  acc1: 97.6562 (97.6183)  acc5: 100.0000 (99.9474)  time: 0.0406  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [6]  [380/468]  eta: 0:00:03  lr: 0.0015  img/s: 3591.614286956696  loss: 0.5605 (0.5589)  acc1: 96.8750 (97.5968)  acc5: 100.0000 (99.9467)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [390/468]  eta: 0:00:03  lr: 0.0015  img/s: 3500.0157245209953  loss: 0.5546 (0.5586)  acc1: 97.6562 (97.6143)  acc5: 100.0000 (99.9480)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [6]  [400/468]  eta: 0:00:02  lr: 0.0015  img/s: 3382.9508188457394  loss: 0.5446 (0.5587)  acc1: 97.6562 (97.6114)  acc5: 100.0000 (99.9493)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [410/468]  eta: 0:00:02  lr: 0.0015  img/s: 3362.062260074522  loss: 0.5734 (0.5591)  acc1: 96.8750 (97.5840)  acc5: 100.0000 (99.9487)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [420/468]  eta: 0:00:01  lr: 0.0015  img/s: 3414.861795237126  loss: 0.5567 (0.5587)  acc1: 96.8750 (97.5987)  acc5: 100.0000 (99.9499)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [430/468]  eta: 0:00:01  lr: 0.0015  img/s: 3449.7507614408905  loss: 0.5381 (0.5586)  acc1: 98.4375 (97.5964)  acc5: 100.0000 (99.9511)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [440/468]  eta: 0:00:01  lr: 0.0015  img/s: 3363.6210035649174  loss: 0.5545 (0.5587)  acc1: 97.6562 (97.5942)  acc5: 100.0000 (99.9504)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [450/468]  eta: 0:00:00  lr: 0.0015  img/s: 3469.7497689508755  loss: 0.5516 (0.5587)  acc1: 97.6562 (97.5956)  acc5: 100.0000 (99.9498)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6]  [460/468]  eta: 0:00:00  lr: 0.0015  img/s: 3430.682352339751  loss: 0.5530 (0.5589)  acc1: 97.6562 (97.5851)  acc5: 100.0000 (99.9492)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [6] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5535 (0.5535)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1226  data: 0.0983  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.780 Acc@5 99.980</span><br><span class="line">Epoch: [7]  [  0/468]  eta: 0:01:16  lr: 0.0013090169943749475  img/s: 2489.0627005174047  loss: 0.5265 (0.5265)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1624  data: 0.1110  max mem: 190</span><br><span class="line">Epoch: [7]  [ 10/468]  eta: 0:00:25  lr: 0.0013090169943749475  img/s: 3451.502838370396  loss: 0.5363 (0.5397)  acc1: 97.6562 (98.2955)  acc5: 100.0000 (100.0000)  time: 0.0547  data: 0.0117  max mem: 190</span><br><span class="line">Epoch: [7]  [ 20/468]  eta: 0:00:21  lr: 0.0013090169943749475  img/s: 3377.842518198806  loss: 0.5363 (0.5402)  acc1: 98.4375 (98.2887)  acc5: 100.0000 (100.0000)  time: 0.0429  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [7]  [ 30/468]  eta: 0:00:20  lr: 0.0013090169943749475  img/s: 1929.3512011931073  loss: 0.5474 (0.5520)  acc1: 97.6562 (97.7571)  acc5: 100.0000 (99.9496)  time: 0.0422  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [7]  [ 40/468]  eta: 0:00:20  lr: 0.0013090169943749475  img/s: 3102.421348866506  loss: 0.5683 (0.5610)  acc1: 96.0938 (97.3133)  acc5: 100.0000 (99.9428)  time: 0.0472  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [7]  [ 50/468]  eta: 0:00:19  lr: 0.0013090169943749475  img/s: 3340.3282148279036  loss: 0.5635 (0.5618)  acc1: 96.0938 (97.3039)  acc5: 100.0000 (99.9540)  time: 0.0484  data: 0.0025  max mem: 190</span><br><span class="line">Epoch: [7]  [ 60/468]  eta: 0:00:18  lr: 0.0013090169943749475  img/s: 3524.926050673968  loss: 0.5575 (0.5615)  acc1: 97.6562 (97.3361)  acc5: 100.0000 (99.9616)  time: 0.0415  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [7]  [ 70/468]  eta: 0:00:17  lr: 0.0013090169943749475  img/s: 3519.4264774328885  loss: 0.5377 (0.5585)  acc1: 98.4375 (97.5022)  acc5: 100.0000 (99.9670)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [ 80/468]  eta: 0:00:16  lr: 0.0013090169943749475  img/s: 3478.787976180448  loss: 0.5332 (0.5582)  acc1: 98.4375 (97.5598)  acc5: 100.0000 (99.9711)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [ 90/468]  eta: 0:00:16  lr: 0.0013090169943749475  img/s: 3446.9821188949027  loss: 0.5633 (0.5601)  acc1: 97.6562 (97.4931)  acc5: 100.0000 (99.9657)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [100/468]  eta: 0:00:15  lr: 0.0013090169943749475  img/s: 3419.908474749019  loss: 0.5747 (0.5619)  acc1: 97.6562 (97.4629)  acc5: 100.0000 (99.9536)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [110/468]  eta: 0:00:15  lr: 0.0013090169943749475  img/s: 3515.669853576761  loss: 0.5706 (0.5618)  acc1: 97.6562 (97.4944)  acc5: 100.0000 (99.9507)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [120/468]  eta: 0:00:14  lr: 0.0013090169943749475  img/s: 3549.489345665871  loss: 0.5600 (0.5613)  acc1: 97.6562 (97.5465)  acc5: 100.0000 (99.9548)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [130/468]  eta: 0:00:14  lr: 0.0013090169943749475  img/s: 2857.048879522753  loss: 0.5624 (0.5615)  acc1: 97.6562 (97.5310)  acc5: 100.0000 (99.9404)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [140/468]  eta: 0:00:13  lr: 0.0013090169943749475  img/s: 3495.7767894932185  loss: 0.5542 (0.5605)  acc1: 97.6562 (97.5842)  acc5: 100.0000 (99.9446)  time: 0.0405  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [150/468]  eta: 0:00:13  lr: 0.0013090169943749475  img/s: 3554.0007811413934  loss: 0.5517 (0.5603)  acc1: 98.4375 (97.6407)  acc5: 100.0000 (99.9483)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [160/468]  eta: 0:00:12  lr: 0.0013090169943749475  img/s: 3441.9214771124502  loss: 0.5565 (0.5605)  acc1: 98.4375 (97.6271)  acc5: 100.0000 (99.9369)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [170/468]  eta: 0:00:12  lr: 0.0013090169943749475  img/s: 3572.7798651733247  loss: 0.5565 (0.5605)  acc1: 97.6562 (97.6243)  acc5: 100.0000 (99.9269)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [180/468]  eta: 0:00:11  lr: 0.0013090169943749475  img/s: 3475.117561007185  loss: 0.5568 (0.5610)  acc1: 97.6562 (97.6088)  acc5: 100.0000 (99.9309)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [190/468]  eta: 0:00:11  lr: 0.0013090169943749475  img/s: 3443.1797233249745  loss: 0.5642 (0.5609)  acc1: 97.6562 (97.6194)  acc5: 100.0000 (99.9346)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [200/468]  eta: 0:00:10  lr: 0.0013090169943749475  img/s: 3543.702389438944  loss: 0.5639 (0.5611)  acc1: 97.6562 (97.6096)  acc5: 100.0000 (99.9378)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [210/468]  eta: 0:00:10  lr: 0.0013090169943749475  img/s: 3572.684762861763  loss: 0.5502 (0.5611)  acc1: 97.6562 (97.5970)  acc5: 100.0000 (99.9408)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [220/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3566.6087280022853  loss: 0.5558 (0.5613)  acc1: 96.8750 (97.5785)  acc5: 100.0000 (99.9434)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [230/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3526.6626727626253  loss: 0.5505 (0.5606)  acc1: 97.6562 (97.6089)  acc5: 100.0000 (99.9459)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [240/468]  eta: 0:00:09  lr: 0.0013090169943749475  img/s: 3495.754027269531  loss: 0.5505 (0.5605)  acc1: 98.4375 (97.6109)  acc5: 100.0000 (99.9449)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [250/468]  eta: 0:00:08  lr: 0.0013090169943749475  img/s: 3506.9660519835124  loss: 0.5430 (0.5600)  acc1: 97.6562 (97.6251)  acc5: 100.0000 (99.9471)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [260/468]  eta: 0:00:08  lr: 0.0013090169943749475  img/s: 3502.4589128676184  loss: 0.5449 (0.5598)  acc1: 98.4375 (97.6503)  acc5: 100.0000 (99.9461)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [270/468]  eta: 0:00:07  lr: 0.0013090169943749475  img/s: 3502.9845296585563  loss: 0.5531 (0.5596)  acc1: 97.6562 (97.6534)  acc5: 100.0000 (99.9452)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [280/468]  eta: 0:00:07  lr: 0.0013090169943749475  img/s: 3531.3484970071695  loss: 0.5551 (0.5595)  acc1: 97.6562 (97.6590)  acc5: 100.0000 (99.9472)  time: 0.0379  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [290/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 3533.8589012782877  loss: 0.5554 (0.5593)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (99.9463)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [300/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 3543.655608506818  loss: 0.5470 (0.5591)  acc1: 97.6562 (97.6537)  acc5: 100.0000 (99.9481)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [310/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 1957.5682104910047  loss: 0.5547 (0.5592)  acc1: 97.6562 (97.6437)  acc5: 100.0000 (99.9498)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [320/468]  eta: 0:00:06  lr: 0.0013090169943749475  img/s: 1364.188460815254  loss: 0.5524 (0.5589)  acc1: 98.4375 (97.6611)  acc5: 100.0000 (99.9489)  time: 0.0678  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [7]  [330/468]  eta: 0:00:05  lr: 0.0013090169943749475  img/s: 3207.1908050371576  loss: 0.5477 (0.5592)  acc1: 97.6562 (97.6586)  acc5: 100.0000 (99.9410)  time: 0.0775  data: 0.0048  max mem: 190</span><br><span class="line">Epoch: [7]  [340/468]  eta: 0:00:05  lr: 0.0013090169943749475  img/s: 3364.8437321767688  loss: 0.5512 (0.5589)  acc1: 97.6562 (97.6631)  acc5: 100.0000 (99.9427)  time: 0.0515  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [7]  [350/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3425.0574935565364  loss: 0.5448 (0.5588)  acc1: 97.6562 (97.6629)  acc5: 100.0000 (99.9444)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [360/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3450.127640432109  loss: 0.5366 (0.5583)  acc1: 98.4375 (97.6887)  acc5: 100.0000 (99.9459)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [370/468]  eta: 0:00:04  lr: 0.0013090169943749475  img/s: 3478.201991538875  loss: 0.5509 (0.5586)  acc1: 97.6562 (97.6773)  acc5: 100.0000 (99.9410)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [380/468]  eta: 0:00:03  lr: 0.0013090169943749475  img/s: 3473.7684373989  loss: 0.5645 (0.5588)  acc1: 96.8750 (97.6686)  acc5: 100.0000 (99.9426)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [390/468]  eta: 0:00:03  lr: 0.0013090169943749475  img/s: 3551.344225858944  loss: 0.5611 (0.5590)  acc1: 96.8750 (97.6443)  acc5: 100.0000 (99.9441)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [400/468]  eta: 0:00:02  lr: 0.0013090169943749475  img/s: 3478.990863023108  loss: 0.5611 (0.5593)  acc1: 96.8750 (97.6387)  acc5: 100.0000 (99.9454)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [410/468]  eta: 0:00:02  lr: 0.0013090169943749475  img/s: 3477.931603666634  loss: 0.5499 (0.5592)  acc1: 97.6562 (97.6448)  acc5: 100.0000 (99.9468)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [420/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3323.3109373742627  loss: 0.5408 (0.5588)  acc1: 98.4375 (97.6581)  acc5: 100.0000 (99.9480)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [430/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3464.0632327416556  loss: 0.5350 (0.5586)  acc1: 97.6562 (97.6581)  acc5: 100.0000 (99.9492)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [7]  [440/468]  eta: 0:00:01  lr: 0.0013090169943749475  img/s: 3507.7221895540138  loss: 0.5480 (0.5584)  acc1: 97.6562 (97.6704)  acc5: 100.0000 (99.9504)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [450/468]  eta: 0:00:00  lr: 0.0013090169943749475  img/s: 3492.6611239054346  loss: 0.5523 (0.5584)  acc1: 97.6562 (97.6649)  acc5: 100.0000 (99.9515)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7]  [460/468]  eta: 0:00:00  lr: 0.0013090169943749475  img/s: 3556.143021792409  loss: 0.5525 (0.5584)  acc1: 97.6562 (97.6613)  acc5: 100.0000 (99.9509)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [7] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5532 (0.5532)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1250  data: 0.1056  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 97.530 Acc@5 99.950</span><br><span class="line">Epoch: [8]  [  0/468]  eta: 0:01:36  lr: 0.0011045284632676536  img/s: 2262.6431328916537  loss: 0.5521 (0.5521)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.2056  data: 0.1490  max mem: 190</span><br><span class="line">Epoch: [8]  [ 10/468]  eta: 0:00:23  lr: 0.0011045284632676536  img/s: 3470.736735947248  loss: 0.5344 (0.5478)  acc1: 98.4375 (97.8693)  acc5: 100.0000 (100.0000)  time: 0.0523  data: 0.0138  max mem: 190</span><br><span class="line">Epoch: [8]  [ 20/468]  eta: 0:00:20  lr: 0.0011045284632676536  img/s: 3469.368591110594  loss: 0.5449 (0.5517)  acc1: 97.6562 (97.7679)  acc5: 100.0000 (99.9256)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 30/468]  eta: 0:00:18  lr: 0.0011045284632676536  img/s: 3585.66532422342  loss: 0.5611 (0.5518)  acc1: 97.6562 (97.7823)  acc5: 100.0000 (99.9496)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 40/468]  eta: 0:00:17  lr: 0.0011045284632676536  img/s: 3444.195821064044  loss: 0.5630 (0.5562)  acc1: 96.8750 (97.5800)  acc5: 100.0000 (99.9619)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 50/468]  eta: 0:00:16  lr: 0.0011045284632676536  img/s: 3318.9554337007526  loss: 0.5700 (0.5597)  acc1: 96.8750 (97.4877)  acc5: 100.0000 (99.9387)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [ 60/468]  eta: 0:00:16  lr: 0.0011045284632676536  img/s: 3511.8293507767785  loss: 0.5537 (0.5556)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (99.9488)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [ 70/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3525.57418948115  loss: 0.5447 (0.5569)  acc1: 97.6562 (97.6012)  acc5: 100.0000 (99.9450)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [ 80/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 2674.5390018631624  loss: 0.5500 (0.5547)  acc1: 97.6562 (97.6852)  acc5: 100.0000 (99.9518)  time: 0.0420  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [8]  [ 90/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3554.4243162543116  loss: 0.5306 (0.5527)  acc1: 98.4375 (97.7850)  acc5: 100.0000 (99.9571)  time: 0.0483  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [8]  [100/468]  eta: 0:00:15  lr: 0.0011045284632676536  img/s: 3394.6514239465832  loss: 0.5306 (0.5515)  acc1: 98.4375 (97.8342)  acc5: 100.0000 (99.9536)  time: 0.0435  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [8]  [110/468]  eta: 0:00:14  lr: 0.0011045284632676536  img/s: 3309.278761280142  loss: 0.5341 (0.5502)  acc1: 98.4375 (97.8885)  acc5: 100.0000 (99.9578)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [120/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3566.2059730045703  loss: 0.5408 (0.5502)  acc1: 98.4375 (97.8758)  acc5: 100.0000 (99.9613)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [130/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3405.6121235957194  loss: 0.5434 (0.5499)  acc1: 97.6562 (97.8769)  acc5: 100.0000 (99.9642)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [140/468]  eta: 0:00:13  lr: 0.0011045284632676536  img/s: 3592.7439370415973  loss: 0.5348 (0.5492)  acc1: 97.6562 (97.9000)  acc5: 100.0000 (99.9612)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [150/468]  eta: 0:00:12  lr: 0.0011045284632676536  img/s: 3486.876656989394  loss: 0.5285 (0.5483)  acc1: 98.4375 (97.9408)  acc5: 100.0000 (99.9534)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [160/468]  eta: 0:00:12  lr: 0.0011045284632676536  img/s: 3574.635372763651  loss: 0.5372 (0.5475)  acc1: 98.4375 (97.9862)  acc5: 100.0000 (99.9515)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [170/468]  eta: 0:00:11  lr: 0.0011045284632676536  img/s: 3459.509572322424  loss: 0.5390 (0.5475)  acc1: 98.4375 (97.9989)  acc5: 100.0000 (99.9543)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [180/468]  eta: 0:00:11  lr: 0.0011045284632676536  img/s: 3559.8686576664986  loss: 0.5453 (0.5478)  acc1: 98.4375 (98.0016)  acc5: 100.0000 (99.9568)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [190/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3072.609495902202  loss: 0.5619 (0.5487)  acc1: 97.6562 (97.9917)  acc5: 100.0000 (99.9591)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [200/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3079.9600254717143  loss: 0.5619 (0.5487)  acc1: 97.6562 (97.9711)  acc5: 100.0000 (99.9611)  time: 0.0421  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [210/468]  eta: 0:00:10  lr: 0.0011045284632676536  img/s: 3520.095675207847  loss: 0.5388 (0.5484)  acc1: 97.6562 (98.0006)  acc5: 100.0000 (99.9630)  time: 0.0402  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [220/468]  eta: 0:00:09  lr: 0.0011045284632676536  img/s: 3515.3705908159322  loss: 0.5334 (0.5481)  acc1: 98.4375 (98.0098)  acc5: 100.0000 (99.9646)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [230/468]  eta: 0:00:09  lr: 0.0011045284632676536  img/s: 3547.4488700938286  loss: 0.5408 (0.5483)  acc1: 98.4375 (97.9978)  acc5: 100.0000 (99.9628)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [240/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3518.2271735355216  loss: 0.5406 (0.5481)  acc1: 97.6562 (97.9999)  acc5: 100.0000 (99.9611)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [250/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3403.798411178809  loss: 0.5354 (0.5477)  acc1: 98.4375 (98.0142)  acc5: 100.0000 (99.9626)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [260/468]  eta: 0:00:08  lr: 0.0011045284632676536  img/s: 3418.7324851309872  loss: 0.5354 (0.5476)  acc1: 98.4375 (98.0244)  acc5: 100.0000 (99.9581)  time: 0.0415  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [8]  [270/468]  eta: 0:00:07  lr: 0.0011045284632676536  img/s: 3439.9366438136735  loss: 0.5399 (0.5473)  acc1: 98.4375 (98.0339)  acc5: 100.0000 (99.9596)  time: 0.0417  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [8]  [280/468]  eta: 0:00:07  lr: 0.0011045284632676536  img/s: 3443.4005631345685  loss: 0.5378 (0.5470)  acc1: 98.4375 (98.0344)  acc5: 100.0000 (99.9611)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [290/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3593.2248547640083  loss: 0.5373 (0.5468)  acc1: 97.6562 (98.0402)  acc5: 100.0000 (99.9597)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [300/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3540.967780657842  loss: 0.5535 (0.5469)  acc1: 97.6562 (98.0456)  acc5: 100.0000 (99.9611)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [310/468]  eta: 0:00:06  lr: 0.0011045284632676536  img/s: 3562.325238208987  loss: 0.5366 (0.5469)  acc1: 98.4375 (98.0481)  acc5: 100.0000 (99.9623)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [320/468]  eta: 0:00:05  lr: 0.0011045284632676536  img/s: 3435.7319612699266  loss: 0.5502 (0.5472)  acc1: 96.8750 (98.0262)  acc5: 100.0000 (99.9635)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [330/468]  eta: 0:00:05  lr: 0.0011045284632676536  img/s: 3478.8105179943755  loss: 0.5403 (0.5468)  acc1: 97.6562 (98.0433)  acc5: 100.0000 (99.9646)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [340/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3482.1725160043325  loss: 0.5320 (0.5465)  acc1: 98.4375 (98.0595)  acc5: 100.0000 (99.9656)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [350/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3570.237620865309  loss: 0.5320 (0.5463)  acc1: 98.4375 (98.0747)  acc5: 100.0000 (99.9666)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [360/468]  eta: 0:00:04  lr: 0.0011045284632676536  img/s: 3473.813391308849  loss: 0.5514 (0.5467)  acc1: 98.4375 (98.0674)  acc5: 100.0000 (99.9675)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [370/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3036.1365176132604  loss: 0.5527 (0.5467)  acc1: 97.6562 (98.0711)  acc5: 100.0000 (99.9684)  time: 0.0452  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [8]  [380/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3556.378590355061  loss: 0.5467 (0.5466)  acc1: 97.6562 (98.0746)  acc5: 100.0000 (99.9692)  time: 0.0491  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [8]  [390/468]  eta: 0:00:03  lr: 0.0011045284632676536  img/s: 3614.294450690382  loss: 0.5329 (0.5464)  acc1: 98.4375 (98.0818)  acc5: 100.0000 (99.9700)  time: 0.0408  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [8]  [400/468]  eta: 0:00:02  lr: 0.0011045284632676536  img/s: 3452.4350471045946  loss: 0.5323 (0.5460)  acc1: 99.2188 (98.1024)  acc5: 100.0000 (99.9708)  time: 0.0368  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [410/468]  eta: 0:00:02  lr: 0.0011045284632676536  img/s: 3487.9640334977034  loss: 0.5392 (0.5461)  acc1: 98.4375 (98.1049)  acc5: 100.0000 (99.9696)  time: 0.0366  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [8]  [420/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3611.0611942908645  loss: 0.5458 (0.5460)  acc1: 98.4375 (98.1072)  acc5: 100.0000 (99.9703)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [430/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3563.412884469873  loss: 0.5407 (0.5459)  acc1: 98.4375 (98.1130)  acc5: 100.0000 (99.9710)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [8]  [440/468]  eta: 0:00:01  lr: 0.0011045284632676536  img/s: 3511.278111694648  loss: 0.5311 (0.5457)  acc1: 98.4375 (98.1257)  acc5: 100.0000 (99.9699)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [450/468]  eta: 0:00:00  lr: 0.0011045284632676536  img/s: 3517.4895465475106  loss: 0.5311 (0.5458)  acc1: 99.2188 (98.1257)  acc5: 100.0000 (99.9688)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8]  [460/468]  eta: 0:00:00  lr: 0.0011045284632676536  img/s: 3581.455421172358  loss: 0.5253 (0.5455)  acc1: 99.2188 (98.1375)  acc5: 100.0000 (99.9695)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [8] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5504 (0.5504)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1249  data: 0.1060  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.000 Acc@5 99.970</span><br><span class="line">Epoch: [9]  [  0/468]  eta: 0:01:14  lr: 0.0008954715367323467  img/s: 2598.3995043946256  loss: 0.5086 (0.5086)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1588  data: 0.1095  max mem: 190</span><br><span class="line">Epoch: [9]  [ 10/468]  eta: 0:00:23  lr: 0.0008954715367323467  img/s: 2439.9008898462994  loss: 0.5330 (0.5354)  acc1: 98.4375 (98.5085)  acc5: 100.0000 (99.9290)  time: 0.0515  data: 0.0102  max mem: 190</span><br><span class="line">Epoch: [9]  [ 20/468]  eta: 0:00:20  lr: 0.0008954715367323467  img/s: 2203.5869723151436  loss: 0.5330 (0.5314)  acc1: 98.4375 (98.7351)  acc5: 100.0000 (99.9628)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [ 30/468]  eta: 0:00:19  lr: 0.0008954715367323467  img/s: 3566.5376469806683  loss: 0.5352 (0.5309)  acc1: 98.4375 (98.6895)  acc5: 100.0000 (99.9748)  time: 0.0395  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [9]  [ 40/468]  eta: 0:00:18  lr: 0.0008954715367323467  img/s: 3491.2756429848805  loss: 0.5292 (0.5306)  acc1: 98.4375 (98.7424)  acc5: 100.0000 (99.9809)  time: 0.0376  data: 0.0009  max mem: 190</span><br><span class="line">Epoch: [9]  [ 50/468]  eta: 0:00:17  lr: 0.0008954715367323467  img/s: 3387.283666464769  loss: 0.5334 (0.5317)  acc1: 98.4375 (98.6520)  acc5: 100.0000 (99.9847)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 60/468]  eta: 0:00:16  lr: 0.0008954715367323467  img/s: 3530.6517953439434  loss: 0.5342 (0.5321)  acc1: 98.4375 (98.6680)  acc5: 100.0000 (99.9872)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 70/468]  eta: 0:00:15  lr: 0.0008954715367323467  img/s: 3613.8078768990517  loss: 0.5293 (0.5325)  acc1: 98.4375 (98.6356)  acc5: 100.0000 (99.9890)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 80/468]  eta: 0:00:15  lr: 0.0008954715367323467  img/s: 3584.276876856828  loss: 0.5376 (0.5332)  acc1: 98.4375 (98.5822)  acc5: 100.0000 (99.9904)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [ 90/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 3455.723990550796  loss: 0.5376 (0.5334)  acc1: 98.4375 (98.5834)  acc5: 100.0000 (99.9914)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [100/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 3509.9925599032395  loss: 0.5252 (0.5334)  acc1: 99.2188 (98.5767)  acc5: 100.0000 (99.9845)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [110/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2140.45439576431  loss: 0.5335 (0.5337)  acc1: 98.4375 (98.5642)  acc5: 100.0000 (99.9859)  time: 0.0437  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [9]  [120/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2856.7904303775913  loss: 0.5341 (0.5336)  acc1: 98.4375 (98.5731)  acc5: 100.0000 (99.9871)  time: 0.0544  data: 0.0045  max mem: 190</span><br><span class="line">Epoch: [9]  [130/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 1459.1936682403655  loss: 0.5305 (0.5335)  acc1: 99.2188 (98.5866)  acc5: 100.0000 (99.9881)  time: 0.0578  data: 0.0061  max mem: 190</span><br><span class="line">Epoch: [9]  [140/468]  eta: 0:00:14  lr: 0.0008954715367323467  img/s: 2368.293897860965  loss: 0.5229 (0.5326)  acc1: 99.2188 (98.6370)  acc5: 100.0000 (99.9889)  time: 0.0550  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [9]  [150/468]  eta: 0:00:13  lr: 0.0008954715367323467  img/s: 3473.1841424283202  loss: 0.5229 (0.5323)  acc1: 99.2188 (98.6445)  acc5: 100.0000 (99.9897)  time: 0.0454  data: 0.0028  max mem: 190</span><br><span class="line">Epoch: [9]  [160/468]  eta: 0:00:13  lr: 0.0008954715367323467  img/s: 3528.6097221126797  loss: 0.5357 (0.5330)  acc1: 98.4375 (98.6122)  acc5: 100.0000 (99.9903)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [170/468]  eta: 0:00:12  lr: 0.0008954715367323467  img/s: 3583.1286298745936  loss: 0.5247 (0.5322)  acc1: 98.4375 (98.6431)  acc5: 100.0000 (99.9909)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [180/468]  eta: 0:00:12  lr: 0.0008954715367323467  img/s: 3462.253727493164  loss: 0.5234 (0.5326)  acc1: 99.2188 (98.6360)  acc5: 100.0000 (99.9871)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [190/468]  eta: 0:00:11  lr: 0.0008954715367323467  img/s: 3527.1955797620376  loss: 0.5289 (0.5326)  acc1: 98.4375 (98.6420)  acc5: 100.0000 (99.9877)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [200/468]  eta: 0:00:11  lr: 0.0008954715367323467  img/s: 3514.059039914124  loss: 0.5289 (0.5330)  acc1: 98.4375 (98.6435)  acc5: 100.0000 (99.9883)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [210/468]  eta: 0:00:10  lr: 0.0008954715367323467  img/s: 3603.982868574037  loss: 0.5418 (0.5334)  acc1: 98.4375 (98.6226)  acc5: 100.0000 (99.9852)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [220/468]  eta: 0:00:10  lr: 0.0008954715367323467  img/s: 3533.626306505542  loss: 0.5405 (0.5335)  acc1: 98.4375 (98.6178)  acc5: 100.0000 (99.9859)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [230/468]  eta: 0:00:09  lr: 0.0008954715367323467  img/s: 3551.649645080411  loss: 0.5366 (0.5336)  acc1: 98.4375 (98.6134)  acc5: 100.0000 (99.9831)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [240/468]  eta: 0:00:09  lr: 0.0008954715367323467  img/s: 3531.1626830134574  loss: 0.5348 (0.5340)  acc1: 98.4375 (98.5996)  acc5: 100.0000 (99.9838)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [250/468]  eta: 0:00:08  lr: 0.0008954715367323467  img/s: 3386.557194221914  loss: 0.5304 (0.5338)  acc1: 98.4375 (98.6118)  acc5: 100.0000 (99.9844)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [260/468]  eta: 0:00:08  lr: 0.0008954715367323467  img/s: 3556.143021792409  loss: 0.5290 (0.5342)  acc1: 98.4375 (98.5961)  acc5: 100.0000 (99.9790)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [270/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3552.77780204218  loss: 0.5240 (0.5341)  acc1: 98.4375 (98.6076)  acc5: 100.0000 (99.9798)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [280/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3469.7273444063853  loss: 0.5242 (0.5341)  acc1: 98.4375 (98.6043)  acc5: 100.0000 (99.9778)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [290/468]  eta: 0:00:07  lr: 0.0008954715367323467  img/s: 3458.595820341691  loss: 0.5285 (0.5343)  acc1: 98.4375 (98.6013)  acc5: 100.0000 (99.9758)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [300/468]  eta: 0:00:06  lr: 0.0008954715367323467  img/s: 3508.5245100281663  loss: 0.5416 (0.5351)  acc1: 98.4375 (98.5751)  acc5: 100.0000 (99.9740)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [310/468]  eta: 0:00:06  lr: 0.0008954715367323467  img/s: 3411.8886325649974  loss: 0.5515 (0.5356)  acc1: 97.6562 (98.5480)  acc5: 100.0000 (99.9749)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [320/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3536.535943665312  loss: 0.5444 (0.5360)  acc1: 97.6562 (98.5349)  acc5: 100.0000 (99.9757)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [330/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3589.9091407556  loss: 0.5402 (0.5362)  acc1: 98.4375 (98.5272)  acc5: 100.0000 (99.9740)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [340/468]  eta: 0:00:05  lr: 0.0008954715367323467  img/s: 3572.2093272384905  loss: 0.5437 (0.5365)  acc1: 98.4375 (98.5085)  acc5: 100.0000 (99.9725)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [350/468]  eta: 0:00:04  lr: 0.0008954715367323467  img/s: 3516.4758143221134  loss: 0.5372 (0.5365)  acc1: 98.4375 (98.4976)  acc5: 100.0000 (99.9733)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [360/468]  eta: 0:00:04  lr: 0.0008954715367323467  img/s: 3577.255392160129  loss: 0.5323 (0.5364)  acc1: 98.4375 (98.4981)  acc5: 100.0000 (99.9740)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [370/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3451.6137892016304  loss: 0.5394 (0.5369)  acc1: 97.6562 (98.4712)  acc5: 100.0000 (99.9747)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [380/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3403.3884345720335  loss: 0.5448 (0.5369)  acc1: 98.4375 (98.4724)  acc5: 100.0000 (99.9754)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [390/468]  eta: 0:00:03  lr: 0.0008954715367323467  img/s: 3540.5707954680347  loss: 0.5286 (0.5366)  acc1: 98.4375 (98.4895)  acc5: 100.0000 (99.9760)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [9]  [400/468]  eta: 0:00:02  lr: 0.0008954715367323467  img/s: 3487.0804884385557  loss: 0.5253 (0.5365)  acc1: 99.2188 (98.4901)  acc5: 100.0000 (99.9747)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [410/468]  eta: 0:00:02  lr: 0.0008954715367323467  img/s: 3550.0291741056667  loss: 0.5383 (0.5368)  acc1: 98.4375 (98.4793)  acc5: 100.0000 (99.9734)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [420/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 1396.214262494181  loss: 0.5412 (0.5369)  acc1: 98.4375 (98.4765)  acc5: 100.0000 (99.9740)  time: 0.0427  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9]  [430/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 3218.3997170484313  loss: 0.5426 (0.5372)  acc1: 97.6562 (98.4665)  acc5: 100.0000 (99.9746)  time: 0.0484  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [9]  [440/468]  eta: 0:00:01  lr: 0.0008954715367323467  img/s: 3407.51427755387  loss: 0.5414 (0.5373)  acc1: 97.6562 (98.4588)  acc5: 100.0000 (99.9752)  time: 0.0437  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [9]  [450/468]  eta: 0:00:00  lr: 0.0008954715367323467  img/s: 3551.7906255168537  loss: 0.5339 (0.5370)  acc1: 98.4375 (98.4618)  acc5: 100.0000 (99.9757)  time: 0.0384  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [9]  [460/468]  eta: 0:00:00  lr: 0.0008954715367323467  img/s: 3539.6604009942444  loss: 0.5245 (0.5369)  acc1: 98.4375 (98.4697)  acc5: 100.0000 (99.9746)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [9] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:11  loss: 0.5483 (0.5483)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1414  data: 0.1171  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.280 Acc@5 99.960</span><br><span class="line">Epoch: [10]  [  0/468]  eta: 0:01:13  lr: 0.0006909830056250527  img/s: 2420.8236928016163  loss: 0.5375 (0.5375)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1576  data: 0.1046  max mem: 190</span><br><span class="line">Epoch: [10]  [ 10/468]  eta: 0:00:24  lr: 0.0006909830056250527  img/s: 3436.0398087643284  loss: 0.5144 (0.5181)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.0530  data: 0.0104  max mem: 190</span><br><span class="line">Epoch: [10]  [ 20/468]  eta: 0:00:21  lr: 0.0006909830056250527  img/s: 3524.069946962138  loss: 0.5215 (0.5234)  acc1: 99.2188 (99.1071)  acc5: 100.0000 (100.0000)  time: 0.0421  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [10]  [ 30/468]  eta: 0:00:19  lr: 0.0006909830056250527  img/s: 3538.447269731422  loss: 0.5242 (0.5217)  acc1: 99.2188 (99.1431)  acc5: 100.0000 (100.0000)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 40/468]  eta: 0:00:18  lr: 0.0006909830056250527  img/s: 3512.8633906955442  loss: 0.5161 (0.5239)  acc1: 99.2188 (99.0091)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 50/468]  eta: 0:00:17  lr: 0.0006909830056250527  img/s: 3348.3280029936386  loss: 0.5179 (0.5234)  acc1: 99.2188 (99.0656)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [ 60/468]  eta: 0:00:16  lr: 0.0006909830056250527  img/s: 3320.2280314415234  loss: 0.5163 (0.5226)  acc1: 99.2188 (99.0907)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 70/468]  eta: 0:00:16  lr: 0.0006909830056250527  img/s: 3438.5706453513694  loss: 0.5123 (0.5220)  acc1: 99.2188 (99.1087)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 80/468]  eta: 0:00:15  lr: 0.0006909830056250527  img/s: 3534.6034103627626  loss: 0.5209 (0.5231)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [ 90/468]  eta: 0:00:15  lr: 0.0006909830056250527  img/s: 3577.0170498837356  loss: 0.5209 (0.5231)  acc1: 99.2188 (99.0556)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [100/468]  eta: 0:00:14  lr: 0.0006909830056250527  img/s: 3524.069946962138  loss: 0.5268 (0.5248)  acc1: 98.4375 (99.0022)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [110/468]  eta: 0:00:14  lr: 0.0006909830056250527  img/s: 3550.2639333421507  loss: 0.5263 (0.5245)  acc1: 99.2188 (99.0217)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [120/468]  eta: 0:00:13  lr: 0.0006909830056250527  img/s: 3530.9072206985907  loss: 0.5166 (0.5249)  acc1: 99.2188 (99.0121)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [130/468]  eta: 0:00:13  lr: 0.0006909830056250527  img/s: 3610.624055093751  loss: 0.5242 (0.5256)  acc1: 99.2188 (98.9742)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [140/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 3567.912382370142  loss: 0.5208 (0.5253)  acc1: 99.2188 (99.0082)  acc5: 100.0000 (99.9945)  time: 0.0363  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [150/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 2762.3779244768484  loss: 0.5192 (0.5255)  acc1: 99.2188 (99.0014)  acc5: 100.0000 (99.9948)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [160/468]  eta: 0:00:12  lr: 0.0006909830056250527  img/s: 3191.291160910658  loss: 0.5192 (0.5253)  acc1: 99.2188 (99.0149)  acc5: 100.0000 (99.9951)  time: 0.0430  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [170/468]  eta: 0:00:11  lr: 0.0006909830056250527  img/s: 3518.8267232960393  loss: 0.5229 (0.5257)  acc1: 98.4375 (98.9766)  acc5: 100.0000 (99.9954)  time: 0.0413  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [180/468]  eta: 0:00:11  lr: 0.0006909830056250527  img/s: 3413.4939311669073  loss: 0.5251 (0.5263)  acc1: 98.4375 (98.9555)  acc5: 100.0000 (99.9957)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [190/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 1556.440813486599  loss: 0.5210 (0.5262)  acc1: 99.2188 (98.9570)  acc5: 100.0000 (99.9959)  time: 0.0407  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [200/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 2556.966489493437  loss: 0.5269 (0.5272)  acc1: 98.4375 (98.9195)  acc5: 100.0000 (99.9961)  time: 0.0485  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [10]  [210/468]  eta: 0:00:10  lr: 0.0006909830056250527  img/s: 3502.276126608042  loss: 0.5357 (0.5273)  acc1: 98.4375 (98.9114)  acc5: 100.0000 (99.9963)  time: 0.0458  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [10]  [220/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3562.79804629433  loss: 0.5338 (0.5279)  acc1: 98.4375 (98.8758)  acc5: 100.0000 (99.9965)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [230/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3597.125038525963  loss: 0.5187 (0.5277)  acc1: 99.2188 (98.8907)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [240/468]  eta: 0:00:09  lr: 0.0006909830056250527  img/s: 3483.7994354498555  loss: 0.5226 (0.5282)  acc1: 99.2188 (98.8654)  acc5: 100.0000 (99.9968)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [250/468]  eta: 0:00:08  lr: 0.0006909830056250527  img/s: 3499.057647311856  loss: 0.5308 (0.5282)  acc1: 98.4375 (98.8546)  acc5: 100.0000 (99.9969)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [260/468]  eta: 0:00:08  lr: 0.0006909830056250527  img/s: 3570.4988062222756  loss: 0.5202 (0.5281)  acc1: 99.2188 (98.8685)  acc5: 100.0000 (99.9940)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [270/468]  eta: 0:00:07  lr: 0.0006909830056250527  img/s: 3405.525712509594  loss: 0.5211 (0.5282)  acc1: 99.2188 (98.8555)  acc5: 100.0000 (99.9942)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [280/468]  eta: 0:00:07  lr: 0.0006909830056250527  img/s: 3477.5711518904527  loss: 0.5305 (0.5286)  acc1: 98.4375 (98.8295)  acc5: 100.0000 (99.9944)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [290/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3542.112530349414  loss: 0.5300 (0.5288)  acc1: 98.4375 (98.8187)  acc5: 100.0000 (99.9946)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [300/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3544.1234734159834  loss: 0.5232 (0.5288)  acc1: 99.2188 (98.8268)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [310/468]  eta: 0:00:06  lr: 0.0006909830056250527  img/s: 3441.08315707162  loss: 0.5262 (0.5292)  acc1: 99.2188 (98.8118)  acc5: 100.0000 (99.9950)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [320/468]  eta: 0:00:05  lr: 0.0006909830056250527  img/s: 3481.9466751412247  loss: 0.5417 (0.5296)  acc1: 98.4375 (98.7977)  acc5: 100.0000 (99.9951)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [330/468]  eta: 0:00:05  lr: 0.0006909830056250527  img/s: 3582.1245170975812  loss: 0.5399 (0.5302)  acc1: 97.6562 (98.7632)  acc5: 100.0000 (99.9953)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [340/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3493.865795484866  loss: 0.5399 (0.5307)  acc1: 97.6562 (98.7422)  acc5: 100.0000 (99.9954)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [350/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3524.671489909269  loss: 0.5373 (0.5309)  acc1: 98.4375 (98.7313)  acc5: 100.0000 (99.9955)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [360/468]  eta: 0:00:04  lr: 0.0006909830056250527  img/s: 3499.719120753044  loss: 0.5248 (0.5307)  acc1: 98.4375 (98.7383)  acc5: 100.0000 (99.9957)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [370/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3413.9063461783035  loss: 0.5168 (0.5305)  acc1: 99.2188 (98.7449)  acc5: 100.0000 (99.9958)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [380/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3589.9571508813224  loss: 0.5213 (0.5303)  acc1: 99.2188 (98.7533)  acc5: 100.0000 (99.9959)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [390/468]  eta: 0:00:03  lr: 0.0006909830056250527  img/s: 3470.4450736273257  loss: 0.5237 (0.5301)  acc1: 99.2188 (98.7612)  acc5: 100.0000 (99.9960)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10]  [400/468]  eta: 0:00:02  lr: 0.0006909830056250527  img/s: 3534.463791014905  loss: 0.5256 (0.5300)  acc1: 99.2188 (98.7629)  acc5: 100.0000 (99.9961)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [10]  [410/468]  eta: 0:00:02  lr: 0.0006909830056250527  img/s: 2385.032927587739  loss: 0.5269 (0.5300)  acc1: 98.4375 (98.7625)  acc5: 100.0000 (99.9962)  time: 0.0418  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [10]  [420/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 1669.8991660938293  loss: 0.5245 (0.5301)  acc1: 98.4375 (98.7622)  acc5: 100.0000 (99.9963)  time: 0.0561  data: 0.0036  max mem: 190</span><br><span class="line">Epoch: [10]  [430/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 3159.8113780561016  loss: 0.5219 (0.5300)  acc1: 98.4375 (98.7583)  acc5: 100.0000 (99.9964)  time: 0.0600  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [10]  [440/468]  eta: 0:00:01  lr: 0.0006909830056250527  img/s: 3148.636799230539  loss: 0.5261 (0.5301)  acc1: 98.4375 (98.7564)  acc5: 100.0000 (99.9929)  time: 0.0488  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [10]  [450/468]  eta: 0:00:00  lr: 0.0006909830056250527  img/s: 3290.860071104573  loss: 0.5279 (0.5301)  acc1: 98.4375 (98.7562)  acc5: 100.0000 (99.9931)  time: 0.0413  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [10]  [460/468]  eta: 0:00:00  lr: 0.0006909830056250527  img/s: 1988.366599260757  loss: 0.5279 (0.5301)  acc1: 99.2188 (98.7578)  acc5: 100.0000 (99.9932)  time: 0.0415  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [10] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5544 (0.5544)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1328  data: 0.1155  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.270 Acc@5 99.970</span><br><span class="line">Epoch: [11]  [  0/468]  eta: 0:01:20  lr: 0.0005000000000000002  img/s: 2074.494628974173  loss: 0.5164 (0.5164)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1717  data: 0.1100  max mem: 190</span><br><span class="line">Epoch: [11]  [ 10/468]  eta: 0:00:25  lr: 0.0005000000000000002  img/s: 3200.5181197651195  loss: 0.5164 (0.5185)  acc1: 99.2188 (99.1477)  acc5: 100.0000 (100.0000)  time: 0.0554  data: 0.0103  max mem: 190</span><br><span class="line">Epoch: [11]  [ 20/468]  eta: 0:00:21  lr: 0.0005000000000000002  img/s: 3149.2278254544603  loss: 0.5217 (0.5232)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (100.0000)  time: 0.0423  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [ 30/468]  eta: 0:00:20  lr: 0.0005000000000000002  img/s: 3227.5514728868584  loss: 0.5288 (0.5235)  acc1: 99.2188 (98.9919)  acc5: 100.0000 (100.0000)  time: 0.0406  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [ 40/468]  eta: 0:00:19  lr: 0.0005000000000000002  img/s: 3470.4226400946354  loss: 0.5152 (0.5219)  acc1: 99.2188 (99.0663)  acc5: 100.0000 (99.9809)  time: 0.0402  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 50/468]  eta: 0:00:18  lr: 0.0005000000000000002  img/s: 3297.773387879458  loss: 0.5106 (0.5212)  acc1: 99.2188 (99.0656)  acc5: 100.0000 (99.9847)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 60/468]  eta: 0:00:17  lr: 0.0005000000000000002  img/s: 3524.5095158378467  loss: 0.5191 (0.5218)  acc1: 99.2188 (99.0523)  acc5: 100.0000 (99.9872)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 70/468]  eta: 0:00:16  lr: 0.0005000000000000002  img/s: 3443.8423277504444  loss: 0.5181 (0.5218)  acc1: 99.2188 (99.0427)  acc5: 100.0000 (99.9890)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 80/468]  eta: 0:00:15  lr: 0.0005000000000000002  img/s: 3546.980126849894  loss: 0.5163 (0.5221)  acc1: 99.2188 (99.0355)  acc5: 100.0000 (99.9904)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [ 90/468]  eta: 0:00:15  lr: 0.0005000000000000002  img/s: 3493.1610753975483  loss: 0.5179 (0.5222)  acc1: 99.2188 (99.0299)  acc5: 100.0000 (99.9914)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [100/468]  eta: 0:00:14  lr: 0.0005000000000000002  img/s: 3373.745770806626  loss: 0.5201 (0.5224)  acc1: 99.2188 (99.0176)  acc5: 100.0000 (99.9923)  time: 0.0377  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [110/468]  eta: 0:00:14  lr: 0.0005000000000000002  img/s: 3415.90471342767  loss: 0.5171 (0.5224)  acc1: 99.2188 (99.0146)  acc5: 100.0000 (99.9930)  time: 0.0377  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [11]  [120/468]  eta: 0:00:13  lr: 0.0005000000000000002  img/s: 3553.130498087334  loss: 0.5166 (0.5222)  acc1: 99.2188 (99.0121)  acc5: 100.0000 (99.9935)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [130/468]  eta: 0:00:13  lr: 0.0005000000000000002  img/s: 3494.0249651815116  loss: 0.5192 (0.5222)  acc1: 99.2188 (99.0160)  acc5: 100.0000 (99.9940)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [140/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3481.5402354009275  loss: 0.5231 (0.5224)  acc1: 99.2188 (99.0193)  acc5: 100.0000 (99.9945)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [150/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3100.5042389521586  loss: 0.5230 (0.5225)  acc1: 99.2188 (99.0273)  acc5: 100.0000 (99.9948)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [160/468]  eta: 0:00:12  lr: 0.0005000000000000002  img/s: 3358.0876940590715  loss: 0.5219 (0.5226)  acc1: 99.2188 (99.0392)  acc5: 100.0000 (99.9951)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [170/468]  eta: 0:00:11  lr: 0.0005000000000000002  img/s: 3427.9004456703574  loss: 0.5155 (0.5224)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (99.9954)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [180/468]  eta: 0:00:11  lr: 0.0005000000000000002  img/s: 3247.5632096301  loss: 0.5159 (0.5225)  acc1: 99.2188 (99.0331)  acc5: 100.0000 (99.9957)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [190/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3533.370487617067  loss: 0.5161 (0.5224)  acc1: 99.2188 (99.0388)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [200/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3471.7018145135216  loss: 0.5194 (0.5228)  acc1: 98.4375 (99.0244)  acc5: 100.0000 (99.9961)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [210/468]  eta: 0:00:10  lr: 0.0005000000000000002  img/s: 3124.103347143988  loss: 0.5306 (0.5233)  acc1: 98.4375 (99.0077)  acc5: 100.0000 (99.9963)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [220/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 3118.1337344709223  loss: 0.5267 (0.5233)  acc1: 99.2188 (99.0243)  acc5: 100.0000 (99.9965)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [230/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 2732.584679594849  loss: 0.5109 (0.5232)  acc1: 100.0000 (99.0294)  acc5: 100.0000 (99.9966)  time: 0.0512  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [11]  [240/468]  eta: 0:00:09  lr: 0.0005000000000000002  img/s: 3494.184149381374  loss: 0.5244 (0.5235)  acc1: 98.4375 (99.0113)  acc5: 100.0000 (99.9935)  time: 0.0530  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [11]  [250/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3441.568450473089  loss: 0.5169 (0.5231)  acc1: 99.2188 (99.0382)  acc5: 100.0000 (99.9938)  time: 0.0464  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [11]  [260/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3567.485626951957  loss: 0.5087 (0.5230)  acc1: 99.2188 (99.0451)  acc5: 100.0000 (99.9940)  time: 0.0440  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [11]  [270/468]  eta: 0:00:08  lr: 0.0005000000000000002  img/s: 3464.443245615167  loss: 0.5222 (0.5231)  acc1: 99.2188 (99.0515)  acc5: 100.0000 (99.9942)  time: 0.0395  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [11]  [280/468]  eta: 0:00:07  lr: 0.0005000000000000002  img/s: 3437.4478144228246  loss: 0.5222 (0.5231)  acc1: 99.2188 (99.0547)  acc5: 100.0000 (99.9944)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [290/468]  eta: 0:00:07  lr: 0.0005000000000000002  img/s: 3604.3216069606315  loss: 0.5156 (0.5230)  acc1: 99.2188 (99.0630)  acc5: 100.0000 (99.9946)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [300/468]  eta: 0:00:06  lr: 0.0005000000000000002  img/s: 3499.4225672513476  loss: 0.5130 (0.5227)  acc1: 99.2188 (99.0734)  acc5: 100.0000 (99.9922)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [310/468]  eta: 0:00:06  lr: 0.0005000000000000002  img/s: 3521.6196261069204  loss: 0.5125 (0.5226)  acc1: 99.2188 (99.0756)  acc5: 100.0000 (99.9925)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [320/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3499.6506808685394  loss: 0.5142 (0.5223)  acc1: 99.2188 (99.0873)  acc5: 100.0000 (99.9927)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [330/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3439.870522127466  loss: 0.5142 (0.5224)  acc1: 99.2188 (99.0889)  acc5: 100.0000 (99.9906)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [340/468]  eta: 0:00:05  lr: 0.0005000000000000002  img/s: 3406.044244811988  loss: 0.5148 (0.5224)  acc1: 99.2188 (99.0927)  acc5: 100.0000 (99.9908)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [350/468]  eta: 0:00:04  lr: 0.0005000000000000002  img/s: 3536.6524288217547  loss: 0.5167 (0.5225)  acc1: 99.2188 (99.0919)  acc5: 100.0000 (99.9889)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [360/468]  eta: 0:00:04  lr: 0.0005000000000000002  img/s: 3499.1944833699413  loss: 0.5167 (0.5223)  acc1: 99.2188 (99.1041)  acc5: 100.0000 (99.9870)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [11]  [370/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3468.449624322456  loss: 0.5129 (0.5222)  acc1: 99.2188 (99.1029)  acc5: 100.0000 (99.9874)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [380/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3447.4469402170425  loss: 0.5251 (0.5225)  acc1: 99.2188 (99.0957)  acc5: 100.0000 (99.9877)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [390/468]  eta: 0:00:03  lr: 0.0005000000000000002  img/s: 3585.473750292183  loss: 0.5270 (0.5227)  acc1: 98.4375 (99.0869)  acc5: 100.0000 (99.9880)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [400/468]  eta: 0:00:02  lr: 0.0005000000000000002  img/s: 3455.968689248516  loss: 0.5184 (0.5230)  acc1: 99.2188 (99.0804)  acc5: 100.0000 (99.9864)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [410/468]  eta: 0:00:02  lr: 0.0005000000000000002  img/s: 3584.205089860336  loss: 0.5204 (0.5232)  acc1: 98.4375 (99.0705)  acc5: 100.0000 (99.9867)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [420/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3539.0304021094266  loss: 0.5257 (0.5233)  acc1: 98.4375 (99.0684)  acc5: 100.0000 (99.9852)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [430/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3503.738951105542  loss: 0.5203 (0.5233)  acc1: 99.2188 (99.0683)  acc5: 100.0000 (99.9855)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [440/468]  eta: 0:00:01  lr: 0.0005000000000000002  img/s: 3516.6140161266026  loss: 0.5149 (0.5232)  acc1: 99.2188 (99.0699)  acc5: 100.0000 (99.9858)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11]  [450/468]  eta: 0:00:00  lr: 0.0005000000000000002  img/s: 3491.661650125522  loss: 0.5159 (0.5232)  acc1: 99.2188 (99.0698)  acc5: 100.0000 (99.9861)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [11]  [460/468]  eta: 0:00:00  lr: 0.0005000000000000002  img/s: 3486.265305592353  loss: 0.5175 (0.5231)  acc1: 99.2188 (99.0747)  acc5: 100.0000 (99.9864)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [11] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5518 (0.5518)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1369  data: 0.1105  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.430 Acc@5 99.990</span><br><span class="line">Epoch: [12]  [  0/468]  eta: 0:02:14  lr: 0.0003308693936411421  img/s: 948.6122759093495  loss: 0.5233 (0.5233)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.2872  data: 0.1522  max mem: 190</span><br><span class="line">Epoch: [12]  [ 10/468]  eta: 0:00:36  lr: 0.0003308693936411421  img/s: 3477.0756526751425  loss: 0.5233 (0.5273)  acc1: 99.2188 (98.9347)  acc5: 100.0000 (100.0000)  time: 0.0804  data: 0.0196  max mem: 190</span><br><span class="line">Epoch: [12]  [ 20/468]  eta: 0:00:26  lr: 0.0003308693936411421  img/s: 3368.8977353304763  loss: 0.5224 (0.5260)  acc1: 99.2188 (99.0327)  acc5: 100.0000 (100.0000)  time: 0.0488  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [12]  [ 30/468]  eta: 0:00:23  lr: 0.0003308693936411421  img/s: 3471.9487812922375  loss: 0.5112 (0.5222)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 40/468]  eta: 0:00:21  lr: 0.0003308693936411421  img/s: 3426.893938620232  loss: 0.5088 (0.5198)  acc1: 99.2188 (99.2950)  acc5: 100.0000 (100.0000)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 50/468]  eta: 0:00:19  lr: 0.0003308693936411421  img/s: 3567.248584717608  loss: 0.5111 (0.5197)  acc1: 99.2188 (99.3107)  acc5: 100.0000 (99.9694)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 60/468]  eta: 0:00:18  lr: 0.0003308693936411421  img/s: 3598.4511009082075  loss: 0.5193 (0.5200)  acc1: 99.2188 (99.2572)  acc5: 100.0000 (99.9744)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 70/468]  eta: 0:00:17  lr: 0.0003308693936411421  img/s: 3365.2866634907105  loss: 0.5134 (0.5187)  acc1: 99.2188 (99.2958)  acc5: 100.0000 (99.9780)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [ 80/468]  eta: 0:00:16  lr: 0.0003308693936411421  img/s: 3487.42026048264  loss: 0.5116 (0.5181)  acc1: 99.2188 (99.3248)  acc5: 100.0000 (99.9807)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [ 90/468]  eta: 0:00:16  lr: 0.0003308693936411421  img/s: 2787.694391077233  loss: 0.5146 (0.5188)  acc1: 99.2188 (99.2788)  acc5: 100.0000 (99.9742)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [100/468]  eta: 0:00:15  lr: 0.0003308693936411421  img/s: 3409.505166293034  loss: 0.5101 (0.5178)  acc1: 99.2188 (99.3038)  acc5: 100.0000 (99.9768)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [110/468]  eta: 0:00:14  lr: 0.0003308693936411421  img/s: 3557.1090512757655  loss: 0.5063 (0.5172)  acc1: 100.0000 (99.3243)  acc5: 100.0000 (99.9789)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [120/468]  eta: 0:00:14  lr: 0.0003308693936411421  img/s: 3579.1871358284775  loss: 0.5063 (0.5173)  acc1: 100.0000 (99.3285)  acc5: 100.0000 (99.9806)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [130/468]  eta: 0:00:13  lr: 0.0003308693936411421  img/s: 3457.0591318570223  loss: 0.5063 (0.5167)  acc1: 99.2188 (99.3380)  acc5: 100.0000 (99.9821)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [140/468]  eta: 0:00:13  lr: 0.0003308693936411421  img/s: 3483.822042257177  loss: 0.5083 (0.5166)  acc1: 99.2188 (99.3351)  acc5: 100.0000 (99.9834)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [150/468]  eta: 0:00:12  lr: 0.0003308693936411421  img/s: 3503.2359673735727  loss: 0.5090 (0.5161)  acc1: 99.2188 (99.3429)  acc5: 100.0000 (99.9845)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [160/468]  eta: 0:00:12  lr: 0.0003308693936411421  img/s: 3429.1046543563934  loss: 0.5088 (0.5158)  acc1: 99.2188 (99.3595)  acc5: 100.0000 (99.9854)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [170/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 3477.0981723034674  loss: 0.5088 (0.5158)  acc1: 99.2188 (99.3604)  acc5: 100.0000 (99.9863)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [180/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 2182.446439968292  loss: 0.5171 (0.5164)  acc1: 99.2188 (99.3482)  acc5: 100.0000 (99.9871)  time: 0.0454  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [190/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 2737.196131315037  loss: 0.5142 (0.5163)  acc1: 99.2188 (99.3496)  acc5: 100.0000 (99.9877)  time: 0.0563  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [12]  [200/468]  eta: 0:00:11  lr: 0.0003308693936411421  img/s: 3381.0972755784514  loss: 0.5142 (0.5168)  acc1: 99.2188 (99.3276)  acc5: 100.0000 (99.9883)  time: 0.0535  data: 0.0055  max mem: 190</span><br><span class="line">Epoch: [12]  [210/468]  eta: 0:00:10  lr: 0.0003308693936411421  img/s: 3422.4374123466864  loss: 0.5139 (0.5166)  acc1: 99.2188 (99.3372)  acc5: 100.0000 (99.9889)  time: 0.0428  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [12]  [220/468]  eta: 0:00:10  lr: 0.0003308693936411421  img/s: 3410.0898904951855  loss: 0.5099 (0.5163)  acc1: 100.0000 (99.3460)  acc5: 100.0000 (99.9894)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [230/468]  eta: 0:00:09  lr: 0.0003308693936411421  img/s: 3545.2936763695916  loss: 0.5056 (0.5161)  acc1: 100.0000 (99.3506)  acc5: 100.0000 (99.9899)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [240/468]  eta: 0:00:09  lr: 0.0003308693936411421  img/s: 3464.9798763408244  loss: 0.5121 (0.5160)  acc1: 99.2188 (99.3517)  acc5: 100.0000 (99.9903)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [250/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 3481.901510483887  loss: 0.5145 (0.5162)  acc1: 99.2188 (99.3464)  acc5: 100.0000 (99.9907)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [260/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 3417.1875067628207  loss: 0.5087 (0.5161)  acc1: 99.2188 (99.3534)  acc5: 100.0000 (99.9880)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [270/468]  eta: 0:00:08  lr: 0.0003308693936411421  img/s: 2321.513592984489  loss: 0.5086 (0.5161)  acc1: 99.2188 (99.3571)  acc5: 100.0000 (99.9885)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [280/468]  eta: 0:00:07  lr: 0.0003308693936411421  img/s: 2544.41190521327  loss: 0.5116 (0.5161)  acc1: 99.2188 (99.3550)  acc5: 100.0000 (99.9889)  time: 0.0479  data: 0.0041  max mem: 190</span><br><span class="line">Epoch: [12]  [290/468]  eta: 0:00:07  lr: 0.0003308693936411421  img/s: 3506.5995571608655  loss: 0.5128 (0.5162)  acc1: 99.2188 (99.3449)  acc5: 100.0000 (99.9893)  time: 0.0457  data: 0.0041  max mem: 190</span><br><span class="line">Epoch: [12]  [300/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 3373.660969233863  loss: 0.5145 (0.5163)  acc1: 99.2188 (99.3433)  acc5: 100.0000 (99.9896)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [310/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 3646.9483394583285  loss: 0.5117 (0.5164)  acc1: 99.2188 (99.3343)  acc5: 100.0000 (99.9900)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [320/468]  eta: 0:00:06  lr: 0.0003308693936411421  img/s: 2982.964190266642  loss: 0.5159 (0.5167)  acc1: 99.2188 (99.3137)  acc5: 100.0000 (99.9903)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [330/468]  eta: 0:00:05  lr: 0.0003308693936411421  img/s: 3034.3518772854954  loss: 0.5159 (0.5167)  acc1: 99.2188 (99.3202)  acc5: 100.0000 (99.9906)  time: 0.0444  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [12]  [340/468]  eta: 0:00:05  lr: 0.0003308693936411421  img/s: 3081.7810433504774  loss: 0.5096 (0.5168)  acc1: 99.2188 (99.3173)  acc5: 100.0000 (99.9908)  time: 0.0476  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [12]  [350/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3615.1219269125363  loss: 0.5097 (0.5167)  acc1: 99.2188 (99.3234)  acc5: 100.0000 (99.9911)  time: 0.0413  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [12]  [360/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3503.578895161027  loss: 0.5110 (0.5166)  acc1: 99.2188 (99.3248)  acc5: 100.0000 (99.9913)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [370/468]  eta: 0:00:04  lr: 0.0003308693936411421  img/s: 3446.561674263337  loss: 0.5092 (0.5166)  acc1: 99.2188 (99.3240)  acc5: 100.0000 (99.9916)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [380/468]  eta: 0:00:03  lr: 0.0003308693936411421  img/s: 3527.2419271124195  loss: 0.5092 (0.5165)  acc1: 99.2188 (99.3233)  acc5: 100.0000 (99.9918)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [390/468]  eta: 0:00:03  lr: 0.0003308693936411421  img/s: 3609.555938037866  loss: 0.5093 (0.5164)  acc1: 99.2188 (99.3286)  acc5: 100.0000 (99.9920)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [400/468]  eta: 0:00:02  lr: 0.0003308693936411421  img/s: 3477.7288403487632  loss: 0.5089 (0.5163)  acc1: 99.2188 (99.3298)  acc5: 100.0000 (99.9922)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [410/468]  eta: 0:00:02  lr: 0.0003308693936411421  img/s: 3580.810458213833  loss: 0.5094 (0.5162)  acc1: 99.2188 (99.3309)  acc5: 100.0000 (99.9924)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12]  [420/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3572.375715312342  loss: 0.5097 (0.5163)  acc1: 99.2188 (99.3338)  acc5: 100.0000 (99.9926)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [430/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3535.953632962748  loss: 0.5195 (0.5164)  acc1: 99.2188 (99.3293)  acc5: 100.0000 (99.9927)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [440/468]  eta: 0:00:01  lr: 0.0003308693936411421  img/s: 3534.1380554275556  loss: 0.5130 (0.5163)  acc1: 99.2188 (99.3339)  acc5: 100.0000 (99.9929)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [450/468]  eta: 0:00:00  lr: 0.0003308693936411421  img/s: 3457.749359164273  loss: 0.5076 (0.5163)  acc1: 99.2188 (99.3383)  acc5: 100.0000 (99.9931)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [12]  [460/468]  eta: 0:00:00  lr: 0.0003308693936411421  img/s: 3558.1227681826012  loss: 0.5090 (0.5163)  acc1: 99.2188 (99.3374)  acc5: 100.0000 (99.9932)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [12] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5338 (0.5338)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1201  data: 0.1064  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.620 Acc@5 99.970</span><br><span class="line">Epoch: [13]  [  0/468]  eta: 0:01:24  lr: 0.00019098300562505265  img/s: 1682.8015572007998  loss: 0.5060 (0.5060)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1802  data: 0.1041  max mem: 190</span><br><span class="line">Epoch: [13]  [ 10/468]  eta: 0:00:23  lr: 0.00019098300562505265  img/s: 3474.173064478555  loss: 0.5087 (0.5116)  acc1: 100.0000 (99.7159)  acc5: 100.0000 (100.0000)  time: 0.0514  data: 0.0097  max mem: 190</span><br><span class="line">Epoch: [13]  [ 20/468]  eta: 0:00:20  lr: 0.00019098300562505265  img/s: 3444.748299668917  loss: 0.5079 (0.5102)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [ 30/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 3511.117366225001  loss: 0.5056 (0.5099)  acc1: 100.0000 (99.6976)  acc5: 100.0000 (100.0000)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [ 40/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 1363.658307488716  loss: 0.5043 (0.5086)  acc1: 100.0000 (99.7523)  acc5: 100.0000 (100.0000)  time: 0.0407  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [ 50/468]  eta: 0:00:18  lr: 0.00019098300562505265  img/s: 3069.394789350012  loss: 0.5024 (0.5089)  acc1: 100.0000 (99.7243)  acc5: 100.0000 (100.0000)  time: 0.0470  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [13]  [ 60/468]  eta: 0:00:17  lr: 0.00019098300562505265  img/s: 3438.2623442163103  loss: 0.5078 (0.5095)  acc1: 99.2188 (99.6926)  acc5: 100.0000 (99.9872)  time: 0.0461  data: 0.0028  max mem: 190</span><br><span class="line">Epoch: [13]  [ 70/468]  eta: 0:00:17  lr: 0.00019098300562505265  img/s: 3492.479358842586  loss: 0.5070 (0.5098)  acc1: 99.2188 (99.6589)  acc5: 100.0000 (99.9890)  time: 0.0406  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [13]  [ 80/468]  eta: 0:00:16  lr: 0.00019098300562505265  img/s: 3470.579680914333  loss: 0.5058 (0.5097)  acc1: 99.2188 (99.6431)  acc5: 100.0000 (99.9904)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [ 90/468]  eta: 0:00:15  lr: 0.00019098300562505265  img/s: 3411.7802208975713  loss: 0.5038 (0.5095)  acc1: 100.0000 (99.6394)  acc5: 100.0000 (99.9914)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [100/468]  eta: 0:00:15  lr: 0.00019098300562505265  img/s: 3476.692863618702  loss: 0.5039 (0.5095)  acc1: 100.0000 (99.6364)  acc5: 100.0000 (99.9923)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [110/468]  eta: 0:00:14  lr: 0.00019098300562505265  img/s: 3369.5955011046394  loss: 0.5042 (0.5100)  acc1: 99.2188 (99.6199)  acc5: 100.0000 (99.9930)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [120/468]  eta: 0:00:14  lr: 0.00019098300562505265  img/s: 2763.3305470342384  loss: 0.5049 (0.5098)  acc1: 100.0000 (99.6320)  acc5: 100.0000 (99.9935)  time: 0.0390  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [130/468]  eta: 0:00:13  lr: 0.00019098300562505265  img/s: 3448.089042459586  loss: 0.5049 (0.5101)  acc1: 100.0000 (99.6243)  acc5: 100.0000 (99.9940)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [140/468]  eta: 0:00:13  lr: 0.00019098300562505265  img/s: 3369.5320559087686  loss: 0.5095 (0.5103)  acc1: 99.2188 (99.6121)  acc5: 100.0000 (99.9945)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [150/468]  eta: 0:00:12  lr: 0.00019098300562505265  img/s: 3471.432436277109  loss: 0.5060 (0.5102)  acc1: 99.2188 (99.6171)  acc5: 100.0000 (99.9948)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [160/468]  eta: 0:00:12  lr: 0.00019098300562505265  img/s: 3444.505187247775  loss: 0.5056 (0.5102)  acc1: 100.0000 (99.6167)  acc5: 100.0000 (99.9951)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [170/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3307.5865570033575  loss: 0.5027 (0.5099)  acc1: 100.0000 (99.6254)  acc5: 100.0000 (99.9954)  time: 0.0389  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [180/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3425.866161277766  loss: 0.5031 (0.5099)  acc1: 100.0000 (99.6202)  acc5: 100.0000 (99.9957)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [190/468]  eta: 0:00:11  lr: 0.00019098300562505265  img/s: 3484.432536978264  loss: 0.5074 (0.5100)  acc1: 99.2188 (99.6073)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [200/468]  eta: 0:00:10  lr: 0.00019098300562505265  img/s: 3348.119189273464  loss: 0.5049 (0.5099)  acc1: 100.0000 (99.6191)  acc5: 100.0000 (99.9961)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [210/468]  eta: 0:00:10  lr: 0.00019098300562505265  img/s: 3368.7074857250423  loss: 0.5055 (0.5099)  acc1: 100.0000 (99.6149)  acc5: 100.0000 (99.9963)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [220/468]  eta: 0:00:09  lr: 0.00019098300562505265  img/s: 3569.2644483595386  loss: 0.5026 (0.5098)  acc1: 100.0000 (99.6182)  acc5: 100.0000 (99.9965)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [230/468]  eta: 0:00:09  lr: 0.00019098300562505265  img/s: 3386.0659337887014  loss: 0.5020 (0.5096)  acc1: 100.0000 (99.6280)  acc5: 100.0000 (99.9966)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [240/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3378.607779588806  loss: 0.5026 (0.5096)  acc1: 100.0000 (99.6304)  acc5: 100.0000 (99.9968)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [250/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3548.785468294521  loss: 0.5026 (0.5094)  acc1: 100.0000 (99.6452)  acc5: 100.0000 (99.9969)  time: 0.0404  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [13]  [260/468]  eta: 0:00:08  lr: 0.00019098300562505265  img/s: 3539.357040201469  loss: 0.5032 (0.5096)  acc1: 100.0000 (99.6288)  acc5: 100.0000 (99.9970)  time: 0.0427  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [13]  [270/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3462.633343437796  loss: 0.5123 (0.5099)  acc1: 99.2188 (99.6166)  acc5: 100.0000 (99.9971)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [280/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3555.130432479323  loss: 0.5059 (0.5099)  acc1: 99.2188 (99.6191)  acc5: 100.0000 (99.9972)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [290/468]  eta: 0:00:07  lr: 0.00019098300562505265  img/s: 3626.9915214733046  loss: 0.5069 (0.5104)  acc1: 99.2188 (99.6000)  acc5: 100.0000 (99.9973)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [300/468]  eta: 0:00:06  lr: 0.00019098300562505265  img/s: 3500.6319084009283  loss: 0.5151 (0.5105)  acc1: 99.2188 (99.5977)  acc5: 100.0000 (99.9948)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [310/468]  eta: 0:00:06  lr: 0.00019098300562505265  img/s: 3429.871409588061  loss: 0.5056 (0.5106)  acc1: 100.0000 (99.5981)  acc5: 100.0000 (99.9950)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [320/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3511.392938898845  loss: 0.5083 (0.5109)  acc1: 99.2188 (99.5887)  acc5: 100.0000 (99.9951)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [330/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3008.3037492365365  loss: 0.5073 (0.5108)  acc1: 100.0000 (99.5917)  acc5: 100.0000 (99.9953)  time: 0.0458  data: 0.0019  max mem: 190</span><br><span class="line">Epoch: [13]  [340/468]  eta: 0:00:05  lr: 0.00019098300562505265  img/s: 3446.9378567347017  loss: 0.5035 (0.5106)  acc1: 100.0000 (99.6014)  acc5: 100.0000 (99.9954)  time: 0.0487  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [13]  [350/468]  eta: 0:00:04  lr: 0.00019098300562505265  img/s: 3523.006181507973  loss: 0.5034 (0.5106)  acc1: 100.0000 (99.5971)  acc5: 100.0000 (99.9955)  time: 0.0402  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [13]  [360/468]  eta: 0:00:04  lr: 0.00019098300562505265  img/s: 3495.3898720002085  loss: 0.5038 (0.5106)  acc1: 100.0000 (99.5996)  acc5: 100.0000 (99.9957)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [370/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3456.858794895239  loss: 0.5027 (0.5105)  acc1: 100.0000 (99.6062)  acc5: 100.0000 (99.9958)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [380/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3468.7857752048176  loss: 0.5026 (0.5104)  acc1: 100.0000 (99.6125)  acc5: 100.0000 (99.9959)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [390/468]  eta: 0:00:03  lr: 0.00019098300562505265  img/s: 3539.4037077081302  loss: 0.5050 (0.5105)  acc1: 100.0000 (99.6084)  acc5: 100.0000 (99.9960)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [400/468]  eta: 0:00:02  lr: 0.00019098300562505265  img/s: 3395.0592981857044  loss: 0.5056 (0.5105)  acc1: 100.0000 (99.6103)  acc5: 100.0000 (99.9961)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [410/468]  eta: 0:00:02  lr: 0.00019098300562505265  img/s: 3485.880490607936  loss: 0.5043 (0.5105)  acc1: 100.0000 (99.6084)  acc5: 100.0000 (99.9962)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [420/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3440.598000512689  loss: 0.5027 (0.5104)  acc1: 100.0000 (99.6122)  acc5: 100.0000 (99.9963)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [430/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3459.130640962862  loss: 0.5031 (0.5104)  acc1: 100.0000 (99.6103)  acc5: 100.0000 (99.9964)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [440/468]  eta: 0:00:01  lr: 0.00019098300562505265  img/s: 3513.5071006923995  loss: 0.5042 (0.5105)  acc1: 100.0000 (99.6120)  acc5: 100.0000 (99.9947)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [13]  [450/468]  eta: 0:00:00  lr: 0.00019098300562505265  img/s: 3505.61498178209  loss: 0.5024 (0.5103)  acc1: 100.0000 (99.6189)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13]  [460/468]  eta: 0:00:00  lr: 0.00019098300562505265  img/s: 3506.6453648245274  loss: 0.5038 (0.5103)  acc1: 100.0000 (99.6153)  acc5: 100.0000 (99.9949)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [13] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:14  loss: 0.5582 (0.5582)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 0.1795  data: 0.1629  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:02</span><br><span class="line">Test:  Acc@1 98.670 Acc@5 99.910</span><br><span class="line">Epoch: [14]  [  0/468]  eta: 0:01:18  lr: 8.645454235739902e-05  img/s: 2322.0256738521157  loss: 0.5286 (0.5286)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1679  data: 0.1128  max mem: 190</span><br><span class="line">Epoch: [14]  [ 10/468]  eta: 0:00:24  lr: 8.645454235739902e-05  img/s: 3280.4039594280825  loss: 0.5064 (0.5121)  acc1: 100.0000 (99.5739)  acc5: 100.0000 (100.0000)  time: 0.0536  data: 0.0106  max mem: 190</span><br><span class="line">Epoch: [14]  [ 20/468]  eta: 0:00:20  lr: 8.645454235739902e-05  img/s: 3501.63652491521  loss: 0.5060 (0.5105)  acc1: 100.0000 (99.5908)  acc5: 100.0000 (100.0000)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 30/468]  eta: 0:00:19  lr: 8.645454235739902e-05  img/s: 3416.948268839104  loss: 0.5057 (0.5089)  acc1: 100.0000 (99.6472)  acc5: 100.0000 (100.0000)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 40/468]  eta: 0:00:17  lr: 8.645454235739902e-05  img/s: 3601.3718824208113  loss: 0.5030 (0.5086)  acc1: 100.0000 (99.6951)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 50/468]  eta: 0:00:17  lr: 8.645454235739902e-05  img/s: 3491.0940220960706  loss: 0.5035 (0.5095)  acc1: 100.0000 (99.6630)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [ 60/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 3246.169034863894  loss: 0.5039 (0.5095)  acc1: 100.0000 (99.6670)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [14]  [ 70/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 3279.902935516388  loss: 0.5030 (0.5087)  acc1: 100.0000 (99.6919)  acc5: 100.0000 (100.0000)  time: 0.0399  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [14]  [ 80/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 1911.368477266335  loss: 0.5030 (0.5087)  acc1: 100.0000 (99.7010)  acc5: 100.0000 (99.9904)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [ 90/468]  eta: 0:00:16  lr: 8.645454235739902e-05  img/s: 2397.2909546369933  loss: 0.5029 (0.5091)  acc1: 100.0000 (99.6995)  acc5: 100.0000 (99.9914)  time: 0.0512  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [14]  [100/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 3118.043186860415  loss: 0.5028 (0.5085)  acc1: 100.0000 (99.7215)  acc5: 100.0000 (99.9923)  time: 0.0517  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [14]  [110/468]  eta: 0:00:15  lr: 8.645454235739902e-05  img/s: 3220.793885619327  loss: 0.5028 (0.5086)  acc1: 100.0000 (99.7185)  acc5: 100.0000 (99.9930)  time: 0.0421  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [14]  [120/468]  eta: 0:00:14  lr: 8.645454235739902e-05  img/s: 3145.684992587992  loss: 0.5033 (0.5085)  acc1: 100.0000 (99.7224)  acc5: 100.0000 (99.9935)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [130/468]  eta: 0:00:14  lr: 8.645454235739902e-05  img/s: 3213.7759393725346  loss: 0.5029 (0.5083)  acc1: 100.0000 (99.7197)  acc5: 100.0000 (99.9940)  time: 0.0420  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [140/468]  eta: 0:00:13  lr: 8.645454235739902e-05  img/s: 2861.4496807410646  loss: 0.5028 (0.5083)  acc1: 100.0000 (99.7174)  acc5: 100.0000 (99.9945)  time: 0.0417  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [150/468]  eta: 0:00:13  lr: 8.645454235739902e-05  img/s: 2756.477801680983  loss: 0.5024 (0.5081)  acc1: 100.0000 (99.7258)  acc5: 100.0000 (99.9948)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [160/468]  eta: 0:00:12  lr: 8.645454235739902e-05  img/s: 3562.7271170806484  loss: 0.5023 (0.5078)  acc1: 100.0000 (99.7428)  acc5: 100.0000 (99.9951)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [170/468]  eta: 0:00:12  lr: 8.645454235739902e-05  img/s: 3452.013271263599  loss: 0.5018 (0.5078)  acc1: 100.0000 (99.7350)  acc5: 100.0000 (99.9954)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [180/468]  eta: 0:00:11  lr: 8.645454235739902e-05  img/s: 3525.967818628417  loss: 0.5033 (0.5079)  acc1: 100.0000 (99.7194)  acc5: 100.0000 (99.9957)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [190/468]  eta: 0:00:11  lr: 8.645454235739902e-05  img/s: 3569.5254913433155  loss: 0.5049 (0.5080)  acc1: 100.0000 (99.7137)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [200/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3462.164418190729  loss: 0.5022 (0.5078)  acc1: 100.0000 (99.7240)  acc5: 100.0000 (99.9961)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [210/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3587.1746846268975  loss: 0.5022 (0.5081)  acc1: 100.0000 (99.7186)  acc5: 100.0000 (99.9963)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [220/468]  eta: 0:00:10  lr: 8.645454235739902e-05  img/s: 3513.277177185038  loss: 0.5015 (0.5081)  acc1: 100.0000 (99.7207)  acc5: 100.0000 (99.9965)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [230/468]  eta: 0:00:09  lr: 8.645454235739902e-05  img/s: 3025.5794053335135  loss: 0.5026 (0.5081)  acc1: 100.0000 (99.7193)  acc5: 100.0000 (99.9966)  time: 0.0430  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [240/468]  eta: 0:00:09  lr: 8.645454235739902e-05  img/s: 3155.651043319814  loss: 0.5033 (0.5079)  acc1: 100.0000 (99.7309)  acc5: 100.0000 (99.9968)  time: 0.0479  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [14]  [250/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3337.5248633896767  loss: 0.5024 (0.5077)  acc1: 100.0000 (99.7385)  acc5: 100.0000 (99.9969)  time: 0.0429  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [14]  [260/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3527.5432145813897  loss: 0.5022 (0.5075)  acc1: 100.0000 (99.7426)  acc5: 100.0000 (99.9970)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [270/468]  eta: 0:00:08  lr: 8.645454235739902e-05  img/s: 3607.8579627165573  loss: 0.5031 (0.5078)  acc1: 100.0000 (99.7348)  acc5: 100.0000 (99.9971)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [280/468]  eta: 0:00:07  lr: 8.645454235739902e-05  img/s: 3467.2176282920655  loss: 0.5059 (0.5078)  acc1: 100.0000 (99.7331)  acc5: 100.0000 (99.9972)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [290/468]  eta: 0:00:07  lr: 8.645454235739902e-05  img/s: 3557.085483336646  loss: 0.5026 (0.5077)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9973)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [300/468]  eta: 0:00:06  lr: 8.645454235739902e-05  img/s: 3545.7619739518664  loss: 0.5026 (0.5081)  acc1: 100.0000 (99.7275)  acc5: 100.0000 (99.9974)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [310/468]  eta: 0:00:06  lr: 8.645454235739902e-05  img/s: 3492.433920532903  loss: 0.5024 (0.5079)  acc1: 100.0000 (99.7337)  acc5: 100.0000 (99.9975)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [320/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3540.3840097070733  loss: 0.5016 (0.5078)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [330/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3359.495591557316  loss: 0.5020 (0.5077)  acc1: 100.0000 (99.7404)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [340/468]  eta: 0:00:05  lr: 8.645454235739902e-05  img/s: 3507.4701074706823  loss: 0.5033 (0.5076)  acc1: 100.0000 (99.7411)  acc5: 100.0000 (99.9977)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [350/468]  eta: 0:00:04  lr: 8.645454235739902e-05  img/s: 3505.5920912587253  loss: 0.5036 (0.5078)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9978)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [360/468]  eta: 0:00:04  lr: 8.645454235739902e-05  img/s: 3241.112941006013  loss: 0.5040 (0.5078)  acc1: 100.0000 (99.7381)  acc5: 100.0000 (99.9978)  time: 0.0428  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [14]  [370/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3499.468187595737  loss: 0.5020 (0.5078)  acc1: 100.0000 (99.7410)  acc5: 100.0000 (99.9979)  time: 0.0486  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [14]  [380/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3273.283777192469  loss: 0.5021 (0.5079)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (99.9979)  time: 0.0429  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [14]  [390/468]  eta: 0:00:03  lr: 8.645454235739902e-05  img/s: 3569.5254913433155  loss: 0.5042 (0.5079)  acc1: 100.0000 (99.7383)  acc5: 100.0000 (99.9980)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [400/468]  eta: 0:00:02  lr: 8.645454235739902e-05  img/s: 3462.8343502883163  loss: 0.5022 (0.5080)  acc1: 100.0000 (99.7350)  acc5: 100.0000 (99.9981)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [14]  [410/468]  eta: 0:00:02  lr: 8.645454235739902e-05  img/s: 3365.64531235307  loss: 0.5032 (0.5081)  acc1: 100.0000 (99.7320)  acc5: 100.0000 (99.9962)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [420/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3430.1343760382324  loss: 0.5045 (0.5080)  acc1: 100.0000 (99.7328)  acc5: 100.0000 (99.9963)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [430/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3436.6776683864855  loss: 0.5019 (0.5079)  acc1: 100.0000 (99.7372)  acc5: 100.0000 (99.9964)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [440/468]  eta: 0:00:01  lr: 8.645454235739902e-05  img/s: 3557.2740355945457  loss: 0.5018 (0.5077)  acc1: 100.0000 (99.7431)  acc5: 100.0000 (99.9965)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [450/468]  eta: 0:00:00  lr: 8.645454235739902e-05  img/s: 3501.362481412882  loss: 0.5031 (0.5078)  acc1: 100.0000 (99.7384)  acc5: 100.0000 (99.9965)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14]  [460/468]  eta: 0:00:00  lr: 8.645454235739902e-05  img/s: 3422.76470325719  loss: 0.5031 (0.5077)  acc1: 100.0000 (99.7407)  acc5: 100.0000 (99.9966)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [14] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5510 (0.5510)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1211  data: 0.1021  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.750 Acc@5 99.920</span><br><span class="line">Epoch: [15]  [  0/468]  eta: 0:01:14  lr: 2.1852399266194312e-05  img/s: 2757.893798127058  loss: 0.5055 (0.5055)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1598  data: 0.1133  max mem: 190</span><br><span class="line">Epoch: [15]  [ 10/468]  eta: 0:00:22  lr: 2.1852399266194312e-05  img/s: 3457.9943447876076  loss: 0.5040 (0.5057)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0488  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [15]  [ 20/468]  eta: 0:00:19  lr: 2.1852399266194312e-05  img/s: 3491.298346924708  loss: 0.5027 (0.5084)  acc1: 100.0000 (99.7396)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [ 30/468]  eta: 0:00:18  lr: 2.1852399266194312e-05  img/s: 3435.9738368  loss: 0.5024 (0.5065)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 40/468]  eta: 0:00:17  lr: 2.1852399266194312e-05  img/s: 3355.107689231077  loss: 0.5020 (0.5062)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 50/468]  eta: 0:00:16  lr: 2.1852399266194312e-05  img/s: 3455.2569347011804  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 60/468]  eta: 0:00:16  lr: 2.1852399266194312e-05  img/s: 3422.2628827864046  loss: 0.5019 (0.5068)  acc1: 100.0000 (99.8207)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 70/468]  eta: 0:00:15  lr: 2.1852399266194312e-05  img/s: 3367.4185823334233  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8239)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 80/468]  eta: 0:00:15  lr: 2.1852399266194312e-05  img/s: 3468.8081875803605  loss: 0.5019 (0.5069)  acc1: 100.0000 (99.8264)  acc5: 100.0000 (99.9904)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [ 90/468]  eta: 0:00:14  lr: 2.1852399266194312e-05  img/s: 3418.9501999643376  loss: 0.5028 (0.5071)  acc1: 100.0000 (99.8025)  acc5: 100.0000 (99.9914)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [100/468]  eta: 0:00:14  lr: 2.1852399266194312e-05  img/s: 3447.491215452584  loss: 0.5026 (0.5069)  acc1: 100.0000 (99.8066)  acc5: 100.0000 (99.9923)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [110/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 3479.8252020663595  loss: 0.5016 (0.5068)  acc1: 100.0000 (99.8100)  acc5: 100.0000 (99.9930)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [120/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 3482.601694365521  loss: 0.5021 (0.5070)  acc1: 100.0000 (99.7998)  acc5: 100.0000 (99.9935)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [130/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 1422.856818766084  loss: 0.5022 (0.5067)  acc1: 100.0000 (99.8092)  acc5: 100.0000 (99.9940)  time: 0.0414  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [140/468]  eta: 0:00:13  lr: 2.1852399266194312e-05  img/s: 2415.8780340734206  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8005)  acc5: 100.0000 (99.9945)  time: 0.0490  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [15]  [150/468]  eta: 0:00:12  lr: 2.1852399266194312e-05  img/s: 3300.9161906752825  loss: 0.5014 (0.5066)  acc1: 100.0000 (99.8086)  acc5: 100.0000 (99.9948)  time: 0.0458  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [15]  [160/468]  eta: 0:00:12  lr: 2.1852399266194312e-05  img/s: 3474.397898033937  loss: 0.5013 (0.5064)  acc1: 100.0000 (99.8156)  acc5: 100.0000 (99.9951)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [170/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3440.487756736839  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8264)  acc5: 100.0000 (99.9954)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [180/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3407.2763921150504  loss: 0.5015 (0.5062)  acc1: 100.0000 (99.8273)  acc5: 100.0000 (99.9957)  time: 0.0378  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [190/468]  eta: 0:00:11  lr: 2.1852399266194312e-05  img/s: 3543.2814055095764  loss: 0.5026 (0.5062)  acc1: 100.0000 (99.8200)  acc5: 100.0000 (99.9959)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [200/468]  eta: 0:00:10  lr: 2.1852399266194312e-05  img/s: 3530.1642677257514  loss: 0.5032 (0.5064)  acc1: 100.0000 (99.8134)  acc5: 100.0000 (99.9961)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [210/468]  eta: 0:00:10  lr: 2.1852399266194312e-05  img/s: 3561.9234499917065  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8223)  acc5: 100.0000 (99.9963)  time: 0.0372  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [15]  [220/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 3406.4332476761524  loss: 0.5014 (0.5061)  acc1: 100.0000 (99.8268)  acc5: 100.0000 (99.9965)  time: 0.0373  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [15]  [230/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 2149.091168193809  loss: 0.5022 (0.5064)  acc1: 100.0000 (99.8174)  acc5: 100.0000 (99.9966)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [240/468]  eta: 0:00:09  lr: 2.1852399266194312e-05  img/s: 1784.0806850921997  loss: 0.5031 (0.5063)  acc1: 100.0000 (99.8185)  acc5: 100.0000 (99.9968)  time: 0.0456  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [250/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 2532.445798761303  loss: 0.5021 (0.5064)  acc1: 100.0000 (99.8164)  acc5: 100.0000 (99.9969)  time: 0.0536  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [260/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 2783.5606619934465  loss: 0.5015 (0.5062)  acc1: 100.0000 (99.8234)  acc5: 100.0000 (99.9970)  time: 0.0567  data: 0.0063  max mem: 190</span><br><span class="line">Epoch: [15]  [270/468]  eta: 0:00:08  lr: 2.1852399266194312e-05  img/s: 3299.638071122147  loss: 0.5016 (0.5064)  acc1: 100.0000 (99.8155)  acc5: 100.0000 (99.9971)  time: 0.0464  data: 0.0040  max mem: 190</span><br><span class="line">Epoch: [15]  [280/468]  eta: 0:00:07  lr: 2.1852399266194312e-05  img/s: 3415.40490231629  loss: 0.5020 (0.5065)  acc1: 100.0000 (99.8165)  acc5: 100.0000 (99.9972)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [290/468]  eta: 0:00:07  lr: 2.1852399266194312e-05  img/s: 3205.0463977839863  loss: 0.5019 (0.5065)  acc1: 100.0000 (99.8148)  acc5: 100.0000 (99.9973)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [300/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3537.0485357578154  loss: 0.5014 (0.5064)  acc1: 100.0000 (99.8183)  acc5: 100.0000 (99.9974)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [310/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3132.0132077893286  loss: 0.5016 (0.5064)  acc1: 100.0000 (99.8216)  acc5: 100.0000 (99.9975)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [320/468]  eta: 0:00:06  lr: 2.1852399266194312e-05  img/s: 3513.254186489369  loss: 0.5017 (0.5063)  acc1: 100.0000 (99.8223)  acc5: 100.0000 (99.9976)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [330/468]  eta: 0:00:05  lr: 2.1852399266194312e-05  img/s: 3602.870319168926  loss: 0.5017 (0.5062)  acc1: 100.0000 (99.8253)  acc5: 100.0000 (99.9976)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [340/468]  eta: 0:00:05  lr: 2.1852399266194312e-05  img/s: 3537.328194079316  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8305)  acc5: 100.0000 (99.9977)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [350/468]  eta: 0:00:04  lr: 2.1852399266194312e-05  img/s: 3544.029890550942  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8331)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [360/468]  eta: 0:00:04  lr: 2.1852399266194312e-05  img/s: 3440.9067206747595  loss: 0.5013 (0.5060)  acc1: 100.0000 (99.8312)  acc5: 100.0000 (99.9978)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [370/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3565.566490227202  loss: 0.5013 (0.5059)  acc1: 100.0000 (99.8315)  acc5: 100.0000 (99.9979)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [380/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3579.234859596256  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (99.9979)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [390/468]  eta: 0:00:03  lr: 2.1852399266194312e-05  img/s: 3276.220102642965  loss: 0.5025 (0.5059)  acc1: 100.0000 (99.8322)  acc5: 100.0000 (99.9980)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [15]  [400/468]  eta: 0:00:02  lr: 2.1852399266194312e-05  img/s: 2341.0613270076046  loss: 0.5033 (0.5059)  acc1: 100.0000 (99.8344)  acc5: 100.0000 (99.9981)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [410/468]  eta: 0:00:02  lr: 2.1852399266194312e-05  img/s: 2289.818783587819  loss: 0.5022 (0.5059)  acc1: 100.0000 (99.8327)  acc5: 100.0000 (99.9981)  time: 0.0504  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [420/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3458.3730272227162  loss: 0.5017 (0.5060)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9981)  time: 0.0494  data: 0.0026  max mem: 190</span><br><span class="line">Epoch: [15]  [430/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3442.3407903257867  loss: 0.5016 (0.5060)  acc1: 100.0000 (99.8296)  acc5: 100.0000 (99.9982)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [440/468]  eta: 0:00:01  lr: 2.1852399266194312e-05  img/s: 3324.9780882662603  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8335)  acc5: 100.0000 (99.9982)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [450/468]  eta: 0:00:00  lr: 2.1852399266194312e-05  img/s: 3340.619202289839  loss: 0.5010 (0.5058)  acc1: 100.0000 (99.8354)  acc5: 100.0000 (99.9983)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15]  [460/468]  eta: 0:00:00  lr: 2.1852399266194312e-05  img/s: 3465.6285269796595  loss: 0.5015 (0.5059)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [15] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1216  data: 0.1018  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [16]  [  0/468]  eta: 0:01:19  lr: 0.0  img/s: 1982.2950379015851  loss: 0.5016 (0.5016)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1703  data: 0.1057  max mem: 190</span><br><span class="line">Epoch: [16]  [ 10/468]  eta: 0:00:23  lr: 0.0  img/s: 3292.55539201256  loss: 0.5020 (0.5069)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0522  data: 0.0099  max mem: 190</span><br><span class="line">Epoch: [16]  [ 20/468]  eta: 0:00:20  lr: 0.0  img/s: 3561.07290346973  loss: 0.5019 (0.5053)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3545.9024873518883  loss: 0.5019 (0.5074)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3467.934319488405  loss: 0.5022 (0.5069)  acc1: 100.0000 (99.8095)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3523.9311585165738  loss: 0.5020 (0.5070)  acc1: 100.0000 (99.8162)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3601.251095056983  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3460.6022508991996  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3590.8215527850607  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8553)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3534.2311166115887  loss: 0.5014 (0.5057)  acc1: 100.0000 (99.8541)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3035.878988023207  loss: 0.5015 (0.5057)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3506.8744210959494  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8663)  acc5: 100.0000 (100.0000)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3458.4398621444907  loss: 0.5019 (0.5057)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3514.818239549576  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8688)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3442.8043606515325  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8670)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3508.180613458447  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [160/468]  eta: 0:00:11  lr: 0.0  img/s: 3484.9753786034676  loss: 0.5014 (0.5057)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 1982.7416128698683  loss: 0.5026 (0.5058)  acc1: 100.0000 (99.8538)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3118.242398545632  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (100.0000)  time: 0.0478  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [16]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3482.8276202091497  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8568)  acc5: 100.0000 (100.0000)  time: 0.0482  data: 0.0044  max mem: 190</span><br><span class="line">Epoch: [16]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3515.3475727138198  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8601)  acc5: 100.0000 (100.0000)  time: 0.0395  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [16]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3514.680178853166  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3472.6674299316296  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8657)  acc5: 100.0000 (100.0000)  time: 0.0402  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [16]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3407.752196211852  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (100.0000)  time: 0.0424  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [16]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3496.163792654337  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8606)  acc5: 100.0000 (100.0000)  time: 0.0393  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [16]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3485.065868652182  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8568)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3557.4861807796547  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3434.3473299045572  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8616)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3463.7950385496306  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3488.0546788204033  loss: 0.5012 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3538.9370880134998  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3528.9344393758133  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3418.710715172664  loss: 0.5019 (0.5056)  acc1: 100.0000 (99.8588)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3593.008425857142  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3582.7460443513137  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8648)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3636.6961917277445  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (100.0000)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3500.449312781994  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3570.3563367449406  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8673)  acc5: 100.0000 (100.0000)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3518.6422246836064  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3430.5946643662737  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3495.4126294818125  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8656)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [16]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3535.2085550229153  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3448.155480481445  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3298.2798867133984  loss: 0.5025 (0.5054)  acc1: 100.0000 (99.8659)  acc5: 100.0000 (99.9982)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3572.4945734267594  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3348.2653561427687  loss: 0.5011 (0.5054)  acc1: 100.0000 (99.8666)  acc5: 100.0000 (99.9983)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [16]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 2339.847162961381  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0439  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [16] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:12  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1549  data: 0.1313  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [17]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2452.9998766351555  loss: 0.5031 (0.5031)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1589  data: 0.1066  max mem: 190</span><br><span class="line">Epoch: [17]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3470.1086010871745  loss: 0.5013 (0.5042)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0490  data: 0.0100  max mem: 190</span><br><span class="line">Epoch: [17]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 2451.6332716852753  loss: 0.5011 (0.5029)  acc1: 100.0000 (99.9628)  acc5: 100.0000 (100.0000)  time: 0.0386  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 30/468]  eta: 0:00:21  lr: 0.0  img/s: 1973.3548187899728  loss: 0.5012 (0.5050)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (99.9748)  time: 0.0498  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [17]  [ 40/468]  eta: 0:00:22  lr: 0.0  img/s: 2807.2709171054626  loss: 0.5012 (0.5047)  acc1: 100.0000 (99.9047)  acc5: 100.0000 (99.9809)  time: 0.0589  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [17]  [ 50/468]  eta: 0:00:21  lr: 0.0  img/s: 3277.0000122077763  loss: 0.5015 (0.5047)  acc1: 100.0000 (99.9081)  acc5: 100.0000 (99.9847)  time: 0.0519  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [17]  [ 60/468]  eta: 0:00:19  lr: 0.0  img/s: 3144.763687697327  loss: 0.5018 (0.5050)  acc1: 100.0000 (99.8847)  acc5: 100.0000 (99.9872)  time: 0.0432  data: 0.0014  max mem: 190</span><br><span class="line">Epoch: [17]  [ 70/468]  eta: 0:00:18  lr: 0.0  img/s: 3306.7716547072773  loss: 0.5023 (0.5064)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (99.9890)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 80/468]  eta: 0:00:17  lr: 0.0  img/s: 3440.2232004972543  loss: 0.5015 (0.5064)  acc1: 100.0000 (99.8457)  acc5: 100.0000 (99.9904)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [ 90/468]  eta: 0:00:17  lr: 0.0  img/s: 3507.928465484008  loss: 0.5013 (0.5062)  acc1: 100.0000 (99.8369)  acc5: 100.0000 (99.9914)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [100/468]  eta: 0:00:16  lr: 0.0  img/s: 3577.9944551076987  loss: 0.5023 (0.5063)  acc1: 100.0000 (99.8376)  acc5: 100.0000 (99.9923)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [110/468]  eta: 0:00:15  lr: 0.0  img/s: 3463.035380477201  loss: 0.5026 (0.5063)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9930)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [120/468]  eta: 0:00:15  lr: 0.0  img/s: 3505.0428083645074  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8450)  acc5: 100.0000 (99.9935)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3500.129816280495  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (99.9940)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3441.568450473089  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8559)  acc5: 100.0000 (99.9945)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3556.6141901291817  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8655)  acc5: 100.0000 (99.9948)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3476.3551785540844  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9951)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3535.7440480502632  loss: 0.5022 (0.5056)  acc1: 100.0000 (99.8584)  acc5: 100.0000 (99.9954)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3560.3172030531923  loss: 0.5022 (0.5056)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (99.9957)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3294.737658639566  loss: 0.5019 (0.5057)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3465.4048269140153  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8523)  acc5: 100.0000 (99.9961)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3518.665285952103  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8556)  acc5: 100.0000 (99.9963)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 1131.3978588889825  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9965)  time: 0.0452  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 2135.9070318871713  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0558  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [17]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3016.535442981076  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9968)  time: 0.0512  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [17]  [250/468]  eta: 0:00:09  lr: 0.0  img/s: 2825.2497658215193  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8693)  acc5: 100.0000 (99.9969)  time: 0.0456  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [17]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 2882.9463170499885  loss: 0.5016 (0.5051)  acc1: 100.0000 (99.8713)  acc5: 100.0000 (99.9970)  time: 0.0464  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [17]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3288.562068923273  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.8616)  acc5: 100.0000 (99.9971)  time: 0.0437  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3306.242183506691  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9972)  time: 0.0411  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3393.085239374309  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9973)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [300/468]  eta: 0:00:07  lr: 0.0  img/s: 3480.4342966795025  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9974)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3473.678533069348  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9975)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3557.3211767824014  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8613)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3493.024710796497  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8607)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3491.5481097533216  loss: 0.5016 (0.5052)  acc1: 100.0000 (99.8648)  acc5: 100.0000 (99.9977)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3523.5148587629947  loss: 0.5014 (0.5052)  acc1: 100.0000 (99.8687)  acc5: 100.0000 (99.9978)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3506.46214135028  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9978)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3623.613226331171  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3479.9379812803027  loss: 0.5022 (0.5053)  acc1: 100.0000 (99.8606)  acc5: 100.0000 (99.9979)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3509.7630961331024  loss: 0.5022 (0.5053)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9980)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3538.9837444463487  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3372.5589366032614  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3518.20411800941  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8682)  acc5: 100.0000 (99.9981)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3518.18106278547  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8659)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3516.79830209814  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [17]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3461.16000593116  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8632)  acc5: 100.0000 (99.9983)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3561.0492829758164  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [17] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.1093  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [18]  [  0/468]  eta: 0:01:43  lr: 0.0  img/s: 1582.0426281777382  loss: 0.5010 (0.5010)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.2210  data: 0.1400  max mem: 190</span><br><span class="line">Epoch: [18]  [ 10/468]  eta: 0:00:26  lr: 0.0  img/s: 3425.472707667375  loss: 0.5014 (0.5025)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0577  data: 0.0130  max mem: 190</span><br><span class="line">Epoch: [18]  [ 20/468]  eta: 0:00:21  lr: 0.0  img/s: 3583.296036735947  loss: 0.5018 (0.5044)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (100.0000)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [ 30/468]  eta: 0:00:19  lr: 0.0  img/s: 3592.9122436004686  loss: 0.5023 (0.5055)  acc1: 100.0000 (99.8236)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 40/468]  eta: 0:00:18  lr: 0.0  img/s: 3484.839652340986  loss: 0.5023 (0.5059)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3621.437806918137  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8162)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3508.08891909199  loss: 0.5019 (0.5055)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3505.271655316954  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3068.0616960117036  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8457)  acc5: 100.0000 (100.0000)  time: 0.0387  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3119.982054336772  loss: 0.5018 (0.5058)  acc1: 100.0000 (99.8455)  acc5: 100.0000 (100.0000)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3181.7023652192465  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3458.952349045177  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8522)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3476.737893250787  loss: 0.5021 (0.5061)  acc1: 100.0000 (99.8386)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3438.6367170737017  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3564.4065329969458  loss: 0.5016 (0.5057)  acc1: 100.0000 (99.8504)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 2829.6127294671987  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8500)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3410.263180628605  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8496)  acc5: 100.0000 (100.0000)  time: 0.0419  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3449.817263515033  loss: 0.5020 (0.5058)  acc1: 100.0000 (99.8492)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3422.459229793392  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8489)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 3534.1147908973016  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3625.668829984805  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (99.9961)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3558.1463498691055  loss: 0.5016 (0.5059)  acc1: 100.0000 (99.8519)  acc5: 100.0000 (99.9963)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3487.5335325451474  loss: 0.5018 (0.5059)  acc1: 100.0000 (99.8480)  acc5: 100.0000 (99.9965)  time: 0.0404  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [18]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3458.0611650735577  loss: 0.5018 (0.5057)  acc1: 100.0000 (99.8546)  acc5: 100.0000 (99.9966)  time: 0.0432  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [18]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3391.713334470494  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8574)  acc5: 100.0000 (99.9968)  time: 0.0404  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [18]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3510.267956035909  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8599)  acc5: 100.0000 (99.9969)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3547.191045979214  loss: 0.5015 (0.5057)  acc1: 100.0000 (99.8533)  acc5: 100.0000 (99.9970)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 2666.648017166018  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8587)  acc5: 100.0000 (99.9971)  time: 0.0450  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [18]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3161.4301815462345  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8610)  acc5: 100.0000 (99.9972)  time: 0.0500  data: 0.0033  max mem: 190</span><br><span class="line">Epoch: [18]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 2749.0535349449287  loss: 0.5010 (0.5054)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9973)  time: 0.0479  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [18]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 1436.9591023939017  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8676)  acc5: 100.0000 (99.9974)  time: 0.0534  data: 0.0050  max mem: 190</span><br><span class="line">Epoch: [18]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 1795.242673514974  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9975)  time: 0.0713  data: 0.0073  max mem: 190</span><br><span class="line">Epoch: [18]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3321.3372184381633  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8710)  acc5: 100.0000 (99.9976)  time: 0.0763  data: 0.0082  max mem: 190</span><br><span class="line">Epoch: [18]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3390.8564571240895  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9976)  time: 0.0527  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [18]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3459.866289448415  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9977)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [350/468]  eta: 0:00:05  lr: 0.0  img/s: 3461.9411650911156  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8709)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3527.357800817335  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9978)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [18]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3433.3809475084417  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3431.603346777544  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8667)  acc5: 100.0000 (99.9979)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3442.7602057175104  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9980)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3547.4723105081966  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3446.229816734602  loss: 0.5017 (0.5055)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9981)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [420/468]  eta: 0:00:02  lr: 0.0  img/s: 3311.5649642240314  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8627)  acc5: 100.0000 (99.9981)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3400.37059650634  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8622)  acc5: 100.0000 (99.9982)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3415.687386275433  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3421.913877061928  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8683)  acc5: 100.0000 (99.9983)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3543.0241867893274  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [18] Total time: 0:00:19</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1238  data: 0.1070  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [19]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2553.5975951408145  loss: 0.5011 (0.5011)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1595  data: 0.1094  max mem: 190</span><br><span class="line">Epoch: [19]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3504.425070823379  loss: 0.5017 (0.5064)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0489  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [19]  [ 20/468]  eta: 0:00:24  lr: 0.0  img/s: 3486.65020554751  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0484  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [19]  [ 30/468]  eta: 0:00:21  lr: 0.0  img/s: 3525.944661539573  loss: 0.5015 (0.5067)  acc1: 100.0000 (99.8488)  acc5: 100.0000 (100.0000)  time: 0.0498  data: 0.0034  max mem: 190</span><br><span class="line">Epoch: [19]  [ 40/468]  eta: 0:00:19  lr: 0.0  img/s: 3489.2788519657097  loss: 0.5016 (0.5076)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0394  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [19]  [ 50/468]  eta: 0:00:18  lr: 0.0  img/s: 3354.8351361315763  loss: 0.5016 (0.5069)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 60/468]  eta: 0:00:17  lr: 0.0  img/s: 3312.6683696765517  loss: 0.5014 (0.5068)  acc1: 100.0000 (99.8463)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 70/468]  eta: 0:00:17  lr: 0.0  img/s: 2751.462736134316  loss: 0.5014 (0.5070)  acc1: 100.0000 (99.8349)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 80/468]  eta: 0:00:16  lr: 0.0  img/s: 3579.8076440935642  loss: 0.5017 (0.5069)  acc1: 100.0000 (99.8360)  acc5: 100.0000 (100.0000)  time: 0.0396  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3475.297523336052  loss: 0.5018 (0.5067)  acc1: 100.0000 (99.8455)  acc5: 100.0000 (99.9914)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [100/468]  eta: 0:00:15  lr: 0.0  img/s: 3478.4273468832407  loss: 0.5018 (0.5063)  acc1: 100.0000 (99.8530)  acc5: 100.0000 (99.9923)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3450.704202901345  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8522)  acc5: 100.0000 (99.9930)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 3475.7925158617118  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (99.9935)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3526.477351550184  loss: 0.5016 (0.5058)  acc1: 100.0000 (99.8569)  acc5: 100.0000 (99.9940)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3456.5917150620016  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8615)  acc5: 100.0000 (99.9945)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3405.741748447382  loss: 0.5017 (0.5060)  acc1: 100.0000 (99.8448)  acc5: 100.0000 (99.9948)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3511.1403289624277  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9951)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3486.763427591671  loss: 0.5012 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9954)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3573.5646524754716  loss: 0.5015 (0.5060)  acc1: 100.0000 (99.8446)  acc5: 100.0000 (99.9957)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 2244.987965309314  loss: 0.5017 (0.5059)  acc1: 100.0000 (99.8487)  acc5: 100.0000 (99.9959)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 2122.91724958777  loss: 0.5017 (0.5059)  acc1: 100.0000 (99.8484)  acc5: 100.0000 (99.9961)  time: 0.0410  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [19]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3555.3894120607674  loss: 0.5016 (0.5058)  acc1: 100.0000 (99.8556)  acc5: 100.0000 (99.9963)  time: 0.0397  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [19]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3395.510220603117  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8480)  acc5: 100.0000 (99.9965)  time: 0.0376  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [19]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3594.1870765605336  loss: 0.5019 (0.5059)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [240/468]  eta: 0:00:08  lr: 0.0  img/s: 3571.2113241936236  loss: 0.5017 (0.5058)  acc1: 100.0000 (99.8541)  acc5: 100.0000 (99.9968)  time: 0.0363  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3560.0102913677174  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8537)  acc5: 100.0000 (99.9969)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3507.401364099616  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8533)  acc5: 100.0000 (99.9970)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3432.788209341731  loss: 0.5022 (0.5058)  acc1: 100.0000 (99.8530)  acc5: 100.0000 (99.9971)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3539.5203818590576  loss: 0.5018 (0.5058)  acc1: 100.0000 (99.8526)  acc5: 100.0000 (99.9972)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3547.2613579300682  loss: 0.5017 (0.5056)  acc1: 100.0000 (99.8577)  acc5: 100.0000 (99.9973)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 2699.906018667525  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9974)  time: 0.0444  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [19]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3534.952506995885  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8643)  acc5: 100.0000 (99.9975)  time: 0.0490  data: 0.0022  max mem: 190</span><br><span class="line">Epoch: [19]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3387.0485972228357  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9976)  time: 0.0421  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [19]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3504.2649521882445  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9976)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3549.5832170791214  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9977)  time: 0.0370  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3632.6606130320047  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8731)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3434.5230942449907  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8723)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3486.7860728829082  loss: 0.5017 (0.5051)  acc1: 100.0000 (99.8737)  acc5: 100.0000 (99.9979)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3491.0259191343816  loss: 0.5014 (0.5052)  acc1: 100.0000 (99.8729)  acc5: 100.0000 (99.9979)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3496.4597940695685  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (99.9980)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3354.898029070276  loss: 0.5026 (0.5053)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3429.082752101377  loss: 0.5030 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9981)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3449.2853187019346  loss: 0.5028 (0.5055)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (99.9981)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3384.3156428278753  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8604)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3651.8849617718283  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9982)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [19]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3311.217747953891  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3503.921890092677  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [19] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1225  data: 0.1034  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [20]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2341.7148440226115  loss: 0.5010 (0.5010)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1601  data: 0.1053  max mem: 190</span><br><span class="line">Epoch: [20]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3526.8248448021022  loss: 0.5020 (0.5086)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (99.9290)  time: 0.0480  data: 0.0099  max mem: 190</span><br><span class="line">Epoch: [20]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3593.922414197064  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (99.9628)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 30/468]  eta: 0:00:17  lr: 0.0  img/s: 3534.9059567939844  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8488)  acc5: 100.0000 (99.9748)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3485.608907644863  loss: 0.5011 (0.5047)  acc1: 100.0000 (99.8857)  acc5: 100.0000 (99.9809)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3497.3936653941864  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9847)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [ 60/468]  eta: 0:00:15  lr: 0.0  img/s: 3581.527098065377  loss: 0.5021 (0.5058)  acc1: 100.0000 (99.8591)  acc5: 100.0000 (99.9872)  time: 0.0377  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3528.6329142211152  loss: 0.5021 (0.5062)  acc1: 100.0000 (99.8349)  acc5: 100.0000 (99.9890)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 2350.4599690908058  loss: 0.5021 (0.5064)  acc1: 100.0000 (99.8167)  acc5: 100.0000 (99.9904)  time: 0.0438  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [20]  [ 90/468]  eta: 0:00:16  lr: 0.0  img/s: 2334.2924002034842  loss: 0.5032 (0.5071)  acc1: 100.0000 (99.7940)  acc5: 100.0000 (99.9914)  time: 0.0559  data: 0.0035  max mem: 190</span><br><span class="line">Epoch: [20]  [100/468]  eta: 0:00:16  lr: 0.0  img/s: 2650.677699823739  loss: 0.5027 (0.5071)  acc1: 100.0000 (99.7989)  acc5: 100.0000 (99.9923)  time: 0.0599  data: 0.0053  max mem: 190</span><br><span class="line">Epoch: [20]  [110/468]  eta: 0:00:16  lr: 0.0  img/s: 3581.6704604587244  loss: 0.5015 (0.5067)  acc1: 100.0000 (99.8170)  acc5: 100.0000 (99.9930)  time: 0.0547  data: 0.0049  max mem: 190</span><br><span class="line">Epoch: [20]  [120/468]  eta: 0:00:15  lr: 0.0  img/s: 3442.914752941931  loss: 0.5014 (0.5064)  acc1: 100.0000 (99.8192)  acc5: 100.0000 (99.9935)  time: 0.0442  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [20]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3491.4345767649966  loss: 0.5014 (0.5060)  acc1: 100.0000 (99.8330)  acc5: 100.0000 (99.9940)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [140/468]  eta: 0:00:14  lr: 0.0  img/s: 3387.8607929626614  loss: 0.5012 (0.5059)  acc1: 100.0000 (99.8338)  acc5: 100.0000 (99.9945)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3553.7184804697067  loss: 0.5013 (0.5059)  acc1: 100.0000 (99.8344)  acc5: 100.0000 (99.9948)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [160/468]  eta: 0:00:13  lr: 0.0  img/s: 3362.335988776993  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (99.9951)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3438.6587415453987  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8492)  acc5: 100.0000 (99.9954)  time: 0.0374  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [180/468]  eta: 0:00:12  lr: 0.0  img/s: 3528.2155029080272  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8532)  acc5: 100.0000 (99.9957)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 1953.906248180283  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (99.9959)  time: 0.0404  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [200/468]  eta: 0:00:11  lr: 0.0  img/s: 3480.118443228667  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (99.9961)  time: 0.0413  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3652.009169631378  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9963)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 3518.8959152640136  loss: 0.5023 (0.5051)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9965)  time: 0.0387  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3478.3597372137924  loss: 0.5024 (0.5052)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3593.104613263551  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9968)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3502.0705148694396  loss: 0.5011 (0.5050)  acc1: 100.0000 (99.8693)  acc5: 100.0000 (99.9969)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3548.550904536231  loss: 0.5014 (0.5049)  acc1: 100.0000 (99.8713)  acc5: 100.0000 (99.9970)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 2910.8002667519695  loss: 0.5018 (0.5049)  acc1: 100.0000 (99.8732)  acc5: 100.0000 (99.9971)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3321.871535791408  loss: 0.5018 (0.5048)  acc1: 100.0000 (99.8777)  acc5: 100.0000 (99.9972)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3177.1833563147648  loss: 0.5011 (0.5048)  acc1: 100.0000 (99.8792)  acc5: 100.0000 (99.9973)  time: 0.0402  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3421.7175926220993  loss: 0.5013 (0.5047)  acc1: 100.0000 (99.8832)  acc5: 100.0000 (99.9974)  time: 0.0398  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3331.3326797304508  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8819)  acc5: 100.0000 (99.9975)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3345.3444414673204  loss: 0.5013 (0.5048)  acc1: 100.0000 (99.8807)  acc5: 100.0000 (99.9976)  time: 0.0390  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3488.5759808699495  loss: 0.5024 (0.5050)  acc1: 100.0000 (99.8749)  acc5: 100.0000 (99.9976)  time: 0.0384  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3523.722996344161  loss: 0.5023 (0.5049)  acc1: 100.0000 (99.8740)  acc5: 100.0000 (99.9977)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 2873.1184416140427  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8776)  acc5: 100.0000 (99.9978)  time: 0.0456  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [20]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3543.959706645367  loss: 0.5016 (0.5051)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9978)  time: 0.0500  data: 0.0031  max mem: 190</span><br><span class="line">Epoch: [20]  [370/468]  eta: 0:00:04  lr: 0.0  img/s: 3406.02263614678  loss: 0.5031 (0.5052)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0420  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [20]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3502.6417182076775  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9979)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3555.130432479323  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3579.879255046043  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8656)  acc5: 100.0000 (99.9981)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3541.8321150547567  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (99.9981)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [20]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3626.5015232266737  loss: 0.5020 (0.5055)  acc1: 100.0000 (99.8608)  acc5: 100.0000 (99.9981)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3511.5077736135368  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8622)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3442.4290798104607  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9982)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3436.281727642797  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3585.8808693677447  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [20] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1171  data: 0.1000  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [21]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2728.0719123961485  loss: 0.5023 (0.5023)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1582  data: 0.1113  max mem: 190</span><br><span class="line">Epoch: [21]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3509.7630961331024  loss: 0.5019 (0.5035)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0104  max mem: 190</span><br><span class="line">Epoch: [21]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3548.363275854092  loss: 0.5011 (0.5025)  acc1: 100.0000 (99.9256)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 30/468]  eta: 0:00:17  lr: 0.0  img/s: 3536.9320245075432  loss: 0.5011 (0.5033)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3387.1127037803462  loss: 0.5015 (0.5049)  acc1: 100.0000 (99.8476)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3461.0038164002062  loss: 0.5020 (0.5049)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3559.3258328637253  loss: 0.5020 (0.5063)  acc1: 100.0000 (99.8335)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3587.1507166004076  loss: 0.5018 (0.5066)  acc1: 100.0000 (99.8239)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3504.219206694211  loss: 0.5022 (0.5073)  acc1: 100.0000 (99.7975)  acc5: 100.0000 (99.9904)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3514.7031882160395  loss: 0.5023 (0.5071)  acc1: 100.0000 (99.8111)  acc5: 100.0000 (99.9914)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3495.3898720002085  loss: 0.5015 (0.5065)  acc1: 100.0000 (99.8298)  acc5: 100.0000 (99.9923)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3589.837127974698  loss: 0.5015 (0.5065)  acc1: 100.0000 (99.8311)  acc5: 100.0000 (99.9930)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3519.795658530509  loss: 0.5017 (0.5061)  acc1: 100.0000 (99.8386)  acc5: 100.0000 (99.9935)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3226.63977354  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8449)  acc5: 100.0000 (99.9940)  time: 0.0443  data: 0.0013  max mem: 190</span><br><span class="line">Epoch: [21]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3418.7324851309872  loss: 0.5012 (0.5058)  acc1: 100.0000 (99.8504)  acc5: 100.0000 (99.9945)  time: 0.0494  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [21]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3301.728209196632  loss: 0.5011 (0.5056)  acc1: 100.0000 (99.8551)  acc5: 100.0000 (99.9948)  time: 0.0424  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [21]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3516.1994432982938  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9951)  time: 0.0375  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [21]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3412.6910930865265  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (99.9954)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3458.840925929505  loss: 0.5017 (0.5052)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (99.9957)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 3420.583945512351  loss: 0.5025 (0.5056)  acc1: 100.0000 (99.8609)  acc5: 100.0000 (99.9959)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3542.0657913835194  loss: 0.5025 (0.5056)  acc1: 100.0000 (99.8640)  acc5: 100.0000 (99.9961)  time: 0.0401  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [21]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 2264.1699083992644  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8704)  acc5: 100.0000 (99.9963)  time: 0.0423  data: 0.0006  max mem: 190</span><br><span class="line">Epoch: [21]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3493.297450646123  loss: 0.5017 (0.5056)  acc1: 100.0000 (99.8657)  acc5: 100.0000 (99.9965)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3334.0221079563803  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9966)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [240/468]  eta: 0:00:08  lr: 0.0  img/s: 3511.278111694648  loss: 0.5009 (0.5056)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9968)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3417.8183855360326  loss: 0.5010 (0.5054)  acc1: 100.0000 (99.8724)  acc5: 100.0000 (99.9969)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3480.8630466495933  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (99.9970)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3405.2881046315442  loss: 0.5010 (0.5053)  acc1: 100.0000 (99.8760)  acc5: 100.0000 (99.9971)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3434.2814229147875  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (99.9972)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3563.2236808920156  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9973)  time: 0.0376  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3592.4794871623294  loss: 0.5019 (0.5055)  acc1: 100.0000 (99.8650)  acc5: 100.0000 (99.9974)  time: 0.0386  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3356.597030229141  loss: 0.5019 (0.5054)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9975)  time: 0.0393  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3536.978628086542  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8686)  acc5: 100.0000 (99.9976)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3451.014739440377  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8702)  acc5: 100.0000 (99.9976)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3516.4527817441085  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9977)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3412.734480084417  loss: 0.5018 (0.5055)  acc1: 100.0000 (99.8665)  acc5: 100.0000 (99.9978)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3465.7851342104245  loss: 0.5018 (0.5056)  acc1: 100.0000 (99.8658)  acc5: 100.0000 (99.9978)  time: 0.0376  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3468.4272165800967  loss: 0.5020 (0.5056)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9979)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3363.4102780962403  loss: 0.5013 (0.5055)  acc1: 100.0000 (99.8647)  acc5: 100.0000 (99.9979)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 2199.209044732099  loss: 0.5015 (0.5056)  acc1: 100.0000 (99.8581)  acc5: 100.0000 (99.9980)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 2553.67047351773  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8617)  acc5: 100.0000 (99.9981)  time: 0.0502  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [21]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 1813.054809971835  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8612)  acc5: 100.0000 (99.9981)  time: 0.0729  data: 0.0018  max mem: 190</span><br><span class="line">Epoch: [21]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3504.1505906925136  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0660  data: 0.0021  max mem: 190</span><br><span class="line">Epoch: [21]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3498.3052402486546  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9982)  time: 0.0414  data: 0.0015  max mem: 190</span><br><span class="line">Epoch: [21]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3455.47932650224  loss: 0.5026 (0.5055)  acc1: 100.0000 (99.8654)  acc5: 100.0000 (99.9982)  time: 0.0375  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3469.52553654864  loss: 0.5017 (0.5055)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0379  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [21]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3408.40123417601  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8644)  acc5: 100.0000 (99.9983)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [21] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1211  data: 0.1003  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [22]  [  0/468]  eta: 0:01:15  lr: 0.0  img/s: 2650.978737692452  loss: 0.5289 (0.5289)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1605  data: 0.1121  max mem: 190</span><br><span class="line">Epoch: [22]  [ 10/468]  eta: 0:00:21  lr: 0.0  img/s: 3643.780071807193  loss: 0.5014 (0.5080)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0478  data: 0.0105  max mem: 190</span><br><span class="line">Epoch: [22]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3613.248478974856  loss: 0.5017 (0.5065)  acc1: 100.0000 (99.8140)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3002.331487881533  loss: 0.5025 (0.5070)  acc1: 100.0000 (99.7984)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3119.275085118003  loss: 0.5025 (0.5063)  acc1: 100.0000 (99.8285)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3240.760777969601  loss: 0.5020 (0.5059)  acc1: 100.0000 (99.8315)  acc5: 100.0000 (100.0000)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3273.243863478399  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8591)  acc5: 100.0000 (100.0000)  time: 0.0399  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 70/468]  eta: 0:00:16  lr: 0.0  img/s: 3546.089855876564  loss: 0.5017 (0.5057)  acc1: 100.0000 (99.8460)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3495.6857423769866  loss: 0.5015 (0.5052)  acc1: 100.0000 (99.8553)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3509.327197615437  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3441.8111485078693  loss: 0.5012 (0.5050)  acc1: 100.0000 (99.8685)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3550.592648439877  loss: 0.5015 (0.5047)  acc1: 100.0000 (99.8733)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3530.7678931965406  loss: 0.5019 (0.5047)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3508.9831436806776  loss: 0.5032 (0.5050)  acc1: 100.0000 (99.8628)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3529.468033212588  loss: 0.5011 (0.5050)  acc1: 100.0000 (99.8615)  acc5: 100.0000 (100.0000)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 2792.9587614385377  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8603)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 2942.4194586181006  loss: 0.5013 (0.5049)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (100.0000)  time: 0.0412  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 2722.703843637636  loss: 0.5013 (0.5051)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (100.0000)  time: 0.0481  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3555.0362674400894  loss: 0.5030 (0.5052)  acc1: 100.0000 (99.8576)  acc5: 100.0000 (100.0000)  time: 0.0497  data: 0.0020  max mem: 190</span><br><span class="line">Epoch: [22]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3453.900964365442  loss: 0.5035 (0.5053)  acc1: 100.0000 (99.8527)  acc5: 100.0000 (100.0000)  time: 0.0437  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [22]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3532.4173070849565  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8601)  acc5: 100.0000 (100.0000)  time: 0.0420  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3577.0647157981703  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0391  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [22]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3581.6704604587244  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (100.0000)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3663.3725597232365  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0360  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3384.7210369697887  loss: 0.5011 (0.5051)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (100.0000)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3542.907281534177  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8599)  acc5: 100.0000 (99.9969)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3555.4835958092162  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8623)  acc5: 100.0000 (99.9970)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3492.0704566150644  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9971)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3521.4348345118  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (99.9972)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [290/468]  eta: 0:00:06  lr: 0.0  img/s: 3505.0428083645074  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9973)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3506.233138931158  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8676)  acc5: 100.0000 (99.9974)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3525.0417722682564  loss: 0.5024 (0.5054)  acc1: 100.0000 (99.8618)  acc5: 100.0000 (99.9975)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3452.812512862729  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9976)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3561.8052942347244  loss: 0.5023 (0.5053)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9976)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [340/468]  eta: 0:00:04  lr: 0.0  img/s: 3574.278394716519  loss: 0.5026 (0.5055)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (99.9977)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3384.678359328702  loss: 0.5016 (0.5055)  acc1: 100.0000 (99.8598)  acc5: 100.0000 (99.9978)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3586.3839088291684  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8637)  acc5: 100.0000 (99.9978)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3521.6196261069204  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8652)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3221.6056214633327  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (99.9979)  time: 0.0375  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3571.116305367275  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3317.9503609215863  loss: 0.5021 (0.5053)  acc1: 100.0000 (99.8675)  acc5: 100.0000 (99.9981)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3300.0842861006613  loss: 0.5026 (0.5054)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (99.9981)  time: 0.0388  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3266.949700001217  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8664)  acc5: 100.0000 (99.9981)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3328.420585372507  loss: 0.5020 (0.5054)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9982)  time: 0.0382  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [22]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3564.5721949632502  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (99.9982)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3527.3114504218024  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8718)  acc5: 100.0000 (99.9983)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [22]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 2299.469375867327  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8712)  acc5: 100.0000 (99.9983)  time: 0.0474  data: 0.0027  max mem: 190</span><br><span class="line">Epoch: [22] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:09  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1234  data: 0.1042  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [23]  [  0/468]  eta: 0:01:14  lr: 0.0  img/s: 2733.669966190069  loss: 0.5012 (0.5012)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.1601  data: 0.1132  max mem: 190</span><br><span class="line">Epoch: [23]  [ 10/468]  eta: 0:00:22  lr: 0.0  img/s: 3446.274060712658  loss: 0.5017 (0.5046)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0492  data: 0.0106  max mem: 190</span><br><span class="line">Epoch: [23]  [ 20/468]  eta: 0:00:19  lr: 0.0  img/s: 3491.5708172370287  loss: 0.5015 (0.5045)  acc1: 100.0000 (99.8512)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 30/468]  eta: 0:00:18  lr: 0.0  img/s: 3525.875192098039  loss: 0.5010 (0.5035)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 40/468]  eta: 0:00:17  lr: 0.0  img/s: 3545.949327626747  loss: 0.5010 (0.5039)  acc1: 100.0000 (99.9047)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 50/468]  eta: 0:00:16  lr: 0.0  img/s: 3406.779102602339  loss: 0.5022 (0.5054)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3442.517373824165  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8719)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 70/468]  eta: 0:00:15  lr: 0.0  img/s: 3544.2638569806436  loss: 0.5016 (0.5044)  acc1: 100.0000 (99.8900)  acc5: 100.0000 (100.0000)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3308.952418519797  loss: 0.5017 (0.5041)  acc1: 100.0000 (99.9035)  acc5: 100.0000 (100.0000)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [ 90/468]  eta: 0:00:14  lr: 0.0  img/s: 3444.7925056143727  loss: 0.5013 (0.5038)  acc1: 100.0000 (99.9141)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3346.699946390056  loss: 0.5014 (0.5042)  acc1: 100.0000 (99.8994)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [110/468]  eta: 0:00:13  lr: 0.0  img/s: 3565.8269925611053  loss: 0.5017 (0.5043)  acc1: 100.0000 (99.9015)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [120/468]  eta: 0:00:13  lr: 0.0  img/s: 3452.7014849543066  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8902)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3512.19693966335  loss: 0.5024 (0.5048)  acc1: 100.0000 (99.8867)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [140/468]  eta: 0:00:12  lr: 0.0  img/s: 3590.485347029948  loss: 0.5020 (0.5048)  acc1: 100.0000 (99.8892)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3541.8087491176334  loss: 0.5026 (0.5050)  acc1: 100.0000 (99.8655)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [160/468]  eta: 0:00:11  lr: 0.0  img/s: 3459.130640962862  loss: 0.5020 (0.5050)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3506.5079454237884  loss: 0.5016 (0.5050)  acc1: 100.0000 (99.8721)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 2821.315311811909  loss: 0.5016 (0.5049)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [190/468]  eta: 0:00:10  lr: 0.0  img/s: 2042.9344353371844  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8609)  acc5: 100.0000 (100.0000)  time: 0.0486  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [23]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 2757.029867302083  loss: 0.5019 (0.5053)  acc1: 100.0000 (99.8562)  acc5: 100.0000 (100.0000)  time: 0.0597  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [23]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3406.9088163063275  loss: 0.5013 (0.5052)  acc1: 100.0000 (99.8593)  acc5: 100.0000 (100.0000)  time: 0.0557  data: 0.0046  max mem: 190</span><br><span class="line">Epoch: [23]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 2642.445376331384  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8586)  acc5: 100.0000 (100.0000)  time: 0.0512  data: 0.0032  max mem: 190</span><br><span class="line">Epoch: [23]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3511.8523228279496  loss: 0.5018 (0.5054)  acc1: 100.0000 (99.8580)  acc5: 100.0000 (100.0000)  time: 0.0497  data: 0.0029  max mem: 190</span><br><span class="line">Epoch: [23]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3404.683434166635  loss: 0.5018 (0.5052)  acc1: 100.0000 (99.8638)  acc5: 100.0000 (100.0000)  time: 0.0423  data: 0.0024  max mem: 190</span><br><span class="line">Epoch: [23]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3574.5639714498775  loss: 0.5021 (0.5055)  acc1: 100.0000 (99.8537)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3545.7151386265473  loss: 0.5029 (0.5056)  acc1: 100.0000 (99.8473)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3458.840925929505  loss: 0.5019 (0.5058)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3398.4548947618296  loss: 0.5020 (0.5057)  acc1: 100.0000 (99.8471)  acc5: 100.0000 (100.0000)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3445.367286169011  loss: 0.5016 (0.5056)  acc1: 100.0000 (99.8523)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3603.692572057619  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8521)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3618.4356242122785  loss: 0.5012 (0.5056)  acc1: 100.0000 (99.8543)  acc5: 100.0000 (99.9975)  time: 0.0364  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [320/468]  eta: 0:00:05  lr: 0.0  img/s: 3546.9098260473165  loss: 0.5012 (0.5055)  acc1: 100.0000 (99.8588)  acc5: 100.0000 (99.9976)  time: 0.0362  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [23]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3492.774736677748  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8607)  acc5: 100.0000 (99.9976)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3468.15834625323  loss: 0.5015 (0.5055)  acc1: 100.0000 (99.8602)  acc5: 100.0000 (99.9977)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3601.323566502992  loss: 0.5015 (0.5054)  acc1: 100.0000 (99.8642)  acc5: 100.0000 (99.9978)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 2893.6212487064504  loss: 0.5011 (0.5053)  acc1: 100.0000 (99.8680)  acc5: 100.0000 (99.9978)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3375.824741879095  loss: 0.5014 (0.5053)  acc1: 100.0000 (99.8694)  acc5: 100.0000 (99.9979)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3237.5166558120463  loss: 0.5017 (0.5052)  acc1: 100.0000 (99.8708)  acc5: 100.0000 (99.9979)  time: 0.0395  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3385.7456233287926  loss: 0.5013 (0.5054)  acc1: 100.0000 (99.8621)  acc5: 100.0000 (99.9980)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3163.553884682243  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8597)  acc5: 100.0000 (99.9981)  time: 0.0394  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3531.023335350293  loss: 0.5014 (0.5054)  acc1: 100.0000 (99.8631)  acc5: 100.0000 (99.9981)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3491.4345767649966  loss: 0.5012 (0.5054)  acc1: 100.0000 (99.8645)  acc5: 100.0000 (99.9981)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3502.344669219579  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8677)  acc5: 100.0000 (99.9982)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3352.5516242241065  loss: 0.5015 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3509.4418972538715  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8649)  acc5: 100.0000 (99.9983)  time: 0.0378  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3522.7056685235857  loss: 0.5021 (0.5054)  acc1: 100.0000 (99.8661)  acc5: 100.0000 (99.9983)  time: 0.0377  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [23] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1268  data: 0.1103  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [24]  [  0/468]  eta: 0:01:47  lr: 0.0  img/s: 1711.2187061733432  loss: 0.5025 (0.5025)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.2291  data: 0.1543  max mem: 190</span><br><span class="line">Epoch: [24]  [ 10/468]  eta: 0:00:27  lr: 0.0  img/s: 3611.1340611148107  loss: 0.5018 (0.5043)  acc1: 100.0000 (99.9290)  acc5: 100.0000 (100.0000)  time: 0.0607  data: 0.0158  max mem: 190</span><br><span class="line">Epoch: [24]  [ 20/468]  eta: 0:00:22  lr: 0.0  img/s: 3511.41590524092  loss: 0.5014 (0.5039)  acc1: 100.0000 (99.9256)  acc5: 100.0000 (100.0000)  time: 0.0407  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [24]  [ 30/468]  eta: 0:00:19  lr: 0.0  img/s: 3481.3144765424895  loss: 0.5014 (0.5041)  acc1: 100.0000 (99.9244)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 40/468]  eta: 0:00:18  lr: 0.0  img/s: 3515.209470431093  loss: 0.5016 (0.5042)  acc1: 100.0000 (99.9238)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 50/468]  eta: 0:00:17  lr: 0.0  img/s: 3570.3563367449406  loss: 0.5016 (0.5043)  acc1: 100.0000 (99.9234)  acc5: 100.0000 (100.0000)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 60/468]  eta: 0:00:16  lr: 0.0  img/s: 3415.687386275433  loss: 0.5020 (0.5050)  acc1: 100.0000 (99.9103)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 70/468]  eta: 0:00:16  lr: 0.0  img/s: 3545.2234424010303  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.9010)  acc5: 100.0000 (100.0000)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 80/468]  eta: 0:00:15  lr: 0.0  img/s: 3526.755340673201  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8939)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [ 90/468]  eta: 0:00:15  lr: 0.0  img/s: 3453.1012188454733  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0369  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [100/468]  eta: 0:00:14  lr: 0.0  img/s: 3487.6014993146546  loss: 0.5016 (0.5054)  acc1: 100.0000 (99.8840)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 2461.0849347220183  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8874)  acc5: 100.0000 (100.0000)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 2650.651525848832  loss: 0.5020 (0.5053)  acc1: 100.0000 (99.8838)  acc5: 100.0000 (100.0000)  time: 0.0487  data: 0.0037  max mem: 190</span><br><span class="line">Epoch: [24]  [130/468]  eta: 0:00:13  lr: 0.0  img/s: 3563.412884469873  loss: 0.5020 (0.5059)  acc1: 100.0000 (99.8628)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0042  max mem: 190</span><br><span class="line">Epoch: [24]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3567.5093329080532  loss: 0.5022 (0.5060)  acc1: 100.0000 (99.8559)  acc5: 100.0000 (100.0000)  time: 0.0371  data: 0.0008  max mem: 190</span><br><span class="line">Epoch: [24]  [150/468]  eta: 0:00:12  lr: 0.0  img/s: 3569.383099527957  loss: 0.5025 (0.5060)  acc1: 100.0000 (99.8500)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3519.657206542761  loss: 0.5023 (0.5060)  acc1: 100.0000 (99.8447)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [170/468]  eta: 0:00:11  lr: 0.0  img/s: 3487.624155493192  loss: 0.5023 (0.5062)  acc1: 100.0000 (99.8355)  acc5: 100.0000 (100.0000)  time: 0.0366  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3531.464640684098  loss: 0.5028 (0.5061)  acc1: 100.0000 (99.8403)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3589.5011065275094  loss: 0.5028 (0.5063)  acc1: 100.0000 (99.8282)  acc5: 100.0000 (100.0000)  time: 0.0368  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [200/468]  eta: 0:00:10  lr: 0.0  img/s: 3495.2533333333336  loss: 0.5031 (0.5062)  acc1: 100.0000 (99.8290)  acc5: 100.0000 (100.0000)  time: 0.0397  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3428.9075441330506  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8334)  acc5: 100.0000 (100.0000)  time: 0.0424  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [220/468]  eta: 0:00:09  lr: 0.0  img/s: 3414.38400386675  loss: 0.5015 (0.5061)  acc1: 100.0000 (99.8339)  acc5: 100.0000 (100.0000)  time: 0.0392  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3399.0143147471654  loss: 0.5015 (0.5060)  acc1: 100.0000 (99.8377)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3501.956296557212  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8412)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3344.2399960133553  loss: 0.5023 (0.5060)  acc1: 100.0000 (99.8413)  acc5: 100.0000 (100.0000)  time: 0.0381  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3525.6899536362084  loss: 0.5022 (0.5059)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [270/468]  eta: 0:00:07  lr: 0.0  img/s: 3487.352300776886  loss: 0.5019 (0.5060)  acc1: 100.0000 (99.8414)  acc5: 100.0000 (100.0000)  time: 0.0374  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 2152.0459854892374  loss: 0.5021 (0.5059)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0529  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [24]  [290/468]  eta: 0:00:07  lr: 0.0  img/s: 3302.459367522314  loss: 0.5020 (0.5060)  acc1: 100.0000 (99.8443)  acc5: 100.0000 (100.0000)  time: 0.0637  data: 0.0030  max mem: 190</span><br><span class="line">Epoch: [24]  [300/468]  eta: 0:00:06  lr: 0.0  img/s: 3530.41962254225  loss: 0.5014 (0.5058)  acc1: 100.0000 (99.8495)  acc5: 100.0000 (100.0000)  time: 0.0480  data: 0.0020  max mem: 190</span><br><span class="line">Epoch: [24]  [310/468]  eta: 0:00:06  lr: 0.0  img/s: 3457.0146105255026  loss: 0.5014 (0.5059)  acc1: 100.0000 (99.8468)  acc5: 100.0000 (99.9975)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [320/468]  eta: 0:00:06  lr: 0.0  img/s: 3494.411580543749  loss: 0.5013 (0.5058)  acc1: 100.0000 (99.8515)  acc5: 100.0000 (99.9976)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [330/468]  eta: 0:00:05  lr: 0.0  img/s: 3519.4956929894715  loss: 0.5011 (0.5058)  acc1: 100.0000 (99.8513)  acc5: 100.0000 (99.9976)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [340/468]  eta: 0:00:05  lr: 0.0  img/s: 3488.8480264098466  loss: 0.5015 (0.5058)  acc1: 100.0000 (99.8534)  acc5: 100.0000 (99.9977)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [350/468]  eta: 0:00:04  lr: 0.0  img/s: 3487.6014993146546  loss: 0.5016 (0.5057)  acc1: 100.0000 (99.8531)  acc5: 100.0000 (99.9978)  time: 0.0373  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [360/468]  eta: 0:00:04  lr: 0.0  img/s: 3494.88928236642  loss: 0.5014 (0.5056)  acc1: 100.0000 (99.8572)  acc5: 100.0000 (99.9978)  time: 0.0371  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [370/468]  eta: 0:00:03  lr: 0.0  img/s: 3483.6185915529513  loss: 0.5014 (0.5055)  acc1: 100.0000 (99.8610)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [380/468]  eta: 0:00:03  lr: 0.0  img/s: 3589.2371338030994  loss: 0.5013 (0.5055)  acc1: 100.0000 (99.8626)  acc5: 100.0000 (99.9979)  time: 0.0370  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [390/468]  eta: 0:00:03  lr: 0.0  img/s: 3577.636805874866  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8641)  acc5: 100.0000 (99.9980)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [400/468]  eta: 0:00:02  lr: 0.0  img/s: 3608.3186837559733  loss: 0.5017 (0.5054)  acc1: 100.0000 (99.8636)  acc5: 100.0000 (99.9981)  time: 0.0369  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [410/468]  eta: 0:00:02  lr: 0.0  img/s: 3593.03247222594  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8669)  acc5: 100.0000 (99.9981)  time: 0.0365  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [420/468]  eta: 0:00:01  lr: 0.0  img/s: 3527.8909179321718  loss: 0.5011 (0.5052)  acc1: 100.0000 (99.8701)  acc5: 100.0000 (99.9981)  time: 0.0371  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [430/468]  eta: 0:00:01  lr: 0.0  img/s: 3469.6824961869556  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8695)  acc5: 100.0000 (99.9982)  time: 0.0372  data: 0.0002  max mem: 190</span><br><span class="line">Epoch: [24]  [440/468]  eta: 0:00:01  lr: 0.0  img/s: 3280.2235731873475  loss: 0.5018 (0.5053)  acc1: 100.0000 (99.8671)  acc5: 100.0000 (99.9982)  time: 0.0373  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [450/468]  eta: 0:00:00  lr: 0.0  img/s: 3400.090640219381  loss: 0.5016 (0.5053)  acc1: 100.0000 (99.8683)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24]  [460/468]  eta: 0:00:00  lr: 0.0  img/s: 3525.0186273415493  loss: 0.5012 (0.5053)  acc1: 100.0000 (99.8678)  acc5: 100.0000 (99.9983)  time: 0.0383  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [24] Total time: 0:00:18</span><br><span class="line">Test:   [ 0/79]  eta: 0:00:10  loss: 0.5526 (0.5526)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 0.1272  data: 0.1089  max mem: 190</span><br><span class="line">Test:  Total time: 0:00:01</span><br><span class="line">Test:  Acc@1 98.760 Acc@5 99.900</span><br><span class="line">Epoch: [25]  [  0/468]  eta: 0:01:19  lr: 0.0  img/s: 2520.7810759796785  loss: 0.5107 (0.5107)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 0.1689  data: 0.1181  max mem: 190</span><br><span class="line">Epoch: [25]  [ 10/468]  eta: 0:00:23  lr: 0.0  img/s: 3186.9908166475716  loss: 0.5024 (0.5065)  acc1: 100.0000 (99.7869)  acc5: 100.0000 (100.0000)  time: 0.0521  data: 0.0111  max mem: 190</span><br><span class="line">Epoch: [25]  [ 20/468]  eta: 0:00:20  lr: 0.0  img/s: 3260.640089400675  loss: 0.5013 (0.5041)  acc1: 100.0000 (99.8884)  acc5: 100.0000 (100.0000)  time: 0.0400  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 30/468]  eta: 0:00:20  lr: 0.0  img/s: 2113.99792093243  loss: 0.5013 (0.5043)  acc1: 100.0000 (99.8992)  acc5: 100.0000 (100.0000)  time: 0.0456  data: 0.0007  max mem: 190</span><br><span class="line">Epoch: [25]  [ 40/468]  eta: 0:00:20  lr: 0.0  img/s: 3353.9967888847937  loss: 0.5018 (0.5048)  acc1: 100.0000 (99.8476)  acc5: 100.0000 (100.0000)  time: 0.0491  data: 0.0017  max mem: 190</span><br><span class="line">Epoch: [25]  [ 50/468]  eta: 0:00:19  lr: 0.0  img/s: 3436.0398087643284  loss: 0.5022 (0.5044)  acc1: 100.0000 (99.8775)  acc5: 100.0000 (100.0000)  time: 0.0437  data: 0.0023  max mem: 190</span><br><span class="line">Epoch: [25]  [ 60/468]  eta: 0:00:18  lr: 0.0  img/s: 3491.0713208136085  loss: 0.5016 (0.5045)  acc1: 100.0000 (99.8847)  acc5: 100.0000 (100.0000)  time: 0.0389  data: 0.0012  max mem: 190</span><br><span class="line">Epoch: [25]  [ 70/468]  eta: 0:00:17  lr: 0.0  img/s: 3572.019188417754  loss: 0.5019 (0.5045)  acc1: 100.0000 (99.8900)  acc5: 100.0000 (100.0000)  time: 0.0380  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 80/468]  eta: 0:00:16  lr: 0.0  img/s: 3435.6660011262993  loss: 0.5024 (0.5046)  acc1: 100.0000 (99.8843)  acc5: 100.0000 (100.0000)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [ 90/468]  eta: 0:00:16  lr: 0.0  img/s: 3535.301672593178  loss: 0.5018 (0.5043)  acc1: 100.0000 (99.8970)  acc5: 100.0000 (100.0000)  time: 0.0372  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [100/468]  eta: 0:00:15  lr: 0.0  img/s: 3394.9304852060527  loss: 0.5015 (0.5049)  acc1: 100.0000 (99.8840)  acc5: 100.0000 (100.0000)  time: 0.0367  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [110/468]  eta: 0:00:14  lr: 0.0  img/s: 3215.5082982457193  loss: 0.5013 (0.5053)  acc1: 100.0000 (99.8733)  acc5: 100.0000 (100.0000)  time: 0.0391  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [120/468]  eta: 0:00:14  lr: 0.0  img/s: 3210.969634985855  loss: 0.5017 (0.5053)  acc1: 100.0000 (99.8709)  acc5: 100.0000 (100.0000)  time: 0.0409  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [130/468]  eta: 0:00:14  lr: 0.0  img/s: 3372.6860574688094  loss: 0.5017 (0.5050)  acc1: 100.0000 (99.8807)  acc5: 100.0000 (100.0000)  time: 0.0408  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [140/468]  eta: 0:00:13  lr: 0.0  img/s: 3508.8226082637284  loss: 0.5019 (0.5051)  acc1: 100.0000 (99.8726)  acc5: 100.0000 (100.0000)  time: 0.0401  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [150/468]  eta: 0:00:13  lr: 0.0  img/s: 3403.971062459184  loss: 0.5014 (0.5051)  acc1: 100.0000 (99.8707)  acc5: 100.0000 (100.0000)  time: 0.0385  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [160/468]  eta: 0:00:12  lr: 0.0  img/s: 3460.245382007554  loss: 0.5012 (0.5052)  acc1: 100.0000 (99.8690)  acc5: 100.0000 (99.9951)  time: 0.0382  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [170/468]  eta: 0:00:12  lr: 0.0  img/s: 3412.3874149876056  loss: 0.5012 (0.5051)  acc1: 100.0000 (99.8766)  acc5: 100.0000 (99.9954)  time: 0.0379  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [180/468]  eta: 0:00:11  lr: 0.0  img/s: 3046.127945440206  loss: 0.5021 (0.5051)  acc1: 100.0000 (99.8748)  acc5: 100.0000 (99.9957)  time: 0.0423  data: 0.0010  max mem: 190</span><br><span class="line">Epoch: [25]  [190/468]  eta: 0:00:11  lr: 0.0  img/s: 3169.0065815492317  loss: 0.5019 (0.5049)  acc1: 100.0000 (99.8773)  acc5: 100.0000 (99.9959)  time: 0.0466  data: 0.0016  max mem: 190</span><br><span class="line">Epoch: [25]  [200/468]  eta: 0:00:11  lr: 0.0  img/s: 3170.6160978461794  loss: 0.5016 (0.5048)  acc1: 100.0000 (99.8795)  acc5: 100.0000 (99.9961)  time: 0.0438  data: 0.0011  max mem: 190</span><br><span class="line">Epoch: [25]  [210/468]  eta: 0:00:10  lr: 0.0  img/s: 3243.109978132438  loss: 0.5014 (0.5048)  acc1: 100.0000 (99.8741)  acc5: 100.0000 (99.9963)  time: 0.0413  data: 0.0005  max mem: 190</span><br><span class="line">Epoch: [25]  [220/468]  eta: 0:00:10  lr: 0.0  img/s: 3291.707513274228  loss: 0.5015 (0.5048)  acc1: 100.0000 (99.8763)  acc5: 100.0000 (99.9965)  time: 0.0406  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [230/468]  eta: 0:00:09  lr: 0.0  img/s: 3252.205986224777  loss: 0.5015 (0.5048)  acc1: 100.0000 (99.8782)  acc5: 100.0000 (99.9966)  time: 0.0402  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [240/468]  eta: 0:00:09  lr: 0.0  img/s: 3223.036818692105  loss: 0.5016 (0.5047)  acc1: 100.0000 (99.8833)  acc5: 100.0000 (99.9968)  time: 0.0404  data: 0.0004  max mem: 190</span><br><span class="line">Epoch: [25]  [250/468]  eta: 0:00:08  lr: 0.0  img/s: 3207.804019980402  loss: 0.5014 (0.5049)  acc1: 100.0000 (99.8786)  acc5: 100.0000 (99.9969)  time: 0.0403  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [260/468]  eta: 0:00:08  lr: 0.0  img/s: 3453.0123810932664  loss: 0.5013 (0.5049)  acc1: 100.0000 (99.8803)  acc5: 100.0000 (99.9970)  time: 0.0397  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [270/468]  eta: 0:00:08  lr: 0.0  img/s: 3455.723990550796  loss: 0.5015 (0.5050)  acc1: 100.0000 (99.8789)  acc5: 100.0000 (99.9971)  time: 0.0384  data: 0.0003  max mem: 190</span><br><span class="line">Epoch: [25]  [280/468]  eta: 0:00:07  lr: 0.0  img/s: 3354.436868939318  loss: 0.5015 (0.5050)  acc1: 100.0000 (99.8804)  acc5: 100.0000 (99.9972)  time: 0.0377  data: 0.0003  max mem: 190</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/blog/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">xiuqhou</div><div class="post-copyright__author_desc">冲冲冲！</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/')">swin transformer分类MNIST</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=swin transformer分类MNIST&amp;url=http://xiuqhou.github.io/blog/2022/09/28/20220928%20swin%20transformer%E5%88%86%E7%B1%BBMNIST/&amp;pic=https://github.com/xiuqhou/picx-images-hosting/raw/master/SwinTransformer结构.webp" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://xiuqhou.github.io/blog" target="_blank">xiuqhou的个人博客</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/blog/tags/Classification/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Classification<span class="tagsPageCount">2</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/arxiv-logo.svg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/blog/2022/08/28/20220828%20%E5%86%B3%E7%AD%96%E6%A0%91/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/决策树logo.webp" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">决策树</div></div></a></div><div class="next-post pull-right"><a href="/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/blog/2023/01/01/20230101%20T%20COGN%20DEV%20SYST2021.%20Bioinspired%20Visual-Integrated%20Model%20for%20Multilabel%20Classification%20of%20Textile%20Defect%20Images/" title="T COGN DEV SYST2021. Bioinspired Visual-Integrated Model for Multilabel Classification of Textile Defect Images"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/IEEElogo.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-01-01</div><div class="title">T COGN DEV SYST2021. Bioinspired Visual-Integrated Model for Multilabel Classification of Textile Defect Images</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description">Trying to be better!</div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/blog/"><h1 class="author-info__name">xiuqhou</h1><div class="author-info__desc">冲冲冲！</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/xiuqhou" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客鸭~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="toc-number">1.</span> <span class="toc-text">安装依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%AE%98%E6%96%B9%E8%AE%AD%E7%BB%83%E5%8F%82%E8%80%83%E4%B8%AD%E7%BB%99%E5%87%BA%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">pytorch官方训练参考中给出的相关代码</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/08/02/20240802%20%E3%80%90Arxiv2023%E3%80%91Detect%20Everything%20with%20Few%20Examples/" title="【Arxiv2023】Detect Everything with Few Examples"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/arxiv-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="【Arxiv2023】Detect Everything with Few Examples"/></a><div class="content"><a class="title" href="/blog/2024/08/02/20240802%20%E3%80%90Arxiv2023%E3%80%91Detect%20Everything%20with%20Few%20Examples/" title="【Arxiv2023】Detect Everything with Few Examples">【Arxiv2023】Detect Everything with Few Examples</a><time datetime="2024-08-02T20:50:00.000Z" title="发表于 2024-08-02 20:50:00">2024-08-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/24/20240724%20%E3%80%90CVPR2024%E3%80%91Few-Shot%20Object%20Detection%20with%20Foundation%20Models/" title="【CVPR2024】Few-Shot Object Detection with Foundation Models"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/cvpr-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="【CVPR2024】Few-Shot Object Detection with Foundation Models"/></a><div class="content"><a class="title" href="/blog/2024/07/24/20240724%20%E3%80%90CVPR2024%E3%80%91Few-Shot%20Object%20Detection%20with%20Foundation%20Models/" title="【CVPR2024】Few-Shot Object Detection with Foundation Models">【CVPR2024】Few-Shot Object Detection with Foundation Models</a><time datetime="2024-07-24T20:50:00.000Z" title="发表于 2024-07-24 20:50:00">2024-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/20/20240720%20%E3%80%90CVPR2023%E3%80%91Salience%20DETR%20Enhancing%20Detection%20Transformer%20with%20Hierarchical%20Salience%20Filtering%20Refinement/" title="【CVPR2024】Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/cvpr-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="【CVPR2024】Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement"/></a><div class="content"><a class="title" href="/blog/2024/07/20/20240720%20%E3%80%90CVPR2023%E3%80%91Salience%20DETR%20Enhancing%20Detection%20Transformer%20with%20Hierarchical%20Salience%20Filtering%20Refinement/" title="【CVPR2024】Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement">【CVPR2024】Salience DETR Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement</a><time datetime="2024-07-20T20:26:00.000Z" title="发表于 2024-07-20 20:26:00">2024-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2024/07/19/20240719%20%E3%80%90NIPS2023%E3%80%91Rank-DETR%20for%20High%20Quality%20Object%20Detection/" title="【NIPS2023】Rank-DETR for High Quality Object Detection"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://nips.cc/static/core/img/neurips-navbar-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="【NIPS2023】Rank-DETR for High Quality Object Detection"/></a><div class="content"><a class="title" href="/blog/2024/07/19/20240719%20%E3%80%90NIPS2023%E3%80%91Rank-DETR%20for%20High%20Quality%20Object%20Detection/" title="【NIPS2023】Rank-DETR for High Quality Object Detection">【NIPS2023】Rank-DETR for High Quality Object Detection</a><time datetime="2024-07-19T11:13:00.000Z" title="发表于 2024-07-19 11:13:00">2024-07-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2023/03/22/20230322%20Linux%E5%91%BD%E4%BB%A4/" title="Linux命令"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/Linux_logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="Linux命令"/></a><div class="content"><a class="title" href="/blog/2023/03/22/20230322%20Linux%E5%91%BD%E4%BB%A4/" title="Linux命令">Linux命令</a><time datetime="2023-03-22T09:27:00.000Z" title="发表于 2023-03-22 09:27:00">2023-03-22</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2024 By <a class="footer-bar-link" href="/blog/" title="xiuqhou" target="_blank">xiuqhou</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/blog/archives/" title="archive"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/blog/tags/" title="tag"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/blog/categories/" title="category"><div class="headline">分类</div><div class="length-num">5</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 0.88rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 0.88rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 0.88rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 0.88rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 0.88rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 0.88rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 0.88rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 0.88rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 0.88rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 0.88rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 0.88rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 0.88rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 0.88rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 0.88rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 0.88rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 0.88rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 0.88rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 0.88rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">优化算法<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 0.88rem;">博客<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 0.88rem;">服务器<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 0.88rem;">画图<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><script src="/blog/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 xiuqhou 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/blog/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    anzhiyu.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/blog/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/blog/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>