<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>xiuqhou的个人博客 - 冲冲冲！</title><meta name="author" content="xiuqhou"><meta name="copyright" content="xiuqhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="xiuqhou的个人博客"><meta name="application-name" content="xiuqhou的个人博客"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="website"><meta property="og:title" content="xiuqhou的个人博客"><meta property="og:url" content="http://xiuqhou.github.io/blog/page/2/index.html"><meta property="og:site_name" content="xiuqhou的个人博客"><meta property="og:description" content="Trying to be better!"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg"><meta property="article:author" content="xiuqhou"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg"><meta name="description" content="Trying to be better!"><link rel="shortcut icon" href="/blog/favicon.ico"><link rel="canonical" href="http://xiuqhou.github.io/blog/page/2/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/blog/',
  preloader: {"source":2},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: undefined,
  algolia: undefined,
  localSearch: {"path":"/blog/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: xiuqhou","link":"链接: ","source":"来源: xiuqhou的个人博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'xiuqhou的个人博客',
  title: 'xiuqhou的个人博客',
  postAI: '',
  pageFillDescription: '',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-08-02 13:27:14',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/blog/" accesskey="h"><div class="title">xiuqhou的个人博客</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 1.05rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 1.05rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 1.05rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 1.05rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 1.05rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 1.05rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 1.05rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 1.05rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 1.05rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 1.05rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 1.05rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 1.05rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 1.05rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 1.05rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 1.05rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 1.05rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 1.05rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 1.05rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">优化算法<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">博客<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">服务器<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 1.05rem;">画图<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/blog/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/03/"><span class="card-archive-list-date">三月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/02/"><span class="card-archive-list-date">二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/01/"><span class="card-archive-list-date">一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/12/"><span class="card-archive-list-date">十二月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/11/"><span class="card-archive-list-date">十一月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav></header><main id="blog-container"><div class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div id="categoryBar"><div class="category-bar" id="category-bar"><div id="catalog-bar"><div id="catalog-list"><div class="catalog-list-item" id="首页"><a href="/">首页</a></div>
    <div class="catalog-list-item" id="/categories/教程/">
      <a href="/categories/教程/">
        教程
      </a>
    </div>
    
    <div class="catalog-list-item" id="/categories/干货笔记/">
      <a href="/categories/干货笔记/">
        干货笔记
      </a>
    </div>
    
    <div class="catalog-list-item" id="/categories/工具总结/">
      <a href="/categories/工具总结/">
        工具总结
      </a>
    </div>
    
    <div class="catalog-list-item" id="/categories/代码/">
      <a href="/categories/代码/">
        代码
      </a>
    </div>
    
    <div class="catalog-list-item" id="/categories/论文精读/">
      <a href="/categories/论文精读/">
        论文精读
      </a>
    </div>
    </div><div class="category-bar-next" id="category-bar-next" onclick="anzhiyu.scrollCategoryBarToRight()"><i class="anzhiyufont anzhiyu-icon-angle-double-right"></i></div><a class="catalog-more" href="/categories/">更多</a></div></div></div><div class="recent-post-item lastestpost-item" onclick="pjax.loadUrl('/blog/2022/12/04/20221204%20NIPS2018.%20Hybrid%20Knowledge%20Routed%20Modules%20for%20Large-scale%20Object%20Detection/')"><div class="post_cover left"><a href="/blog/2022/12/04/20221204%20NIPS2018.%20Hybrid%20Knowledge%20Routed%20Modules%20for%20Large-scale%20Object%20Detection/" title="NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/neurips-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/12/04/20221204%20NIPS2018.%20Hybrid%20Knowledge%20Routed%20Modules%20for%20Large-scale%20Object%20Detection/" title="NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection">未读</a></div><a class="article-title" href="/blog/2022/12/04/20221204%20NIPS2018.%20Hybrid%20Knowledge%20Routed%20Modules%20for%20Large-scale%20Object%20Detection/" title="NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection">NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-04T10:52:00.000Z" title="发表于 2022-12-04 10:52:00" time="2022-12-04 10:52:00">2022-12-04</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Object-Detection/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Object Detection</span></a><a class="article-meta__tags" href="/blog/tags/Knowledge-Graph/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Knowledge Graph</span></a></span></div><div class="content">NIPS2018. Hybrid Knowledge Routed Modules for Large-scale Object Detection
论文地址
代码地址
主流的目标检测方法单独处理每个区域的识别问题，忽略了一个场景内物体之间存在的语义互相关性。着会导致面临长尾问题时后续的性能下降。

长尾问题指少量种类的样本占据了大多数，而大量类别仅有少数样本。

文章利用了多种人类常识来推理大尺度目标类别以及达到一张图片内语义上的连贯性。文章提出了混合知识路由模块Hybrid Knowledge Routed Modules(HKRM)融合两种形式的推理：精确的知识模块和非精确的知识模块。通过在区域-区域的图结构上建模，两种模块均可以被个性化，并在特定知识形式的指导下与每张图片的视觉模式相协调。HKRM轻量、通用可以轻松地集成多种知识来赋予任何检测网络全局语义推理的能力。在大规模目标检测基准数据集上的实验表明HKRM在VisualGenome和ADE数据集上分别能够获得大约34.5%的mAP提升。
主要问题
多数基于区域目标检测方法将每个候选框单独地看作是分类和回归问题，因此检测效果 ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/12/03/20221203%20TMM2017.%20Attentive%20Contexts%20for%20Object%20Detection/')"><div class="post_cover left"><a href="/blog/2022/12/03/20221203%20TMM2017.%20Attentive%20Contexts%20for%20Object%20Detection/" title="TMM2017. Attentive Contexts for Object Detection" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/IEEElogo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="TMM2017. Attentive Contexts for Object Detection" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/12/03/20221203%20TMM2017.%20Attentive%20Contexts%20for%20Object%20Detection/" title="TMM2017. Attentive Contexts for Object Detection">未读</a></div><a class="article-title" href="/blog/2022/12/03/20221203%20TMM2017.%20Attentive%20Contexts%20for%20Object%20Detection/" title="TMM2017. Attentive Contexts for Object Detection">TMM2017. Attentive Contexts for Object Detection</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-03T10:33:00.000Z" title="发表于 2022-12-03 10:33:00" time="2022-12-03 10:33:00">2022-12-03</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Attention/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Attention</span></a><a class="article-meta__tags" href="/blog/tags/Object-Detection/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Object Detection</span></a><a class="article-meta__tags" href="/blog/tags/Context/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Context</span></a></span></div><div class="content">【TMM 2017】Attentive Contexts for Object Detection

北京理工大学、北京交通大学、中山大学、新加坡国立大学
论文地址：https://arxiv.org/pdf/1603.07415.pdf
作者简介：Shuicheng Yan（颜水成），北京大学博士学位，微软亚洲研究院实习，香港中文大学汤晓鸥教授的多媒体实验室任博士后，美国伊利诺伊大学香槟分校师从黄煦涛（Tomas Huang），后加入新加坡国立大学创立机器学习与计算机视觉实验室，拥有终身教职。目前与昆仑万维创始人周亚辉一起出任天工智能联席CEO，并兼任昆仑万维2050全球研究院院长。

本文首次使用基于注意力机制的全局和局部上下文信息来进行目标检测，并通过LSTM递归地生成注意力图，最终融合全局和局部上下文信息提高检测性能！
文章贡献/创新点

文章提出了最新的注意力到上下文CNN（AC-CNN）目标检测模型，能够有效地上下文化主流基于候选框的CNN检测器。
基于注意力机制的全局上下文子网能够递归地生成注意力位置图来帮助利用最具判别性的特征以指导局部目标检测。
每个候选框内外的局部上 ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/12/01/20221201%20ICCV2017.%20Spatial%20Memory%20for%20Context%20Reasoning%20in%20Object%20Detection/')"><div class="post_cover left"><a href="/blog/2022/12/01/20221201%20ICCV2017.%20Spatial%20Memory%20for%20Context%20Reasoning%20in%20Object%20Detection/" title="ICCV2017. Spatial Memory for Context Reasoning in Object Detection" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/ICCV2017logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="ICCV2017. Spatial Memory for Context Reasoning in Object Detection" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/12/01/20221201%20ICCV2017.%20Spatial%20Memory%20for%20Context%20Reasoning%20in%20Object%20Detection/" title="ICCV2017. Spatial Memory for Context Reasoning in Object Detection">未读</a></div><a class="article-title" href="/blog/2022/12/01/20221201%20ICCV2017.%20Spatial%20Memory%20for%20Context%20Reasoning%20in%20Object%20Detection/" title="ICCV2017. Spatial Memory for Context Reasoning in Object Detection">ICCV2017. Spatial Memory for Context Reasoning in Object Detection</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-01T21:59:00.000Z" title="发表于 2022-12-01 21:59:00" time="2022-12-01 21:59:00">2022-12-01</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Object-Detection/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Object Detection</span></a><a class="article-meta__tags" href="/blog/tags/Context/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Context</span></a></span></div><div class="content">ICCV2017. Spatial Memory for Context Reasoning in Object Detection
建模实例级的情景、物体与物体之间的关系时非常有挑战的，因为需要推理不同类别、位置的锚框。实例级别的空间推理本质上需要建模以往检测之上的条件分布。但最新的目标检测方法基本都是并行检测所有目标，然后执行非极大值抑制。尽管在图片caption等任务中使用了图像级别的记忆，但是没有捕获空间布局的关系。另一方面，建模物体和物体之间的关系需要空间推理，我们不仅需要存储空间布局，也需要有效的推理模块来提取空间模式。文章提出了概念上简单但有效的空间记忆网络Spatial Memory Network(SMN)来建模实例级别的情景关系。空间记忆将物体实例集成会了伪“图像”表示，可以喂给其他卷积网络来完成物体到物体的情景推理。这就引出顺序推理体系结构，其中图片和记忆是被并行地处理，用于获取检测结果，检测结果又会用于更新记忆。SMN在COCO数据集上比Faster RCNN获得了2.2%的提升。
主要问题
情景对图片理解和视觉识别任务非常重要，目前有两类常用的情景模型：图像/ ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/12/01/20221201%20CVPRW2022.%20ResNeSt-Split-Attention%20Networks/')"><div class="post_cover left"><a href="/blog/2022/12/01/20221201%20CVPRW2022.%20ResNeSt-Split-Attention%20Networks/" title="CVPRW2022. ResNeSt Split-Attention Networks" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPRW2022. ResNeSt Split-Attention Networks" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/12/01/20221201%20CVPRW2022.%20ResNeSt-Split-Attention%20Networks/" title="CVPRW2022. ResNeSt Split-Attention Networks">未读</a></div><a class="article-title" href="/blog/2022/12/01/20221201%20CVPRW2022.%20ResNeSt-Split-Attention%20Networks/" title="CVPRW2022. ResNeSt Split-Attention Networks">CVPRW2022. ResNeSt Split-Attention Networks</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-01T14:55:00.000Z" title="发表于 2022-12-01 14:55:00" time="2022-12-01 14:55:00">2022-12-01</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Attention/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Attention</span></a></span></div><div class="content">CVPRW2022. ResNeSt: Split-Attention Networks
为了增强卷积网络的表示能力，文章提出了多分枝的架构，采用不同网络分支之间按通道的注意力来利用特征图注意力和多路径表示的互补能力。所提的模块可以作为残差块简易的替换，产生多特征交互的更强的表示能力。使用所提模块替换ResNet的基本模块，文章提出了新的ResNeSt网络。
主要问题
卷积神经网络能够聚集空间和通道维度上的邻域信息，实现稠密的特征练级，Inception网络探索学习独立的特征多路径表示，对每个通道维度采用单独的卷积滤波核，最后再拼接到一起，因此能够实现输入通道连接的解耦。已有工作探索了空间和通道依赖性，利用了注意力机制。SE通道注意力采用全局池化来压缩通道统计量，预测一组注意力因子来和原始特征做通道乘积，从而使用全局情景信息来选择性地强调特征。这种注意力机制和人类主是网络地注意力选择阶段很相似，都在于寻找所识别物体中最有信息量的部分。
文章贡献/创新点
文章提出一种结合多路径网络布局和按通道注意力策略的网络架构Split-Attention Block，能够捕获跨通道之间的特征相关性， ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/29/20221129%20TPAMI2022(Submission).%20UniFormer-Unifying%20Convolution%20and%20Self-attention%20for%20Visual%20Recognition/')"><div class="post_cover left"><a href="/blog/2022/11/29/20221129%20TPAMI2022(Submission).%20UniFormer-Unifying%20Convolution%20and%20Self-attention%20for%20Visual%20Recognition/" title="TPAMI2022(Submission). UniFormer-Unifying Convolution and Self-attention for Visual Revognition" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/arxiv-logo.svg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="TPAMI2022(Submission). UniFormer-Unifying Convolution and Self-attention for Visual Revognition" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/29/20221129%20TPAMI2022(Submission).%20UniFormer-Unifying%20Convolution%20and%20Self-attention%20for%20Visual%20Recognition/" title="TPAMI2022(Submission). UniFormer-Unifying Convolution and Self-attention for Visual Revognition">未读</a></div><a class="article-title" href="/blog/2022/11/29/20221129%20TPAMI2022(Submission).%20UniFormer-Unifying%20Convolution%20and%20Self-attention%20for%20Visual%20Recognition/" title="TPAMI2022(Submission). UniFormer-Unifying Convolution and Self-attention for Visual Revognition">TPAMI2022(Submission). UniFormer-Unifying Convolution and Self-attention for Visual Revognition</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-29T10:44:00.000Z" title="发表于 2022-11-29 10:44:00" time="2022-11-29 10:44:00">2022-11-29</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Attention/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Attention</span></a><a class="article-meta__tags" href="/blog/tags/Transformer/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Transformer</span></a></span></div><div class="content">TPAMI2022(Submission). UniFormer: Unifying Convolution and Self-attention for Visual Revognition
这篇文章是ICLR的接受论文，目前扩展后投往TPAMI但还未接受。
论文地址
代码地址
由于视觉数据中较大的局部冗余和复杂的全局依赖，学习图像和视频确定性的表示是一项具有挑战性的任务。卷积网络能够有效通过小邻域聚集有效降低局部冗余，但感受野尺寸使其难以捕获长期依赖，视觉transformer能够有效捕获长期依赖，但是盲目地在所有token之间建立相似性比较会带来较高的冗余。文章提出了一种最新的Unified transFormer(UniFormer)，无缝集成卷积和自注意力的优点。文章在浅层和深层分别使用全局token affinity，因此能够通过有效的表示学习解决局部冗余和全局依赖。
主要问题
视频数据中面临两项挑战，局部冗余，即局部区域内的视觉内容很相似；复杂的全局依赖，即不同区域的目标之间具有动态关系。主流方法如卷积和ViTs分别使用卷积和自注意力作为核心操作，但仅能解决其中一个问题。 ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/24/20221124%20CVPR2022.%20Semantic-Aware%20Domain%20Generalized%20Segmentation/')"><div class="post_cover left"><a href="/blog/2022/11/24/20221124%20CVPR2022.%20Semantic-Aware%20Domain%20Generalized%20Segmentation/" title="CVPR2022. Semantic-Aware Domain Generalized Segmentation" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPR2022. Semantic-Aware Domain Generalized Segmentation" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/24/20221124%20CVPR2022.%20Semantic-Aware%20Domain%20Generalized%20Segmentation/" title="CVPR2022. Semantic-Aware Domain Generalized Segmentation">未读</a></div><a class="article-title" href="/blog/2022/11/24/20221124%20CVPR2022.%20Semantic-Aware%20Domain%20Generalized%20Segmentation/" title="CVPR2022. Semantic-Aware Domain Generalized Segmentation">CVPR2022. Semantic-Aware Domain Generalized Segmentation</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-24T19:46:00.000Z" title="发表于 2022-11-24 19:46:00" time="2022-11-24 19:46:00">2022-11-24</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Semantic-Segmentation/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Semantic Segmentation</span></a><a class="article-meta__tags" href="/blog/tags/Domain-Generalization/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Domain Generalization</span></a></span></div><div class="content">CVPR2022. Semantic-Aware Domain Generalized Segmentation
论文地址
支撑材料
arXiv
代码地址
12345678@InProceedings&#123;Peng_2022_CVPR,    author    = &#123;Peng, Duo and Lei, Yinjie and Hayat, Munawar and Guo, Yulan and Li, Wen&#125;,    title     = &#123;Semantic-Aware Domain Generalized Segmentation&#125;,    booktitle = &#123;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&#125;,    month     = &#123;June&#125;,    year      = &#123;2022&#125;,    pages     = &#123 ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/22/20221122%20CVPR2022.%20Compound%20Domain%20Generalization%20via%20Meta-Knowledge%20Encoding/')"><div class="post_cover left"><a href="/blog/2022/11/22/20221122%20CVPR2022.%20Compound%20Domain%20Generalization%20via%20Meta-Knowledge%20Encoding/" title="CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2018logo.webp" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/22/20221122%20CVPR2022.%20Compound%20Domain%20Generalization%20via%20Meta-Knowledge%20Encoding/" title="CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding">未读</a></div><a class="article-title" href="/blog/2022/11/22/20221122%20CVPR2022.%20Compound%20Domain%20Generalization%20via%20Meta-Knowledge%20Encoding/" title="CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding">CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-22T14:13:00.000Z" title="发表于 2022-11-22 14:13:00" time="2022-11-22 14:13:00">2022-11-22</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Domain-Generalization/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Domain Generalization</span></a></span></div><div class="content">CVPR2022. Compound Domain Generalization via Meta-Knowledge Encoding
论文地址
Arxiv
12345678@InProceedings&#123;Chen_2022_CVPR,    author    = &#123;Chen, Chaoqi and Li, Jiongcheng and Han, Xiaoguang and Liu, Xiaoqing and Yu, Yizhou&#125;,    title     = &#123;Compound Domain Generalization via Meta-Knowledge Encoding&#125;,    booktitle = &#123;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&#125;,    month     = &#123;June&#125;,    year      = &#123;2022&# ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/22/20221122%20CVPR2022.%20Memory-Augmented%20Non-Local%20Attention%20for%20Video%20Super-Resolution/')"><div class="post_cover left"><a href="/blog/2022/11/22/20221122%20CVPR2022.%20Memory-Augmented%20Non-Local%20Attention%20for%20Video%20Super-Resolution/" title="CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/22/20221122%20CVPR2022.%20Memory-Augmented%20Non-Local%20Attention%20for%20Video%20Super-Resolution/" title="CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution">未读</a></div><a class="article-title" href="/blog/2022/11/22/20221122%20CVPR2022.%20Memory-Augmented%20Non-Local%20Attention%20for%20Video%20Super-Resolution/" title="CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution">CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-22T09:12:00.000Z" title="发表于 2022-11-22 09:12:00" time="2022-11-22 09:12:00">2022-11-22</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Attention/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Attention</span></a><a class="article-meta__tags" href="/blog/tags/Super-Resolution/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Super Resolution</span></a></span></div><div class="content">CVPR2022. Memory-Augmented Non-Local Attention for Video Super-Resolution
论文地址

文章提出了一种简单但有效的视频超分辨率方法。以往方法普遍采用时间上近邻的视频帧来帮助当前帧实现超分，但会存在空间帧对齐的问题，另外从附近相似的低分辨率图片中可能无法获得有用信息。文章采用跨帧非局部注意力机制来实现无需帧对齐的超分辨率，并设计了一种最新的记忆增强注意力模块来记住超分辨率训练过程中通用的视频细节，具有更好的精度和更高的泛化性能。
主要问题
视频超分辨率致力于从低分辨率视频中恢复高频细节，但存在两个问题。由于视频是运动的，因此融合相邻帧的信息前视频需要进行对齐，但在跑酷等快速变化的视频很难实现。此外低频视频中往往缺乏帮助视频超分辨率有用的信息，已有工作尝试从高频参考图片中迁移纹理来帮助视频超分，但无法满足时间一致性的问题。也有工作尝试从近邻帧来融合信息。但文章认为近邻帧获得的信息仍然有限，快速运动的视频中相邻帧之间的信息关联性更少，因此更难以挖掘有用信息。文章提出Cross-Frame Non-Local Attent ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/21/20221121%20CVPR2022.%20Pin%20the%20Memory%20Learning%20to%20Generalize%20Semantic%20Segmentation/')"><div class="post_cover left"><a href="/blog/2022/11/21/20221121%20CVPR2022.%20Pin%20the%20Memory%20Learning%20to%20Generalize%20Semantic%20Segmentation/" title="CVPR2022. Pin the Memory Learning to Generalize Semantic Segmentation" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPR2022. Pin the Memory Learning to Generalize Semantic Segmentation" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/21/20221121%20CVPR2022.%20Pin%20the%20Memory%20Learning%20to%20Generalize%20Semantic%20Segmentation/" title="CVPR2022. Pin the Memory Learning to Generalize Semantic Segmentation">未读</a></div><a class="article-title" href="/blog/2022/11/21/20221121%20CVPR2022.%20Pin%20the%20Memory%20Learning%20to%20Generalize%20Semantic%20Segmentation/" title="CVPR2022. Pin the Memory Learning to Generalize Semantic Segmentation">CVPR2022. Pin the Memory Learning to Generalize Semantic Segmentation</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-21T11:53:00.000Z" title="发表于 2022-11-21 11:53:00" time="2022-11-21 11:53:00">2022-11-21</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Semantic-Segmentation/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Semantic Segmentation</span></a><a class="article-meta__tags" href="/blog/tags/Meta-Learning/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Meta Learning</span></a></span></div><div class="content">Pin the Memory: Learning to Generalize Semantic Segmentation
论文地址

深度学习模型在语义分割领域取得了一些突破，但在源域训练的模型通常在新的挑战域中不能正常发挥效果，因而会影响泛化性能。文章基于元学习框架提出了记忆引导的领域泛化方法，该方法抽取出领域不变的语义类别概念知识，融入到类别记忆中。根据元学习的概念，文章反复训练记忆引导的网络，并模拟虚拟测试来：1）学习如何记忆领域无关的和独特的类信息；2）提供外部设置的记忆作为类别指导，以减少在任意新领域测试时数据表达的模糊性。文章提出了记忆发散和特征凝聚力损失，以指导面向类别感知领域泛化过程的记忆读取和更新过程。在多种基准数据集上的大量实验，表明了模型相比目前最新方法具有更好的泛化性能。
现有问题
语义分割近期的许多进展主要来自于在大批量稠密标注数据集上的深度神经网络，但在给定数据集（源域）上训练的模型不能很好地迁移到模型训练过程中没有见过的新领域（目标域）。克服两个领域分布的差异对于处理意外和未见过的新数据非常重要，尤其是在医疗诊断、自动驾驶等一些代替人工的任务上。
为了解决 ...</div></div></div><div class="recent-post-item" onclick="pjax.loadUrl('/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/')"><div class="post_cover left"><a href="/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/" title="CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks" style="display: flex;height: 100%;"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/img/CVPR2022logo.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks" style="pointer-events: none"></a></div><div class="recent-post-info"><div class="recent-post-info-top"><div class="recent-post-info-top-tips"><div class="article-categories-original">论文精读</div><a class="unvisited-post" href="/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/" title="CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks">未读</a></div><a class="article-title" href="/blog/2022/11/20/20221120%20CVPR2022.%20FAM%20Visual%20Explanations%20for%20the%20Feature%20Represenetations%20from%20Deep%20Convolutional%20Networks/" title="CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks">CVPR2022. FAM Visual Explanations for the Feature Representations from Deep Convolutional Networks</a></div><div class="article-meta-wrap"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days" style="font-size: 15px; display:none"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-20T19:30:00.000Z" title="发表于 2022-11-20 19:30:00" time="2022-11-20 19:30:00">2022-11-20</time><time class="time_hidden" datetime="2024-08-02T13:27:02.849Z" title="更新于 2024-08-02 13:27:02" time="2024-08-02 13:27:02">2024-08-02</time></span><span class="article-meta tags"><a class="article-meta__tags" href="/blog/tags/Explainability/" event.cancelbubble onclick="window.event.cancelBubble=true;"><span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Explainability</span></a></span></div><div class="content">CVPR2022. FAM: Visual Explanations for the Feature Representations from Deep Convolutional Networks
论文地址

近几年越来越多的注意力机制被用来解释特征表示模型的内部机理，但传统方法不能完全解决特征表示问题。特别是对于不属于任何一个类别的图片，仅依据现有类别和图片间的相似性并不能对其提供可靠的视觉解释。文章提出一种新的视觉注意力解释范式：Feature Activation Mapping (FAM)，特征激活映射。遵照这个范式，文章设计了Grad-FAM和Score-FAM来可视化特征表示。与以往方法不同之处在于，FAM关注对特征向量本身最具贡献的图片区域。主观和客观实验表明Score-FAM在人识别任务中能够实现有较好的特征表示可解释性，FAM还可以应用于其他类似自监督表示学习、开放集识别等任务中。
现有问题
模型理解对于卷积网络的应用越来越重要，分类模型中有关特征表示可解释性的方法包括：基于区域贡献的方法和基于图片相似性的方法。
基于区域贡献的方法Class Activation  ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/blog/"><i class="anzhiyufont anzhiyu-icon-chevron-left fa-fw" style="font-weight: inherit; font-size: 1rem;"></i><div class="pagination_tips_prev">上页</div></a><a class="page-number" href="/blog/">1</a><span class="page-number current">2</span><a class="page-number" href="/blog/page/3/#content-inner">3</a><a class="page-number" href="/blog/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/blog/page/3/#content-inner"><div class="pagination_tips_next">下页</div><i style="font-weight: inherit; font-size: 1rem;" class="anzhiyufont anzhiyu-icon-chevron-right fa-fw"></i></a><div class="toPageGroup"><input id="toPageText" oninput="value=value.replace(/[^0-9]/g,'')" maxlength="3" onkeyup="this.value=this.value.replace(/[^u4e00-u9fa5w]/g,'')" aria-label="toPage"><a id="toPageButton" onclick="anzhiyu.toPage()"><i class="anzhiyufont anzhiyu-icon-angles-right" style="font-weight: inherit; font-size: 1rem;"></i></a></div></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://github.com/xiuqhou/picx-images-hosting/raw/master/wechat_logo.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description">Trying to be better!</div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/blog/"><h1 class="author-info__name">xiuqhou</h1><div class="author-info__desc">冲冲冲！</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/xiuqhou" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客鸭~</div></div><div class="sticky_layout"><div class="card-widget"><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 1.05rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 1.05rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 1.05rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 1.05rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 1.05rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 1.05rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 1.05rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 1.05rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 1.05rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 1.05rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 1.05rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 1.05rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 1.05rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 1.05rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 1.05rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 1.05rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 1.05rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 1.05rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">优化算法<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">博客<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">服务器<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 1.05rem;">画图<sup>1</sup></a></div></div><hr/><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/blog/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/03/"><span class="card-archive-list-date">三月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/02/"><span class="card-archive-list-date">二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2023/01/"><span class="card-archive-list-date">一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/12/"><span class="card-archive-list-date">十二月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/11/"><span class="card-archive-list-date">十一月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/blog/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/><div class="card-webinfo"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="webinfo-item-title"><i class="anzhiyufont anzhiyu-icon-file-lines"></i><div class="item-name">文章总数 :</div></div><div class="item-count">32</div></div><div class="webinfo-item"><div class="webinfo-item-title"><i class="anzhiyufont anzhiyu-icon-stopwatch"></i><div class="item-name">建站天数 :</div></div><div class="item-count" id="runtimeshow" data-publishDate="2024-07-18T00:00:00.000Z"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></div></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2024 By <a class="footer-bar-link" href="/blog/" title="xiuqhou" target="_blank">xiuqhou</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/blog/archives/" title="archive"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/blog/tags/" title="tag"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/blog/categories/" title="category"><div class="headline">分类</div><div class="length-num">5</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/blog/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/blog/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/blog/tags/Attention/" style="font-size: 0.88rem;">Attention<sup>4</sup></a><a href="/blog/tags/Classification/" style="font-size: 0.88rem;">Classification<sup>2</sup></a><a href="/blog/tags/Context/" style="font-size: 0.88rem;">Context<sup>2</sup></a><a href="/blog/tags/Detection-Transformer-DETR/" style="font-size: 0.88rem;">Detection Transformer (DETR)<sup>3</sup></a><a href="/blog/tags/Domain-Generalization/" style="font-size: 0.88rem;">Domain Generalization<sup>2</sup></a><a href="/blog/tags/Explainability/" style="font-size: 0.88rem;">Explainability<sup>1</sup></a><a href="/blog/tags/Few-Shot-Object-Detection/" style="font-size: 0.88rem;">Few-Shot Object Detection<sup>2</sup></a><a href="/blog/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/blog/tags/Graph-Reasoning/" style="font-size: 0.88rem;">Graph Reasoning<sup>2</sup></a><a href="/blog/tags/Knowledge-Graph/" style="font-size: 0.88rem;">Knowledge Graph<sup>1</sup></a><a href="/blog/tags/LaTeX/" style="font-size: 0.88rem;">LaTeX<sup>3</sup></a><a href="/blog/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>2</sup></a><a href="/blog/tags/Machine-Learning/" style="font-size: 0.88rem;">Machine Learning<sup>2</sup></a><a href="/blog/tags/Meta-Learning/" style="font-size: 0.88rem;">Meta Learning<sup>1</sup></a><a href="/blog/tags/Object-Detection/" style="font-size: 0.88rem;">Object Detection<sup>8</sup></a><a href="/blog/tags/Relationship-Detection/" style="font-size: 0.88rem;">Relationship Detection<sup>1</sup></a><a href="/blog/tags/Semantic-Segmentation/" style="font-size: 0.88rem;">Semantic Segmentation<sup>2</sup></a><a href="/blog/tags/Super-Resolution/" style="font-size: 0.88rem;">Super Resolution<sup>1</sup></a><a href="/blog/tags/Transformer/" style="font-size: 0.88rem;">Transformer<sup>1</sup></a><a href="/blog/tags/v2ray/" style="font-size: 0.88rem;">v2ray<sup>1</sup></a><a href="/blog/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">优化算法<sup>1</sup></a><a href="/blog/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 0.88rem;">博客<sup>1</sup></a><a href="/blog/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 0.88rem;">服务器<sup>1</sup></a><a href="/blog/tags/%E7%94%BB%E5%9B%BE/" style="font-size: 0.88rem;">画图<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><script src="/blog/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 xiuqhou 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/blog/js/search/local-search.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="anzhiyu"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/blog/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/blog/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>